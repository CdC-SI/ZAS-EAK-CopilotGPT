{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe3204-88f3-4056-bc1d-30ab95b08efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple  # for type hints\n",
    "\n",
    "import numpy as np  # for manipulating arrays\n",
    "import pandas as pd  # for manipulating data in dataframes\n",
    "import pickle  # for saving the embeddings cache\n",
    "import plotly.express as px  # for plots\n",
    "import random  # for generating run IDs\n",
    "from sklearn.model_selection import train_test_split  # for splitting train & test data\n",
    "import torch  # for matrix optimization\n",
    "\n",
    "from utils.embedding import get_embedding\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf7c7d6-38ad-473e-ad35-d9981b38ea77",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a68477-68eb-4a50-b6b6-defebb60c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"indexing/data/memento_eval_qa_FZ.csv\")\n",
    "\n",
    "df = df[[\"question\", \"answer\"]]\n",
    "df = df.rename(columns={\"question\": \"text_1\",\n",
    "                   \"answer\": \"text_2\"})\n",
    "df[\"label\"] = 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203bb36-cf20-4065-b8e0-ba1b301479f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb2b7c-a1d1-471a-87e7-1f88ca538281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a90a58-b311-4065-8ad6-98a8b38d90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"indexing/data/memento_eval_qa_FZ.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d77258-d14a-4a5a-a22e-3dc26453d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"url\", \"question\", \"answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50396a06-ebef-4aca-816b-03f13b0c73f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7611d90-bb5b-400a-8a8e-a37cd76c20c1",
   "metadata": {},
   "source": [
    "# Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15385c-f754-4f3d-a704-ca252e5c5c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "test_fraction = 0.2\n",
    "random_seed = 42\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=test_fraction, stratify=df[\"label\"], random_state=random_seed\n",
    ")\n",
    "train_df.loc[:, \"dataset\"] = \"train\"\n",
    "test_df.loc[:, \"dataset\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26ad93-b714-48c3-a535-760836f50e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_train: \", len(train_df))\n",
    "print(\"n_test: \", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9a7aa-5bbe-4906-b189-90c855c16660",
   "metadata": {},
   "source": [
    "# Enrich dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88822bda-165f-4d66-b840-3a650fb48176",
   "metadata": {},
   "source": [
    "# Generate negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91785a5-50b0-4607-ad57-e89c62500824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "negative_examples = []\n",
    "for i, row in train_df.iterrows():\n",
    "    for neg_text in train_df[~train_df.index.isin([i])].text_2:\n",
    "        negative_examples.append(\n",
    "            {\"text_1\": row.text_1,\n",
    "             \"text_2\": neg_text,\n",
    "             \"label\": -1,\n",
    "             \"dataset\": \"train\"}\n",
    "        )\n",
    "negatives = pd.DataFrame.from_dict(negative_examples)\n",
    "\n",
    "train_df = pd.concat([train_df, negatives], axis=0)\n",
    "\n",
    "# test\n",
    "negative_examples = []\n",
    "for i, row in test_df.iterrows():\n",
    "    for neg_text in test_df[~test_df.index.isin([i])].text_2:\n",
    "        negative_examples.append(\n",
    "            {\"text_1\": row.text_1,\n",
    "             \"text_2\": neg_text,\n",
    "             \"label\": -1,\n",
    "             \"dataset\": \"test\"}\n",
    "        )\n",
    "negatives = pd.DataFrame.from_dict(negative_examples)\n",
    "\n",
    "test_df = pd.concat([test_df, negatives], axis=0)\n",
    "\n",
    "df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062314f5-2c4f-4d1c-ab51-52c4d2e64dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_train: \", len(train_df))\n",
    "print(\"n_test: \", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de96652-1c6d-4e6a-ba15-1e10d4f10ac7",
   "metadata": {},
   "source": [
    "# Compute and cache embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b985ea8-b141-4ca4-98e1-b0d5a95e3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = lambda a, b: dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5bec4-74bb-4119-a3e8-a3067c75123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a cache of embeddings to avoid recomputing\n",
    "# cache is a dict of tuples (text, engine) -> embedding\n",
    "embedding_cache_path = \"indexing/data/embedding_cache.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(embedding_cache_path, \"rb\") as f:\n",
    "        embedding_cache = pickle.load(f)\n",
    "except Exception as e:\n",
    "    #precomputed_embedding_cache_path = \"https://cdn.openai.com/API/examples/data/snli_embedding_cache.pkl\"\n",
    "    #embedding_cache = pd.read_pickle(precomputed_embedding_cache_path)\n",
    "    pass\n",
    "\n",
    "\n",
    "# this function will get embeddings from the cache and save them there afterward\n",
    "def get_embedding_with_cache(\n",
    "    text: str,\n",
    "    embedding_cache: dict = embedding_cache,\n",
    "    embedding_cache_path: str = embedding_cache_path,\n",
    ") -> list:\n",
    "    if text not in embedding_cache.keys():\n",
    "        # if not in cache, call API to get embedding\n",
    "        embedding_cache[text] = get_embedding(text)\n",
    "        # save embeddings cache to disk after each update\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6415c-9bef-435f-8180-2d1016286f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column of embeddings\n",
    "for column in [\"text_1\", \"text_2\"]:\n",
    "    df[f\"{column}_embedding\"] = df[column].apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014f65e-0e0f-462c-a05b-840289efa74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column of cosine similarity between embeddings\n",
    "df[\"cosine_similarity\"] = df.apply(\n",
    "    lambda row: cos_sim(row[\"text_1_embedding\"], row[\"text_2_embedding\"]),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf296aa-f07d-487a-a4e5-dc134d89548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514023dd-6a96-4d1d-9ac9-c764967270ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"indexing/data/embedding_cache.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da3355-386a-4f17-b108-22af02166354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"indexing/data/embedding_cache.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755afe4-6efe-4d63-948e-31b5663bc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"indexing/data/embedding_cache.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcf78e-59a6-4c38-a75d-af3c629d74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df[\"text_1_embedding\"] = df.apply(lambda row: ast.literal_eval(row[\"text_1_embedding\"]), axis=1)\n",
    "df[\"text_2_embedding\"] = df.apply(lambda row: ast.literal_eval(row[\"text_2_embedding\"]), axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f0d67-3159-4f42-9fd7-f3056f541634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample so label proportions are equivalent\n",
    "(df[df.dataset==\"train\"].label==-1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69b45a-adc5-4924-aac8-af0deb71a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[df.dataset==\"train\"].label==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccd25a-b7e3-49a2-a9ef-21ded6d0ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c18a77-6786-4fd5-8f07-be17a8669434",
   "metadata": {},
   "source": [
    "# Plot cosine similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0beca-eb88-47b5-9946-03b263b2de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de637a91-1e7a-4868-a0cf-846071aef1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy (and its standard error) of predicting label=1 if similarity>x\n",
    "# x is optimized by sweeping from -1 to 1 in steps of 0.01\n",
    "def accuracy_and_se(cosine_similarity: float, labeled_similarity: int) -> Tuple[float]:\n",
    "    accuracies = []\n",
    "    for threshold_thousandths in range(-1000, 1000, 1):\n",
    "        threshold = threshold_thousandths / 1000\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for cs, ls in zip(cosine_similarity, labeled_similarity):\n",
    "            total += 1\n",
    "            if cs > threshold:\n",
    "                prediction = 1\n",
    "            else:\n",
    "                prediction = -1\n",
    "            if prediction == ls:\n",
    "                correct += 1\n",
    "        accuracy = correct / total\n",
    "        accuracies.append(accuracy)\n",
    "    a = max(accuracies)\n",
    "    n = len(cosine_similarity)\n",
    "    standard_error = (a * (1 - a) / n) ** 0.5  # standard error of binomial\n",
    "    return a, standard_error\n",
    "\n",
    "\n",
    "# check that training and test sets are balanced\n",
    "fig = px.histogram(\n",
    "    df,\n",
    "    x=\"cosine_similarity\",\n",
    "    color=\"label\",\n",
    "    barmode=\"overlay\",\n",
    "    width=500,\n",
    "    facet_row=\"dataset\",\n",
    ")\n",
    "pio.show(fig)\n",
    "\n",
    "for dataset in [\"train\", \"test\"]:\n",
    "    data = df[df[\"dataset\"] == dataset]\n",
    "    a, se = accuracy_and_se(data[\"cosine_similarity\"], data[\"label\"])\n",
    "    print(f\"{dataset} accuracy: {a:0.1%} ± {1.96 * se:0.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60869c72-3198-46f7-b5ad-140ae1d6bc96",
   "metadata": {},
   "source": [
    "# Train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89723174-ee8c-4d9f-b0b8-e3f5bdebd0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_multiplied_by_matrix(\n",
    "    embedding: List[float], matrix: torch.tensor\n",
    ") -> np.array:\n",
    "    embedding_tensor = torch.tensor(embedding).float()\n",
    "    modified_embedding = embedding_tensor @ matrix\n",
    "    modified_embedding = modified_embedding.detach().numpy()\n",
    "    return modified_embedding\n",
    "\n",
    "\n",
    "# compute custom embeddings and new cosine similarities\n",
    "def apply_matrix_to_embeddings_dataframe(matrix: torch.tensor, df: pd.DataFrame):\n",
    "    for column in [\"text_1_embedding\", \"text_2_embedding\"]:\n",
    "        df[f\"{column}_custom\"] = df[column].apply(\n",
    "            lambda x: embedding_multiplied_by_matrix(x, matrix)\n",
    "        )\n",
    "    df[\"cosine_similarity_custom\"] = df.apply(\n",
    "        lambda row: cos_sim(\n",
    "            row[\"text_1_embedding_custom\"], row[\"text_2_embedding_custom\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da266739-11e6-4e76-81a0-b96b564b8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_matrix(\n",
    "    modified_embedding_length: int = 1536,  # in my brief experimentation, bigger was better (2048 is length of babbage encoding)\n",
    "    batch_size: int = 100,\n",
    "    max_epochs: int = 100,\n",
    "    learning_rate: float = 100.0,  # seemed to work best when similar to batch size - feel free to try a range of values\n",
    "    dropout_fraction: float = 0.0,  # in my testing, dropout helped by a couple percentage points (definitely not necessary)\n",
    "    df: pd.DataFrame = df,\n",
    "    print_progress: bool = True,\n",
    "    save_results: bool = True,\n",
    ") -> torch.tensor:\n",
    "    \"\"\"Return matrix optimized to minimize loss on training data.\"\"\"\n",
    "    run_id = random.randint(0, 2 ** 31 - 1)  # (range is arbitrary)\n",
    "    # convert from dataframe to torch tensors\n",
    "    # e is for embedding, s for similarity label\n",
    "    def tensors_from_dataframe(\n",
    "        df: pd.DataFrame,\n",
    "        embedding_column_1: str,\n",
    "        embedding_column_2: str,\n",
    "        similarity_label_column: str,\n",
    "    ) -> Tuple[torch.tensor]:\n",
    "        e1 = np.stack(np.array(df[embedding_column_1].values))\n",
    "        e2 = np.stack(np.array(df[embedding_column_2].values))\n",
    "        s = np.stack(np.array(df[similarity_label_column].astype(\"float\").values))\n",
    "\n",
    "        e1 = torch.from_numpy(e1).float()\n",
    "        e2 = torch.from_numpy(e2).float()\n",
    "        s = torch.from_numpy(s).float()\n",
    "\n",
    "        return e1, e2, s\n",
    "\n",
    "    e1_train, e2_train, s_train = tensors_from_dataframe(\n",
    "        df[df[\"dataset\"] == \"train\"], \"text_1_embedding\", \"text_2_embedding\", \"label\"\n",
    "    )\n",
    "    e1_test, e2_test, s_test = tensors_from_dataframe(\n",
    "        df[df[\"dataset\"] == \"test\"], \"text_1_embedding\", \"text_2_embedding\", \"label\"\n",
    "    )\n",
    "\n",
    "    # create dataset and loader\n",
    "    dataset = torch.utils.data.TensorDataset(e1_train, e2_train, s_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # define model (similarity of projected embeddings)\n",
    "    def model(embedding_1, embedding_2, matrix, dropout_fraction=dropout_fraction):\n",
    "        e1 = torch.nn.functional.dropout(embedding_1, p=dropout_fraction)\n",
    "        e2 = torch.nn.functional.dropout(embedding_2, p=dropout_fraction)\n",
    "        modified_embedding_1 = e1 @ matrix  # @ is matrix multiplication\n",
    "        modified_embedding_2 = e2 @ matrix\n",
    "        similarity = torch.nn.functional.cosine_similarity(\n",
    "            modified_embedding_1, modified_embedding_2\n",
    "        )\n",
    "        return similarity\n",
    "\n",
    "    # define loss function to minimize\n",
    "    def mse_loss(predictions, targets):\n",
    "        difference = predictions - targets\n",
    "        return torch.sum(difference * difference) / difference.numel()\n",
    "\n",
    "    # initialize projection matrix\n",
    "    embedding_length = len(df[\"text_1_embedding\"].values[0])\n",
    "    matrix = torch.randn(\n",
    "        embedding_length, modified_embedding_length, requires_grad=True\n",
    "    )\n",
    "\n",
    "    epochs, types, losses, accuracies, matrices = [], [], [], [], []\n",
    "    for epoch in range(1, 1 + max_epochs):\n",
    "        # iterate through training dataloader\n",
    "        for a, b, actual_similarity in train_loader:\n",
    "            # generate prediction\n",
    "            predicted_similarity = model(a, b, matrix)\n",
    "            # get loss and perform backpropagation\n",
    "            loss = mse_loss(predicted_similarity, actual_similarity)\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            with torch.no_grad():\n",
    "                matrix -= matrix.grad * learning_rate\n",
    "                # set gradients to zero\n",
    "                matrix.grad.zero_()\n",
    "        # calculate test loss\n",
    "        test_predictions = model(e1_test, e2_test, matrix)\n",
    "        test_loss = mse_loss(test_predictions, s_test)\n",
    "\n",
    "        # compute custom embeddings and new cosine similarities\n",
    "        apply_matrix_to_embeddings_dataframe(matrix, df)\n",
    "\n",
    "        # calculate test accuracy\n",
    "        for dataset in [\"train\", \"test\"]:\n",
    "            data = df[df[\"dataset\"] == dataset]\n",
    "            a, se = accuracy_and_se(data[\"cosine_similarity_custom\"], data[\"label\"])\n",
    "\n",
    "            # record results of each epoch\n",
    "            epochs.append(epoch)\n",
    "            types.append(dataset)\n",
    "            losses.append(loss.item() if dataset == \"train\" else test_loss.item())\n",
    "            accuracies.append(a)\n",
    "            matrices.append(matrix.detach().numpy())\n",
    "\n",
    "            # optionally print accuracies\n",
    "            if print_progress is True:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}/{max_epochs}: {dataset} accuracy: {a:0.1%} ± {1.96 * se:0.1%}\"\n",
    "                )\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        {\"epoch\": epochs, \"type\": types, \"loss\": losses, \"accuracy\": accuracies}\n",
    "    )\n",
    "    data[\"run_id\"] = run_id\n",
    "    data[\"modified_embedding_length\"] = modified_embedding_length\n",
    "    data[\"batch_size\"] = batch_size\n",
    "    data[\"max_epochs\"] = max_epochs\n",
    "    data[\"learning_rate\"] = learning_rate\n",
    "    data[\"dropout_fraction\"] = dropout_fraction\n",
    "    data[\n",
    "        \"matrix\"\n",
    "    ] = matrices  # saving every single matrix can get big; feel free to delete/change\n",
    "    if save_results is True:\n",
    "        data.to_csv(f\"indexing/data/{run_id}_optimization_results.csv\", index=False)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e9891-45e1-494b-957e-06eb9cc6233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example hyperparameter search\n",
    "# I recommend starting with max_epochs=10 while initially exploring\n",
    "results = []\n",
    "max_epochs = 10\n",
    "dropout_fraction = 0.2\n",
    "for batch_size, learning_rate in [(10, 10), (100, 100), (1000, 1000)]:\n",
    "    result = optimize_matrix(\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        max_epochs=max_epochs,\n",
    "        dropout_fraction=dropout_fraction,\n",
    "        save_results=False,\n",
    "    )\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdeac11-0dbe-453b-96a1-9a2ee26ff32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df = pd.concat(results)\n",
    "\n",
    "# plot training loss and test loss over time\n",
    "px.line(\n",
    "    runs_df,\n",
    "    line_group=\"run_id\",\n",
    "    x=\"epoch\",\n",
    "    y=\"loss\",\n",
    "    color=\"type\",\n",
    "    hover_data=[\"batch_size\", \"learning_rate\", \"dropout_fraction\"],\n",
    "    facet_row=\"learning_rate\",\n",
    "    facet_col=\"batch_size\",\n",
    "    width=500,\n",
    ").show()\n",
    "\n",
    "# plot accuracy over time\n",
    "px.line(\n",
    "    runs_df,\n",
    "    line_group=\"run_id\",\n",
    "    x=\"epoch\",\n",
    "    y=\"accuracy\",\n",
    "    color=\"type\",\n",
    "    hover_data=[\"batch_size\", \"learning_rate\", \"dropout_fraction\"],\n",
    "    facet_row=\"learning_rate\",\n",
    "    facet_col=\"batch_size\",\n",
    "    width=500,\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ad82c-ef81-49b8-ae5a-ec169e2870d2",
   "metadata": {},
   "source": [
    "# Plot before/after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bba7eb-6917-4262-9be7-9c0bf3759424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply result of best run to original data\n",
    "best_run = runs_df.sort_values(by=\"accuracy\", ascending=False).iloc[0]\n",
    "best_matrix = best_run[\"matrix\"]\n",
    "apply_matrix_to_embeddings_dataframe(best_matrix, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6e7ba-140b-4dcd-8a99-a140ba470971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot similarity distribution BEFORE customization\n",
    "px.histogram(\n",
    "    df,\n",
    "    x=\"cosine_similarity\",\n",
    "    color=\"label\",\n",
    "    barmode=\"overlay\",\n",
    "    width=500,\n",
    "    facet_row=\"dataset\",\n",
    ").show()\n",
    "\n",
    "test_df = df[df[\"dataset\"] == \"test\"]\n",
    "a, se = accuracy_and_se(test_df[\"cosine_similarity\"], test_df[\"label\"])\n",
    "print(f\"Test accuracy: {a:0.1%} ± {1.96 * se:0.1%}\")\n",
    "\n",
    "# plot similarity distribution AFTER customization\n",
    "px.histogram(\n",
    "    df,\n",
    "    x=\"cosine_similarity_custom\",\n",
    "    color=\"label\",\n",
    "    barmode=\"overlay\",\n",
    "    width=500,\n",
    "    facet_row=\"dataset\",\n",
    ").show()\n",
    "\n",
    "a, se = accuracy_and_se(test_df[\"cosine_similarity_custom\"], test_df[\"label\"])\n",
    "print(f\"Test accuracy after customization: {a:0.1%} ± {1.96 * se:0.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4c12a2-a589-4d40-8b19-a2072ab81c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c1c7d-bd37-43e8-86a7-81554075aee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_copilot",
   "language": "python",
   "name": "venv_copilot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

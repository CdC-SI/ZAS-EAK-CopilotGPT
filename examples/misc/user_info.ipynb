{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "POSTGRES_USER=os.environ.get('POSTGRES_USER', None)\n",
    "POSTGRES_PASSWORD=os.environ.get('POSTGRES_PASSWORD', None)\n",
    "POSTGRES_DB=os.environ.get('POSTGRES_DB', None)\n",
    "POSTGRES_HOST=os.environ.get('POSTGRES_HOST', None)\n",
    "POSTGRES_PORT=os.environ.get('POSTGRES_PORT', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "def get_db():\n",
    "\n",
    "    DATABASE_URL = \"postgresql://admin:pg_password@localhost:5432/pg_db\"\n",
    "\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "\n",
    "    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "    db = SessionLocal()\n",
    "\n",
    "    return db\n",
    "\n",
    "db = get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import Session\n",
    "from database.models import Document\n",
    "\n",
    "def get_by_url(db: Session, url: str):\n",
    "    \"\"\"\n",
    "    Get a document by its URL field\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db: Session\n",
    "        Database session\n",
    "    url: str\n",
    "        Document\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Document\n",
    "    \"\"\"\n",
    "    return db.query(Document).filter(Document.url == url).one_or_none()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_uuid = \"ed33c97e-de84-49a3-a4a8-bcef44522ec4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database.models import ChatHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids = db.query(ChatHistory).filter(ChatHistory.user_uuid == user_uuid).distinct()\n",
    "uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.user_uuid for x in uuids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import distinct\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_user_conversations(session):\n",
    "    # Query distinct pairs of user_uuid and conversation_uuid\n",
    "    results = session.query(\n",
    "        distinct(ChatHistory.user_uuid),\n",
    "        ChatHistory.conversation_uuid\n",
    "    ).all()\n",
    "\n",
    "    # Create a defaultdict to automatically handle new users\n",
    "    user_conversations = defaultdict(list)\n",
    "\n",
    "    # Group conversation_uuids by user_uuid\n",
    "    for user_uuid, conv_uuid in results:\n",
    "        user_conversations[user_uuid].append(conv_uuid)\n",
    "\n",
    "    # Convert defaultdict to regular dict if needed\n",
    "    return dict(user_conversations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_user_conversations(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_user_messages(session):\n",
    "    # Query all assistant messages\n",
    "    results = session.query(ChatHistory).filter(\n",
    "        ChatHistory.role == \"user\"\n",
    "    ).order_by(ChatHistory.timestamp).all()\n",
    "\n",
    "    # Create nested defaultdict structure\n",
    "    nested_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Organize messages by user_uuid and conversation_uuid\n",
    "    for msg in results:\n",
    "        nested_dict[msg.user_uuid][msg.conversation_uuid].append(msg.message)\n",
    "\n",
    "    # Convert defaultdict to regular dict\n",
    "    return {\n",
    "        user_uuid: dict(conversations)\n",
    "        for user_uuid, conversations in nested_dict.items()\n",
    "    }\n",
    "\n",
    "def get_user_assistant_messages(session):\n",
    "    # Query all assistant messages\n",
    "    results = session.query(ChatHistory).filter(\n",
    "        ChatHistory.role == \"assistant\"\n",
    "    ).order_by(ChatHistory.timestamp).all()\n",
    "\n",
    "    # Create nested defaultdict structure\n",
    "    nested_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Organize messages by user_uuid and conversation_uuid\n",
    "    for msg in results:\n",
    "        nested_dict[msg.user_uuid][msg.conversation_uuid].append(msg.message)\n",
    "\n",
    "    # Convert defaultdict to regular dict\n",
    "    return {\n",
    "        user_uuid: dict(conversations)\n",
    "        for user_uuid, conversations in nested_dict.items()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_messages = get_user_messages(db)\n",
    "user_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate overs conversations to calculate token count !!!\n",
    "in_tokens = \"\".join(user_messages[\"ed33c97e-de84-49a3-a4a8-bcef44522ec4\"][\"d21b264a-04fd-4d40-8057-2b710e189cf6\"])\n",
    "in_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assistant_messages = get_user_assistant_messages(db)\n",
    "assistant_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_messages[\"ed33c97e-de84-49a3-a4a8-bcef44522ec4\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tokens = \" \".join(assistant_messages[\"ed33c97e-de84-49a3-a4a8-bcef44522ec4\"][\"d21b264a-04fd-4d40-8057-2b710e189cf6\"])\n",
    "out_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in_tokens = len(tokenizer.encode(in_tokens))\n",
    "n_in_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out_tokens = len(tokenizer.encode(out_tokens))\n",
    "n_out_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "# get cached input tokens\n",
    "# system prompt for rag/retrievers/etc. depending on user selection\n",
    "\n",
    "# model providers\n",
    "class ModelProvider(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    AZUREOPENAI = \"azure_openai\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    GROQ = \"groq\"\n",
    "    GEMINI = \"gemini\"\n",
    "    MISTRAL = \"mistral\"\n",
    "\n",
    "# get output tokens\n",
    "class ModelPricingService:\n",
    "\n",
    "    _PRICING = {\n",
    "        ModelProvider.OPENAI: {\n",
    "            \"IN\": 0.15 / 1_000_0000,\n",
    "            \"IN_CACHED\": 0.075 / 1_000_0000,\n",
    "            \"OUT\": 0.6 / 1_000_0000,\n",
    "        },\n",
    "        ModelProvider.AZUREOPENAI: {\n",
    "            \"IN\": 0.15 / 1_000_0000,\n",
    "            \"IN_CACHED\": 0.075 / 1_000_0000,\n",
    "            \"OUT\": 0.6 / 1_000_0000,\n",
    "        },\n",
    "        ModelProvider.ANTHROPIC: {\n",
    "            \"IN\": 0.15 / 1_000_0000,\n",
    "            \"IN_CACHED\": 0.075 / 1_000_0000,\n",
    "            \"OUT\": 0.6 / 1_000_0000,\n",
    "        },\n",
    "        ModelProvider.GROQ: {\n",
    "            \"IN\": 0.15 / 1_000_0000,\n",
    "            \"IN_CACHED\": 0.075 / 1_000_0000,\n",
    "            \"OUT\": 0.6 / 1_000_0000,\n",
    "        },\n",
    "        ModelProvider.GEMINI: {\n",
    "            \"IN\": 0.15 / 1_000_0000,\n",
    "            \"IN_CACHED\": 0.075 / 1_000_0000,\n",
    "            \"OUT\": 0.6 / 1_000_0000,\n",
    "        },\n",
    "        ModelProvider.MISTRAL: {\n",
    "            \"IN\": 0.15 / 1_000_0000,\n",
    "            \"IN_CACHED\": 0.075 / 1_000_0000,\n",
    "            \"OUT\": 0.6 / 1_000_0000,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_input_cost(cls, n_tokens: int, model_provider: ModelProvider, **kwargs) -> float:\n",
    "        input_cost = cls._PRICING[model_provider].get(\"IN\") * n_tokens\n",
    "        return input_cost\n",
    "\n",
    "    @classmethod\n",
    "    def get_input_cached_cost(cls, n_tokens: int, model_provider: ModelProvider, **kwargs) -> float:\n",
    "        input_cost = cls._PRICING[model_provider].get(\"IN_CACHED\") * n_tokens\n",
    "        return input_cost\n",
    "\n",
    "    @classmethod\n",
    "    def get_output_cost(cls, n_tokens: int, model_provider: ModelProvider, **kwargs) -> float:\n",
    "        input_cost = cls._PRICING[model_provider].get(\"OUT\") * n_tokens\n",
    "        return input_cost\n",
    "\n",
    "    @classmethod\n",
    "    def get_total_cost(cls, n_in_tokens: int, n_in_cached_tokens: int = 0, n_out_tokens: int, model_provider: ModelProvider, **kwargs) -> float:\n",
    "        n_in_tokens_cost = cls.get_input_cost(n_in_tokens, model_provider)\n",
    "        n_in_cached_tokens_cost = cls.get_input_cached_cost(n_in_cached_tokens, model_provider)\n",
    "        n_out_tokens_cost = cls.get_output_cost(n_out_tokens, model_provider)\n",
    "\n",
    "        cost = {\n",
    "            \"n_in_tokens_cost\": n_in_tokens_cost,\n",
    "            \"n_in_cached_tokens_cost\": n_in_cached_tokens_cost,\n",
    "            \"n_out_tokens_cost\": n_out_tokens_cost,\n",
    "            \"total_cost\": n_in_tokens_cost + n_in_cached_tokens_cost + n_out_tokens_cost\n",
    "        }\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_in_cached_tokens = 0\n",
    "ModelPricingService.get_total_cost(n_in_tokens=n_in_tokens, n_out_tokens=n_out_tokens, model_provider=ModelProvider.OPENAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelPricingService.get_input_cost(n_in_tokens, ModelProvider.OPENAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelPricingService.get_output_cost(n_out_tokens, ModelProvider.OPENAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_copilot",
   "language": "python",
   "name": "venv_copilot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

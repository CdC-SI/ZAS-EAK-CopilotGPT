{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"docs/images/DSPy8.png\" alt=\"DSPy7 Image\" height=\"150\"/>\n",
    "\n",
    "## **DSPy**: Programming with Foundation Models\n",
    "\n",
    "[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/intro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the **DSPy** framework for **Programming with Foundation Models**, i.e., language models (LMs) and retrieval models (RMs).\n",
    "\n",
    "**DSPy** emphasizes programming over prompting. It unifies techniques for **prompting** and **fine-tuning** LMs as well as improving them with **reasoning** and **tool/retrieval augmentation**, all expressed through a _minimalistic set of Pythonic operations that compose and learn_.\n",
    "\n",
    "**DSPy** provides **composable and declarative modules** for instructing LMs in a familiar Pythonic syntax. On top of that, **DSPy** introduces an **automatic compiler that teaches LMs** how to conduct the declarative steps in your program. The **DSPy compiler** will internally _trace_ your program and then **craft high-quality prompts for large LMs (or train automatic finetunes for small LMs)** to teach them the steps of your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0] Setting Up\n",
    "\n",
    "As we'll start to see below, **DSPy** can routinely teach powerful models like `GPT-3.5` and local models like `T5-base` or `Llama2-13b` to be much more reliable at complex tasks. **DSPy** will compile the _same program_ into different few-shot prompts and/or finetunes for each LM.\n",
    "\n",
    "Let's begin by setting things up. The snippet below will also install **DSPy** if it's not there already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==1.30.5 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (1.30.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from openai==1.30.5) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from openai==1.30.5) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from openai==1.30.5) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from openai==1.30.5) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from openai==1.30.5) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from openai==1.30.5) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from openai==1.30.5) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai==1.30.5) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.30.5) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.30.5) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.30.5) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.30.5) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.30.5) (2.18.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages/pydantic/_internal/_config.py:334: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "/Users/kieranschubert/Desktop/if/eak-copilot/venv_copilot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "! pip install openai==1.30.5\n",
    "    \n",
    "import dspy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1] Getting Started\n",
    "\n",
    "We'll start by setting up the language model (LM) and retrieval model (RM). **DSPy** supports multiple API and local models. In this notebook, we'll work with GPT-3.5 (`gpt-3.5-turbo`) and the retriever `ColBERTv2`.\n",
    "\n",
    "To make things easy, we've set up a ColBERTv2 server hosting a Wikipedia 2017 \"abstracts\" search index (i.e., containing first paragraph of each article from this [2017 dump](https://hotpotqa.github.io/wiki-readme.html)), so you don't need to worry about setting one up! It's free.\n",
    "\n",
    "**Note:** _If you want to run this notebook without changing the examples, you don't need an API key. All examples are already cached internally so you can inspect them!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "def get_db():\n",
    "    \n",
    "    DATABASE_URL = \"postgresql://admin:pg_password@localhost:5432/pg_db\"\n",
    "    \n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    \n",
    "    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "    \n",
    "    db = SessionLocal()\n",
    "\n",
    "    return db\n",
    "\n",
    "db = get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = dspy.OpenAI(model='gpt-4o')\n",
    "rm = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "\n",
    "dspy.settings.configure(lm=llm, rm=rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional\n",
    "\n",
    "class DSPythonicRMClient(dspy.Retrieve):\n",
    "    def __init__(self, url: str, port:int = None, k:int = 3):\n",
    "        super().__init__(k=k)\n",
    "\n",
    "        self.url = f`{url}:{port}` if port else url\n",
    "\n",
    "    def forward(self, query_or_queries:str, k:Optional[int]) -> dspy.Prediction:\n",
    "        params = {\"query\": query_or_queries, \"k\": k if k else self.k}\n",
    "        response = requests.get(self.url, params=params)\n",
    "\n",
    "        response = response.json()[\"retrieved_passages\"]    # List of top k passages\n",
    "        return dspy.Prediction(\n",
    "            passages=response\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonicRMClient:\n",
    "    def __init__(self, url: str, port:int = None):\n",
    "        self.url = f`{url}:{port}` if port else url\n",
    "\n",
    "    def __call__(self, query:str, k:int) -> List[str]:\n",
    "        # Only accept single query input, feel free to modify it to support \n",
    "\n",
    "        params = {\"query\": query, \"k\": k}\n",
    "        response = requests.get(self.url, params=params)\n",
    "\n",
    "        response = response.json()[\"retrieved_passages\"]    # List of top k passages\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from rag.retrievers import RetrieverClient, BaseRetriever, Reranker\n",
    "from rag.factory import RetrieverFactory\n",
    "from rag.llm.factory import LLMFactory\n",
    "from config.base_config import rag_config\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"Dictionary with dot notation access.\"\"\"\n",
    "    def __getattr__(self, key):\n",
    "        return self[key]\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self[key] = value\n",
    "\n",
    "    def __delattr__(self, key):\n",
    "        del self[key]\n",
    "\n",
    "class RetrieverWrapper(RetrieverClient):\n",
    "    def __init__(self, retrievers: List[BaseRetriever], reranker: Reranker, db, k):\n",
    "        super().__init__(retrievers, reranker)\n",
    "        self.db = db\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, query_or_queries: str, k: Optional[int] = None) -> List[dotdict]:\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "        docs = self.get_documents(db=self.db, query=query_or_queries, k=k)\n",
    "        return [dotdict({\"long_text\": doc[\"text\"]}) for doc in docs]\n",
    "\n",
    "    def __call__(self, query: str, k: Optional[int] = None) -> List[dotdict]:\n",
    "        return self.forward(query_or_queries=query, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = LLMFactory.get_llm_client(llm_model=rag_config[\"llm\"][\"model\"], stream=rag_config[\"llm\"][\"stream\"])\n",
    "retriever_client = RetrieverFactory.get_retriever_client(retrieval_method=rag_config[\"retrieval\"][\"retrieval_method\"], llm_client=llm_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4o'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_client.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<rag.retrievers.TopKRetriever at 0x1692ffb10>]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_client.retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rag.reranker.Reranker at 0x169555dd0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_client.reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "\n",
    "retriever = RetrieverWrapper(retrievers=retriever_client.retrievers,\n",
    "                             reranker=retriever_client.reranker,\n",
    "                             db=db,\n",
    "                             k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=llm, rm=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve = dspy.Retrieve(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-23 16:37:32,910 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-23 16:37:32,953 - rag.reranker - INFO - Reranking 3 documents...\n",
      "2024-08-23 16:37:33,210 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2024-08-23 16:37:33,215 - rag.reranker - INFO - Finished reranking 3 documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    passages=[\"Familienzulagen sind einmalige oder periodische Geldleistungen, die ausgerichtet werden, um die finanzielle Belastung durch ein oder mehrere Kinder teilweise auszugleichen. Sie umfassen Kinder- und Ausbildungszulagen sowie die von einzelnen Kantonen vorgesehene Geburts- und Adoptionszulage.\\nAngestellte beantragen Familienzulagen über Ihren Arbeitgeber. Dieser leitet das ausgefüllte Anmeldeformular der Familienausgleichskasse weiter, welche die Anmeldung prüft und ihren Entscheid wiederum dem Arbeitgeber mitteilt.\\nInhaltsverzeichnis\\n- Welche Arten von Familienzulagen werden ausgerichtet?\\n- Wie hoch sind die Familienzulagen?\\n- Werden die Zulagen nach dem Wohnkanton oder dem Arbeitskanton bestimmt?\\n- Wer hat Anspruch auf Familienzulagen?\\n- Welcher Elternteil bezieht die Familienzulagen?\\n- Diagramm: Wer hat Anspruch auf Familienzulagen?\\n- Wie können Sie Ihren Anspruch auf Familienzulagen bei der Familienausgleichskasse der EAK (FAK-EAK) geltend machen?\\n- Wie können Sie einen bisherigen Anspruch auf Ausbildungszulagen verlängern?\\n- Wie werden die Familienzulagen der Familienausgleichskasse der EAK ausgerichtet?\\n- Weiterführende Informationen\\nWelche Arten von Familienzulagen werden ausgerichtet?\\nSofern die jeweiligen Voraussetzungen erfüllt sind, werden gemäss dem Bundesgesetz über die Familienzulagen (FamZG) pro Kind und Monat folgende Familienzulagen ausgerichtet:\\nZulagenart | Anspruchsdauer |\\n---|---|\\nKinderzulage | Kinder bis 16 Jahre |\\nZulage für erwerbsunfähige Kinder | Kinder ab 16 bis 20 Jahre |\\nAusbildungszulage | Jugendliche in nachobligatorischer Ausbildung ab 15 bis 25 Jahre* |\\nGeburts- oder Adoptionszulagen | Einmalige Leistung bei Geburt oder Adoption |\\n*Seit dem 1. August 2020 (Art. 3 Abs. 1 Bst. b FamZG): ab Ausbildungsbeginn, sofern 15-jährig und obligatorische Schulzeit beendet\\nWie hoch sind die Familienzulagen?\\nDas Bundesgesetz über die Familienzulagen (FamZG) schreibt die untenstehenden Mindestbeträge vor. Die Kantone können höhere Leistungen vorsehen.\\nZulagenart | Mindestansatz nach FamZG |\\n---|---|\\nKinderzulage (KiZu) | CHF 200 |\\nZulage für erwerbsunfähige Kinder | CHF 200 |\\nAusbildungszulage (AuZu) | CHF 250 |\\nGeburts- oder Adoptionszulagen (GeZu/AZu) |\\nNach kantonalen Regelungen |\\nWerden die Zulagen nach dem Wohnkanton oder dem Arbeitskanton bestimmt?\\nDie Familienzulagen werden nach dem Erwerbsortprinzip ausgerichtet.\\nDiese Landkarte zeigt den im jeweiligen Kanton aktuell geltenden Betrag der Kinderzulage\\nDiese Landkarte zeigt den im jeweiligen Kanton aktuell geltenden Betrag der Ausbildungszulage\\nDiese Landkarte zeigt den im jeweiligen Kanton aktuell geltenden Betrag der Geburts- und Adoptionszulage\\nWer hat Anspruch auf Familienzulagen?\\nSie haben Anspruch auf Familienzulagen, sofern Sie ein jährliches Erwerbseinkommen von mindestens CHF 7'350 erzielen (entspricht dem halbem jährlichen Betrag der minimalen vollen Altersrente der AHV).\\nAnspruch auf Familienzulagen haben:\\nArbeitnehmer und Selbständigerwerbende; Nichterwerbstätige (Arbeitnehmer und Selbständigerwerbende, welche das für den Bezug der Familienzulagen notwendige Mindesteinkommen nicht erreichen, gelten ebenfalls als Nichterwerbstätige).\\nFür welche Kinder besteht Anspruch auf Familienzulagen?\\nKindsverhältnis | Voraussetzungen |\\n---|---|\\nEigene Kinder | Leibliche oder adoptierte Kinder, unabhängig davon, ob Sie als Eltern verheiratet sind. |\\nStiefkinder | Stiefkinder, die überwiegend im Haushalt des Stiefelternteils leben oder bis zur Mündigkeit lebten. |\\nPflegekinder | Pflegekinder, die Sie unentgeltlich zur dauernden Pflege und Erziehung aufgenommen haben. |\\nKinder des eingetragenen Partners | gleichgeschlechtliche Partnerschaft |\\nGeschwister und Enkelkinder | Geschwister und Enkelkinder für deren Unterhalt Sie überwiegend aufkommen |\\nFalls Ihre Kinder im Ausland wohnen, gelten spezielle Bestimmungen für den Anspruch auf Familienzulagen. In die Länder der EU und EFTA werden die Familienzulagen (mit Ausnahme der Geburts- und Adoptionszulagen) an Staatsangehörige dieser Länder exportiert. Wenn der andere Elternteil und die Kinder in einem Staat der EU oder EFTA leben, ist das Erwerbsortsprinzip massgebend für die Ausrichtung der Familienzulagen. Sind die Familienzulagenleistungen im nachrangig zuständigen Staat höher, so kann der in diesem Staat erwerbstätige Elternteil die Auszahlung des Differenzbetrages beantragen. Für die Arbeitnehmer, die von ihrem Arbeitgeber mit Sitz in der Schweiz ins Ausland geschickt werden, werden die Familienzulagen exportiert.\\nWelcher Elternteil bezieht die Familienzulagen?\\nFür jedes Kind darf nur eine Zulage derselben Art ausgerichtet werden. Erfüllen mehrere Personen die Voraussetzungen für den Bezug von Familienzulagen, richtet sich der Anspruch nach der folgenden Rangordnung (Anspruchskonkurrenz - Art. 7, Abs. 1 FamZG):\\nBeschreibung des Diagramms: Wer hat Anspruch auf Familienzulagen?\\nDas Baumdiagramm ist eine graphische Darstellung von Artikel 7, Absatz 1 des Bundesgesetzes über die Familienzulagen (FamZG). In diesem wird anhand von verschiedenen aufeinanderfolgenden Prinzipien eine abgestufte Reihenfolge definiert, welche für die Ermittlung des erstanspruchsberechtigten Elternteils angewendet wird.\\nDas erste Prinzip zur Ermittlung des erstanspruchsberechtigten Elternteils ist das Erwerbsprinzip. Wenn nur ein Elternteil erwerbstätig ist, so ist Buchstabe a von Artikel 7, Abs. 1 des FamZG massgebend. Wenn beide Elternteile erwerbstätig sind, geht es weiter zur nächsten Stufe.\\nDas nächste Prinzip ist das elterliche-Sorgerechtsprinzip. Wenn nur ein Elternteil Inhaber der elterlichen Sorge ist, so ist Buchstabe b von Artikel 7, Abs. 1 des FamZG massgebend. Üben die Eltern das Sorgerecht jedoch gemeinsam aus, geht es weiter zur nächsten Stufe.\\nDas nächste Prinzip ist das Obhutsprinzip. Wenn die Eltern getrennt sind und das Kind überwiegend bei einem Elternteil lebt, so ist Buchstabe c von Artikel 7, Abs. 1 des FamZG massgebend. Leben die Eltern jedoch mit dem Kind im gemeinsamen Haushalt, geht es weiter zur nächsten Stufe.\\nDas nächste Prinzip ist das Wohnsitzprinzip. Wenn nur ein Elternteil seinen Arbeitsort im Wohnsitzkanton des Kindes hat, so ist Buchstabe d von Artikel 7, Abs. 1 des FamZG massgebend. Haben beide Elternteile ihren Arbeitsort im oder beide ausserhalb des Wohnsitzkantons des Kindes, geht es weiter zur nächsten Stufe.\\nDas nächste Prinzip ist das Einkommensprinzip. Wenn nur ein Elternteil unselbständig erwerbstätig ist, so ist Buchstabe e von Artikel 7, Abs. 1 des FamZG massgebend. Sind beide Elternteile unselbständig erwerbstätig, so ist der Elternteil mit dem höheren AHV-pflichtigen Einkommen erstanspruchsberechtigt. Sind beide Elternteile Selbständig-erwerbend, so ist gemäss Buchstabe f von Artikel 7, Abs. 1 des FamZG der Elternteil mit dem höheren AHV-pflichtigen Einkommen erstanspruchsberechtigt.\\nEs können mehr als zwei Personen für das gleiche Kind anspruchsberechtigt sein. Anspruch auf eine allfällige Differenzzahlung hat aber ausschliesslich die zweitanspruchsberechtigte Person. Die Differenzzahlung muss durch die zweitanspruchsberechtigte Person bzw. dessen Arbeitgeber bei der zuständigen Familienausgleichskasse beantragt werden.\\nWie können Sie Ihren Anspruch auf Familienzulagen bei der Familienausgleichskasse der EAK (FAK-EAK) geltend machen?\\nUm einen Antrag auf Familienzulagen zu stellen, wenden Sie sich bitte an Ihren Arbeitgeber. Er wird Ihnen das Anmeldeformular sowie alle weiteren notwendigen Unterlagen aushändigen.\\nDas vollständig ausgefüllte Formular übermitteln Sie anschliessend zusammen mit sämtlichen benötigten Beilagen an den für Sie zuständigen Personaldienst.\\nWie können Sie einen bisherigen Anspruch auf Ausbildungszulagen verlängern?\\nSie reichen Ihrem Arbeitgeber eine entsprechende Bestätigung (z.B. eine Kopie des Lehr- oder Praktikumsvertrags bzw. eine Schul- Studien- oder Immatrikulationsbestätigung etc.) ein. Als Ausbildungsbestätigung gelten jegliche Dokumente, welche von einer Ausbildungsanstalt für ein bestimmtes Kind (muss namentlich erwähnt sein) ausgestellt werden und Angaben über Beginn und Ende sowie die Art der Ausbildung enthalten.\\nWie werden die Familienzulagen der Familienausgleichskasse der EAK ausgerichtet?\\nSowohl für die Bestätigung des erstmaligen Anspruchs als auch für sämtliche Änderungen eines bestehenden Familienzulagenanspruchs erstellt die Familienausgleichskasse einen Entscheid zuhanden des Arbeitgebers. Die Auszahlung der Familienzulagen erfolgt anschliessend durch den Arbeitgeber zusammen mit dem Lohn.\\nJede Gutheissung, Ablehnung, Mutation und/oder Rückforderung von Familienzulagen wird dem Arbeitgeber durch die Familienausgleichskasse schriftlich mitgeteilt.\\nWerden die Familienzulagen nicht für die Bedürfnisse des Kindes verwendet, kann dessen gesetzlicher Vertreter oder das mündige Kind selbst ein Gesuch um Drittauszahlung beim Arbeitgeber des Elternteils stellen, welcher bisher die Familienzulagen bezogen hat.\\nWeiterführende Informationen\\nLinks\\nLetzte Änderung 08.05.2024\", 'Die Familienzulagen sollen die Kosten, die den Eltern durch den Unterhalt ihrer Kinder entstehen, teilweise ausgleichen. Sie umfassen Kinder- und Ausbildungszulagen sowie die von einzelnen Kantonen eingeführten Geburts- und Adoptionszulagen.\\nDie Familienausgleichskassen entscheiden (auf Antrag) über die Ausrichtung der Familienzulagen. Sie rechnen mit den Arbeitgebern und Selbständigerwerbenden die Familienzulagen und die Beiträge ab. Die Auszahlung der Familienzulagen für Arbeitnehmer erfolgt in der Regel zusammen mit dem Lohn durch die Arbeitgeber.\\nDie Familienzulagen in der Landwirtschaft sowie die Familienzulagen für Nichterwerbstätige werden über die kantonalen Familienausgleichskassen und ihre Zweigstellen abgewickelt.\\nLetzte Änderung 29.05.2018', 'Auf einen Blick\\n\\nDie Familienzulagen sollen die Kosten, die den Eltern durch den Unterhalt ihrer Kinder entstehen, teilweise ausgleichen. Sie umfassen die Kinder- und Ausbildungszulagen sowie die von einzelnen Kantonen eingeführten Geburts- und Adoptionszulagen.\\nNach dem Bundesgesetz über die Familienzulagen (FamZG) werden in allen Kantonen mindestens die folgenden Zulagen pro Kind und Monat ausge -\\nrichtet:\\n• eine Kinderzulage von 200 Franken pro Kind und Monat;\\n• eine Ausbildungszulage von 250 Franken pro Kind und Monat.\\nAnspruch auf Familienzulagen haben alle Arbeitnehmenden, alle Selbstän -\\ndigerwerbenden sowie Nichterwerbstätige mit bescheidenen Einkommen und arbeitslose Mütter, die eine Mutterschaftsentschädigung beziehen, ohne Einkommensgrenze. Für die Beschäftigten in der Landwirtschaft gilt eine Sonderregelung (siehe Merkblatt 6.09 – Familienzulagen in der Land -\\nwirtschaft ).\\nDieses Merkblatt informiert Arbeitnehmende, Selbständigerwerbende und Nichterwerbstätige mit Kindern sowie Arbeitgebende.']\n",
       ")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#docs = retrieve(query_or_queries=dev_example.question)\n",
    "docs = retrieve(query_or_queries=[\"Welche Arten von Familienzulagen werden ausgerichtet?\"])\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs.passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last line above, we configure **DSPy** to use the turbo LM and the ColBERTv2 retriever (over Wikipedia 2017 abstracts) by default. This will be easy to overwrite for local parts of our programs if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A word on the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build your own **DSPy programs** for various tasks, e.g., question answering, information extraction, or text-to-SQL.\n",
    "\n",
    "Whatever the task, the general workflow is:\n",
    "\n",
    "1. **Collect a little bit of data.** Define examples of the inputs and outputs of your program (e.g., questions and their answers). This could just be a handful of quick examples you wrote down. If large datasets exist, the more the merrier!\n",
    "1. **Write your program.** Define the modules (i.e., sub-tasks) of your program and the way they should interact together to solve your task.\n",
    "1. **Define some validation logic.** What makes for a good run of your program? Maybe the answers need to have a certain length or stick to a particular format? Specify the logic that checks that.\n",
    "1. **Compile!** Ask **DSPy** to _compile_ your program using your data. The compiler will use your data and validation logic to optimize your program (e.g., prompts and modules) so it's efficient and effective!\n",
    "1. **Iterate.** Repeat the process by improving your data, program, validation, or by using more advanced features of the **DSPy** compiler.\n",
    "\n",
    "Let's now see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2] Task Examples\n",
    "\n",
    "**DSPy** accommodates a wide variety of applications and tasks. **In this intro notebook, we will work on the example task of multi-hop question answering (QA).**\n",
    "\n",
    "Other notebooks and tutorials will present different tasks. Now, let us load a tiny sample from the HotPotQA multi-hop dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.datasets import HotPotQA\n",
    "\n",
    "# Load the dataset.\n",
    "dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n",
    "\n",
    "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
    "trainset = [x.with_inputs('question') for x in dataset.train]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev]\n",
    "\n",
    "len(trainset), len(devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': 'At My Window was released by which American singer-songwriter?', 'answer': 'John Townes Van Zandt'}) (input_keys=None),\n",
       " Example({'question': 'which  American actor was Candace Kita  guest starred with ', 'answer': 'Bill Murray'}) (input_keys=None),\n",
       " Example({'question': 'Which of these publications was most recently published, Who Put the Bomp or Self?', 'answer': 'Self'}) (input_keys=None),\n",
       " Example({'question': 'The Victorians - Their Story In Pictures is a documentary series written by an author born in what year?', 'answer': '1950'}) (input_keys=None),\n",
       " Example({'question': 'Which magazine has published articles by Scott Shaw, Tae Kwon Do Times or Southwest Art?', 'answer': 'Tae Kwon Do Times'}) (input_keys=None),\n",
       " Example({'question': 'In what year was the club founded that played Manchester City in the 1972 FA Charity Shield', 'answer': '1874'}) (input_keys=None),\n",
       " Example({'question': 'Which is taller, the Empire State Building or the Bank of America Tower?', 'answer': 'The Empire State Building'}) (input_keys=None),\n",
       " Example({'question': 'Which American actress who made their film debut in the 1995 teen drama \"Kids\" was the co-founder of Voto Latino?', 'answer': 'Rosario Dawson'}) (input_keys=None),\n",
       " Example({'question': 'Tombstone stared an actor born May 17, 1955 known as who?', 'answer': 'Bill Paxton'}) (input_keys=None),\n",
       " Example({'question': 'What is the code name for the German offensive that started this Second World War engagement on the Eastern Front (a few hundred kilometers from Moscow) between Soviet and German forces, which included 102nd Infantry Division?', 'answer': 'Operation Citadel'}) (input_keys=None),\n",
       " Example({'question': 'Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?', 'answer': 'Kerry Condon'}) (input_keys=None),\n",
       " Example({'question': 'Which company distributed this 1977 American animated film produced by Walt Disney Productions for which Sherman Brothers wrote songs?', 'answer': 'Buena Vista Distribution'}) (input_keys=None),\n",
       " Example({'question': 'Samantha Cristoforetti and Mark Shuttleworth are both best known for being first in their field to go where? ', 'answer': 'space'}) (input_keys=None),\n",
       " Example({'question': 'Having the combination of excellent foot speed and bat speed helped Eric Davis, create what kind of outfield for the Los Angeles Dodgers? ', 'answer': '\"Outfield of Dreams\"'}) (input_keys=None),\n",
       " Example({'question': 'Which Pakistani cricket umpire who won 3 consecutive ICC umpire of the year awards in 2009, 2010, and 2011 will be in the ICC World Twenty20?', 'answer': 'Aleem Sarwar Dar'}) (input_keys=None),\n",
       " Example({'question': 'The Organisation that allows a community to influence their operation or use and to enjoy the benefits arisingwas founded in what year?', 'answer': '2010'}) (input_keys=None),\n",
       " Example({'question': '\"Everything Has Changed\" is a song from an album released under which record label ?', 'answer': 'Big Machine Records'}) (input_keys=None),\n",
       " Example({'question': 'Who is older, Aleksandr Danilovich Aleksandrov or Anatoly Fomenko?', 'answer': 'Aleksandr Danilovich Aleksandrov'}) (input_keys=None),\n",
       " Example({'question': 'On the coast of what ocean is the birthplace of Diogal Sakho?', 'answer': 'Atlantic'}) (input_keys=None),\n",
       " Example({'question': 'This American guitarist best known for her work with the Iron Maidens is an ancestor of a composer who was known as what?', 'answer': 'The Waltz King'}) (input_keys=None)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'copy',\n",
       " 'get',\n",
       " 'inputs',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'labels',\n",
       " 'toDict',\n",
       " 'values',\n",
       " 'with_inputs',\n",
       " 'without']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': 'At My Window was released by which American singer-songwriter?', 'answer': 'John Townes Van Zandt'}) (input_keys={'question'}),\n",
       " Example({'question': 'which  American actor was Candace Kita  guest starred with ', 'answer': 'Bill Murray'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which of these publications was most recently published, Who Put the Bomp or Self?', 'answer': 'Self'}) (input_keys={'question'}),\n",
       " Example({'question': 'The Victorians - Their Story In Pictures is a documentary series written by an author born in what year?', 'answer': '1950'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which magazine has published articles by Scott Shaw, Tae Kwon Do Times or Southwest Art?', 'answer': 'Tae Kwon Do Times'}) (input_keys={'question'}),\n",
       " Example({'question': 'In what year was the club founded that played Manchester City in the 1972 FA Charity Shield', 'answer': '1874'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which is taller, the Empire State Building or the Bank of America Tower?', 'answer': 'The Empire State Building'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which American actress who made their film debut in the 1995 teen drama \"Kids\" was the co-founder of Voto Latino?', 'answer': 'Rosario Dawson'}) (input_keys={'question'}),\n",
       " Example({'question': 'Tombstone stared an actor born May 17, 1955 known as who?', 'answer': 'Bill Paxton'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the code name for the German offensive that started this Second World War engagement on the Eastern Front (a few hundred kilometers from Moscow) between Soviet and German forces, which included 102nd Infantry Division?', 'answer': 'Operation Citadel'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?', 'answer': 'Kerry Condon'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which company distributed this 1977 American animated film produced by Walt Disney Productions for which Sherman Brothers wrote songs?', 'answer': 'Buena Vista Distribution'}) (input_keys={'question'}),\n",
       " Example({'question': 'Samantha Cristoforetti and Mark Shuttleworth are both best known for being first in their field to go where? ', 'answer': 'space'}) (input_keys={'question'}),\n",
       " Example({'question': 'Having the combination of excellent foot speed and bat speed helped Eric Davis, create what kind of outfield for the Los Angeles Dodgers? ', 'answer': '\"Outfield of Dreams\"'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which Pakistani cricket umpire who won 3 consecutive ICC umpire of the year awards in 2009, 2010, and 2011 will be in the ICC World Twenty20?', 'answer': 'Aleem Sarwar Dar'}) (input_keys={'question'}),\n",
       " Example({'question': 'The Organisation that allows a community to influence their operation or use and to enjoy the benefits arisingwas founded in what year?', 'answer': '2010'}) (input_keys={'question'}),\n",
       " Example({'question': '\"Everything Has Changed\" is a song from an album released under which record label ?', 'answer': 'Big Machine Records'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who is older, Aleksandr Danilovich Aleksandrov or Anatoly Fomenko?', 'answer': 'Aleksandr Danilovich Aleksandrov'}) (input_keys={'question'}),\n",
       " Example({'question': 'On the coast of what ocean is the birthplace of Diogal Sakho?', 'answer': 'Atlantic'}) (input_keys={'question'}),\n",
       " Example({'question': 'This American guitarist best known for her work with the Iron Maidens is an ancestor of a composer who was known as what?', 'answer': 'The Waltz King'}) (input_keys={'question'})]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = [Example()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "trainset = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just loaded `trainset` (20 examples) and `devset` (50 examples). Each example in our **training set** contains just a **question** and its (human-annotated) **answer**.\n",
    "\n",
    "**DSPy** typically requires very minimal labeling. Whereas your pipeline may involve six or seven complex steps, you only need labels for the initial question and the final answer. **DSPy** will bootstrap any intermediate labels needed to support your pipeline. If you change your pipeline in any way, the data bootstrapped will change accordingly!\n",
    "\n",
    "Now, let's look at some data examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_example = trainset[0]\n",
    "print(f\"Question: {train_example.question}\")\n",
    "print(f\"Answer: {train_example.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples in the **dev set** contain a third field, namely, **titles** of relevant Wikipedia articles. This is not essential but, for the sake of this intro, it'll help us get a sense of how well our programs are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the nationality of the chef and restaurateur featured in Restaurant: Impossible?\n",
      "Answer: English\n",
      "Relevant Wikipedia Titles: {'Restaurant: Impossible', 'Robert Irvine'}\n"
     ]
    }
   ],
   "source": [
    "dev_example = devset[18]\n",
    "print(f\"Question: {dev_example.question}\")\n",
    "print(f\"Answer: {dev_example.answer}\")\n",
    "print(f\"Relevant Wikipedia Titles: {dev_example.gold_titles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the raw data, we'd applied `x.with_inputs('question')` to each example to tell **DSPy** that our input field in each example will be just `question`. Any other fields are labels or metadata that are not given to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"For this dataset, training examples have input keys {train_example.inputs().keys()} and label keys {train_example.labels().keys()}\")\n",
    "print(f\"For this dataset, dev examples have input keys {dev_example.inputs().keys()} and label keys {dev_example.labels().keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there's nothing special about the HotPotQA dataset: it's just a list of examples.\n",
    "\n",
    "You can define your own examples as below. A future notebook will guide you through creating your own data in unusual or data-scarce settings, which is a context where **DSPy** excels.\n",
    "\n",
    "```\n",
    "dspy.Example(field1=value, field2=value2, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3] Building Blocks\n",
    "\n",
    "In **DSPy**, we will maintain a clean separation between **defining your modules in a declarative way** and **calling them in a pipeline to solve the task**.\n",
    "\n",
    "This allows you to focus on the information flow of your pipeline. **DSPy** will then take your program and automatically optimize **how to prompt** (or finetune) LMs **for your particular pipeline** so it works well.\n",
    "\n",
    "If you have experience with PyTorch, you can think of DSPy as the PyTorch of the foundation model space. Before we see this in action, let's first understand some key pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the Language Model: **Signatures** & **Predictors**\n",
    "\n",
    "Every call to the LM in a **DSPy** program needs to have a **Signature**.\n",
    "\n",
    "A signature consists of three simple elements:\n",
    "\n",
    "- A minimal description of the sub-task the LM is supposed to solve.\n",
    "- A description of one or more input fields (e.g., input question) that we will give to the LM.\n",
    "- A description of one or more output fields (e.g., the question's answer) that we will expect from the LM.\n",
    "\n",
    "Let's define a simple signature for basic question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `BasicQA`, the docstring describes the sub-task here (i.e., answering questions). Each `InputField` or `OutputField` can optionally contain a description `desc` too. When it's not given, it's inferred from the field's name (e.g., `question`).\n",
    "\n",
    "Notice that there isn't anything special about this signature in **DSPy**. We can just as easily define a signature that takes a long snippet from a PDF and outputs structured information, for instance.\n",
    "\n",
    "Anyway, now that we have a signature, let's define and use a **Predictor**. A predictor is a module that knows how to use the LM to implement a signature. Importantly, predictors can **learn** to fit their behavior to the task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predictor.\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "\n",
    "# Call the predictor on a particular input.\n",
    "pred = generate_answer(question=dev_example.question)\n",
    "\n",
    "# Print the input and the prediction.\n",
    "print(f\"Question: {dev_example.question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we asked the predictor about the the chef featured in \"Restaurant: Impossible\". The model outputs an answer (\"American\").\n",
    "\n",
    "For visibility, we can inspect how this extremely basic predictor implemented our signature. Let's inspect the history of our LM (**turbo**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It happens that this chef is both British and American, but we have no way of knowing if the model just guessed \"American\" because it's a common answer. In general, adding **retrieval** and **learning** will help the LM be more factual, and we'll explore this in a minute!\n",
    "\n",
    "But before we do that, how about we _just_ change the predictor? It would be nice to allow the model to elicit a chain of thought along with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predictor. Notice we're just changing the class. The signature BasicQA is unchanged.\n",
    "generate_answer_with_chain_of_thought = dspy.ChainOfThought(BasicQA)\n",
    "\n",
    "# Call the predictor on the same input.\n",
    "pred = generate_answer_with_chain_of_thought(question=dev_example.question)\n",
    "\n",
    "# Print the input, the chain of thought, and the prediction.\n",
    "print(f\"Question: {dev_example.question}\")\n",
    "print(f\"Thought: {pred.rationale.split('.', 1)[1].strip()}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is indeed a better answer: the model figures out that the chef in question is **Robert Irvine** and correctly identifies that he's British.\n",
    "\n",
    "These predictors (`dspy.Predict` and `dspy.ChainOfThought`) can be applied to _any_ signature. As we'll see below, they can also be optimized to learn from your data and validation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the Retrieval Model\n",
    "\n",
    "Using the retriever is pretty simple. A module `dspy.Retrieve(k)` will search for the top-`k` passages that match a given query.\n",
    "\n",
    "By default, this will use the retriever we configured at the top of this notebook, namely, ColBERTv2 over a Wikipedia index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve = dspy.Retrieve(k=3)\n",
    "topK_passages = retrieve(dev_example.question).passages\n",
    "\n",
    "print(f\"Top {retrieve.k} passages for question: {dev_example.question} \\n\", '-' * 30, '\\n')\n",
    "\n",
    "for idx, passage in enumerate(topK_passages):\n",
    "    print(f'{idx+1}]', passage, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to any other queries you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve(\"When was the first FIFA World Cup held?\").passages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4] Program 1: Basic Retrieval-Augmented Generation (“RAG”)\n",
    "\n",
    "Let's define our first complete program for this task. We'll build a retrieval-augmented pipeline for answer generation.\n",
    "\n",
    "Given a question, we'll search for the top-3 passages in Wikipedia and then feed them as context for answer generation.\n",
    "\n",
    "Let's start by defining this signature: `context, question --> answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now let's define the actual program. This is a class that inherits from `dspy.Module`.\n",
    "\n",
    "It needs two methods:\n",
    "\n",
    "- The `__init__` method will simply declare the sub-modules it needs: `dspy.Retrieve` and `dspy.ChainOfThought`. The latter is defined to implement our `GenerateAnswer` signature.\n",
    "- The `forward` method will describe the control flow of answering the question using the modules we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compiling the RAG program\n",
    "\n",
    "Having defined this program, let's now **compile** it. Compiling a program will update the parameters stored in each module. In our setting, this is primarily in the form of collecting and selecting good demonstrations for inclusion in your prompt(s).\n",
    "\n",
    "Compiling depends on three things:\n",
    "\n",
    "1. **A training set.** We'll just use our 20 question–answer examples from `trainset` above.\n",
    "1. **A metric for validation.** We'll define a quick `validate_context_and_answer` that checks that the predicted answer is correct. It'll also check that the retrieved context does actually contain that answer.\n",
    "1. **A specific teleprompter.** The **DSPy** compiler includes a number of **teleprompters** that can optimize your programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teleprompters:** Teleprompters are powerful optimizers that can take any program and learn to bootstrap and select effective prompts for its modules. Hence the name, which means \"prompting at a distance\".\n",
    "\n",
    "Different teleprompters offer various tradeoffs in terms of how much they optimize cost versus quality, etc. We will use a simple default `BootstrapFewShot` in this notebook.\n",
    "\n",
    "\n",
    "_If you're into analogies, you could think of this as your training data, your loss function, and your optimizer in a standard DNN supervised learning setup. Whereas SGD is a basic optimizer, there are more sophisticated (and more expensive!) ones like Adam or RMSProp._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Validation logic: check that the predicted answer is correct.\n",
    "# Also check that the retrieved context does actually contain that answer.\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM\n",
    "\n",
    "# Set up a basic teleprompter, which will compile our RAG program.\n",
    "teleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n",
    "\n",
    "# Compile!\n",
    "compiled_rag = teleprompter.compile(RAG(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've compiled our RAG program, let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask any question you like to this simple RAG program.\n",
    "my_question = \"What castle did David Gregory inherit?\"\n",
    "\n",
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "pred = compiled_rag(my_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "print(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. How about we inspect the last prompt for the LM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we haven't written any of this detailed demonstrations, we see that **DSPy** was able to bootstrap this 3,000 token prompt for **3-shot retrieval augmented generation with hard negative passages and chain of thought** from our extremely simple program.\n",
    "\n",
    "This illustrates the power of composition and learning. Of course, this was just generated by a particular teleprompter, which may or may not be perfect in each setting. As you'll see in **DSPy**, there is a large but systematic space of options you have to optimize and validate the quality and cost of your programs.\n",
    "\n",
    "If you're so inclined, you can easily inspect the learned objects themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameter in compiled_rag.named_predictors():\n",
    "    print(name)\n",
    "    print(parameter.demos[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating the Answers\n",
    "\n",
    "We can now evaluate our `compiled_rag` program on the dev set. Of course, this tiny set is _not_ meant to be a reliable benchmark, but it'll be instructive to use it for illustration.\n",
    "\n",
    "For a start, let's evaluate the accuracy (exact match) of the predicted answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
    "evaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "# Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate_on_hotpotqa(compiled_rag, metric=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating the Retrieval\n",
    "\n",
    "It may also be instructive to look at the accuracy of retrieval. There are multiple ways to do this. Often, we can just check whether the retrieved passages contain the answer.\n",
    "\n",
    "That said, since our dev set includes the gold titles that should be retrieved, we can just use these here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_passages_retrieved(example, pred, trace=None):\n",
    "    gold_titles = set(map(dspy.evaluate.normalize_text, example['gold_titles']))\n",
    "    found_titles = set(map(dspy.evaluate.normalize_text, [c.split(' | ')[0] for c in pred.context]))\n",
    "\n",
    "    return gold_titles.issubset(found_titles)\n",
    "\n",
    "compiled_rag_retrieval_score = evaluate_on_hotpotqa(compiled_rag, metric=gold_passages_retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this simple `compiled_rag` program is able to answer a decent fraction of the questions correctly (on this tiny set, over 40%), the quality of retrieval is much lower.\n",
    "\n",
    "This potentially suggests that the LM is often relying on the knowledge it memorized during training to answer questions. To address this weak retrieval, let's explore a second program that involves more advanced search behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5] Program 2: Multi-Hop Search (“Baleen”)\n",
    "\n",
    "From exploring the harder questions in the training/dev sets, it becomes clear that a single search query is often not enough for this task. For instance, this can be seen when a question ask about, say, the birth city of the writer of \"Right Back At It Again\". A search query identifies the author correctly as \"Jeremy McKinnon\", but it wouldn't figure out when he was born.\n",
    "\n",
    "The standard approach for this challenge in the retrieval-augmented NLP literature is to build multi-hop search systems, like GoldEn (Qi et al., 2019) and Baleen (Khattab et al., 2021). These systems read the retrieved results and then generate additional queries to gather additional information if necessary. Using **DSPy**, we can easily simulate such systems in a few lines of code.\n",
    "\n",
    "\n",
    "We'll still use the `GenerateAnswer` signature from the RAG implementation above. All we need now is a **signature** for the \"hop\" behavior: taking some partial context and a question, generate a search query to find missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    query = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We could have written `context = GenerateAnswer.signature.context` to avoid duplicating the description of the `context` field.\n",
    "\n",
    "Now, let's define the program itself `SimplifiedBaleen`. There are many possible ways to implement this, but we'll keep this version down to the key elements for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate\n",
    "\n",
    "class SimplifiedBaleen(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_query[hop](context=context, question=question).query\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=pred.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `__init__` method defines a few key sub-modules:\n",
    "\n",
    "- **generate_query**: For each hop, we will have one `dspy.ChainOfThought` predictor with the `GenerateSearchQuery` signature.\n",
    "- **retrieve**: This module will do the actual search, using the generated queries.\n",
    "- **generate_answer**: This `dspy.Predict` module will be used after all the search steps. It has a `GenerateAnswer`, to actually produce an answer.\n",
    "\n",
    "The `forward` method uses these sub-modules in simple control flow.\n",
    "\n",
    "1. First, we'll loop up to `self.max_hops` times.\n",
    "1. In each iteration, we'll generate a search query using the predictor at `self.generate_query[hop]`.\n",
    "1. We'll retrieve the top-k passages using that query.\n",
    "1. We'll add the (deduplicated) passages to our accumulator of `context`.\n",
    "1. After the loop, we'll use `self.generate_answer` to produce an answer.\n",
    "1. We'll return a prediction with the retrieved `context` and predicted `answer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect the zero-shot version of the Baleen program\n",
    "\n",
    "We will also compile this program shortly. But, before that, we can try it out in a \"zero-shot\" setting (i.e., without any compilation).\n",
    "\n",
    "Using a program in zero-shot (uncompiled) setting doesn't mean that quality will be bad. It just means that we're bottlenecked directly by the reliability of the underlying LM to understand our sub-tasks from minimal instructions.\n",
    "\n",
    "This is often just fine when using the most expensive/powerful models (e.g., GPT-4) on the easiest and most standard tasks (e.g., answering simple questions about popular entities).\n",
    "\n",
    "However, a zero-shot approach quickly falls short for more specialized tasks, for novel domains/settings, and for more efficient (or open) models. **DSPy** can help you in all of these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask any question you like to this simple RAG program.\n",
    "my_question = \"How many storeys are in the castle that David Gregory inherited?\"\n",
    "\n",
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "uncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program\n",
    "pred = uncompiled_baleen(my_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "print(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the last **three** calls to the LM (i.e., generating the first hop's query, generating the second hop's query, and generating the answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compiling the Baleen program\n",
    "\n",
    "Now is the time to compile our multi-hop (`SimplifiedBaleen`) program.\n",
    "\n",
    "We will first define our validation logic, which will simply require that:\n",
    "\n",
    "- The predicted answer matches the gold answer.\n",
    "- The retrieved context contains the gold answer.\n",
    "- None of the generated queries is rambling (i.e., none exceeds 100 characters in length).\n",
    "- None of the generated queries is roughly repeated (i.e., none is within 0.8 or higher F1 score of earlier queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_context_and_answer_and_hops(example, pred, trace=None):\n",
    "    if not dspy.evaluate.answer_exact_match(example, pred): return False\n",
    "    if not dspy.evaluate.answer_passage_match(example, pred): return False\n",
    "\n",
    "    hops = [example.question] + [outputs.query for *_, outputs in trace if 'query' in outputs]\n",
    "\n",
    "    if max([len(h) for h in hops]) > 100: return False\n",
    "    if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))): return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did for RAG, we'll use one of the most basic teleprompters in **DSPy**, namely, `BootstrapFewShot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teleprompter = BootstrapFewShot(metric=validate_context_and_answer_and_hops)\n",
    "compiled_baleen = teleprompter.compile(SimplifiedBaleen(), teacher=SimplifiedBaleen(passages_per_hop=2), trainset=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating the Retrieval\n",
    "\n",
    "Earlier, it appeared like our simple RAG program was not very effective at finding all evidence required for answering each question. Is this resolved by the adding some extra steps in the `forward` function of `SimplifiedBaleen`? What about compiling, does it help for that? \n",
    "\n",
    "The answer for these questions is not always going to be obvious. However, **DSPy** makes it extremely easy to try many diverse approaches with minimal effort.\n",
    "\n",
    "Let's evaluate the quality of retrieval of our compiled and uncompiled Baleen pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_baleen_retrieval_score = evaluate_on_hotpotqa(uncompiled_baleen, metric=gold_passages_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_baleen_retrieval_score = evaluate_on_hotpotqa(compiled_baleen, metric=gold_passages_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"## Retrieval Score for RAG: {compiled_rag_retrieval_score}\")  # note that for RAG, compilation has no effect on the retrieval step\n",
    "print(f\"## Retrieval Score for uncompiled Baleen: {uncompiled_baleen_retrieval_score}\")\n",
    "print(f\"## Retrieval Score for compiled Baleen: {compiled_baleen_retrieval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! There might be something to this compiled, multi-hop program then. But this is far from all you can do: **DSPy** gives you a clean space of composable operators to deal with any shortcomings you see.\n",
    "\n",
    "We can inspect a few concrete examples. If we see failure causes, we can:\n",
    "\n",
    "1. Expand our pipeline by using additional sub-modules (e.g., maybe summarize after retrieval?)\n",
    "1. Modify our pipeline by using more complex logic (e.g., maybe we need to break out of the multi-hop loop if we found all information we need?) \n",
    "1. Refine our validation logic (e.g., maybe use a metric that use a second **DSPy** program to do the answer evaluation, instead of relying on strict string matching)\n",
    "1. Use a different teleprompter to optimize your pipeline more aggressively.\n",
    "1. Add more or better training examples!\n",
    "\n",
    "\n",
    "Or, if you really want, we can tweak the descriptions in the Signatures we use in your program to make them more precisely suited for their sub-tasks. This is akin to prompt engineering and should be a final resort, given the other powerful options that **DSPy** gives us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_baleen(\"How many storeys are in the castle that David Gregory inherited?\")\n",
    "turbo.inspect_history(n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_copilot",
   "language": "python",
   "name": "venv_copilot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80de7e36-c556-40df-8561-8ab63c15a086",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f632d-d146-483c-8bba-897ab0f555d7",
   "metadata": {},
   "source": [
    "- You need to run `docker-compose up` to initialize the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff5822-b351-4636-8112-b68ab89d78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from config.base_config import rag_config\n",
    "from rag.rag_processor import processor\n",
    "from rag.rag_processor import llm_client\n",
    "from rag.models import RAGRequest\n",
    "\n",
    "from indexing.pipelines.ahv import ahv_indexer\n",
    "from database.service import document_service\n",
    "from schemas.document import DocumentCreate\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84a3a2-09cd-49f4-8ce1-687ed1868975",
   "metadata": {},
   "source": [
    "### Define utilitary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbbed3d-fc78-450e-a710-631e1b2d603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_USER = os.environ.get(\"POSTGRES_USER\", None)\n",
    "POSTGRES_PASSWORD = os.environ.get(\"POSTGRES_PASSWORD\", None)\n",
    "POSTGRES_PORT = os.environ.get(\"POSTGRES_PORT\", None)\n",
    "POSTGRES_DB = os.environ.get(\"POSTGRES_DB\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2313bb1b-da2e-4725-8bb9-a8172430ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db():\n",
    "\n",
    "    DATABASE_URL = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@localhost:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "\n",
    "    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "    db = SessionLocal()\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c779d8-9530-4737-bb70-2ce9a99d2f89",
   "metadata": {},
   "source": [
    "### Setup config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d4e0584-cca5-49eb-a9af-b2f5790702b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd16427-c670-4545-818d-6a220ad4ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a7945-7011-491f-9fe3-65fafb30ab9f",
   "metadata": {},
   "source": [
    "### Connect to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5392d648-97e8-4340-b5dc-30a119530aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1b8964-c2ff-4b30-b686-3e354fcc1c3d",
   "metadata": {},
   "source": [
    "# Scraping/Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212d485-3123-415f-8cd3-42c16c988495",
   "metadata": {},
   "source": [
    "### GOAL: CHUNK PDFS BY SECTION\n",
    "    - FAMIILIENZULAGEN\n",
    "        - ANSPRUCH, UNTERSTELLUNG, etc.\n",
    "    - BEITRAGE:\n",
    "        - ...\n",
    "\n",
    "If doesn't work -> try recursive summarization -> BUT ARE SECTIONS EXCLUSIVE?\n",
    "\n",
    "Need to find sections for each PDF (manual task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86c471-a9fc-49aa-aed8-4a46afb866f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing.scraper import scraper\n",
    "from indexing.pipelines.ahv import AHVParser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from io import BytesIO\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import (\n",
    "    LTTextContainer,\n",
    "    LTChar,\n",
    "    LTTextLine,\n",
    "    LTTextLineHorizontal,\n",
    ")\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import resolve1\n",
    "import json\n",
    "from itertools import groupby\n",
    "\n",
    "def extract_urls(pdf_bytes):\n",
    "    \"\"\"\n",
    "    Extracts URLs from a PDF byte stream in the order they appear.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "\n",
    "    parser = PDFParser(BytesIO(pdf_bytes))\n",
    "    document = PDFDocument(parser)\n",
    "\n",
    "    # Iterate through pages to extract annotations\n",
    "    for page in PDFPage.create_pages(document):\n",
    "        annotations = page.annots\n",
    "        if annotations:\n",
    "            annotations = resolve1(annotations)\n",
    "            for annotation in annotations:\n",
    "                annotation_data = resolve1(annotation)\n",
    "                # Check if the annotation is a link\n",
    "                if annotation_data.get('Subtype').name == 'Link':\n",
    "                    action = annotation_data.get('A')\n",
    "                    if action and action.resolve().get('S').name == 'URI':\n",
    "                        uri = action.resolve().get('URI')\n",
    "                        if uri:\n",
    "                            urls.append(uri.decode('utf-8'))\n",
    "    return urls\n",
    "\n",
    "def process_paragraph(chars, url_list, url_index):\n",
    "    \"\"\"\n",
    "    Processes a paragraph, replacing italic text with markdown links.\n",
    "\n",
    "    Parameters:\n",
    "    - chars: List of LTChar objects in the paragraph.\n",
    "    - url_list: List of URLs extracted from the PDF.\n",
    "    - url_index: Current index in the URL list.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with updated text and url_index.\n",
    "    \"\"\"\n",
    "    new_paragraph_parts = []\n",
    "\n",
    "    # Group chars by their style (italic or regular)\n",
    "    for is_italic, group in groupby(chars, key=lambda c: 'Italic' in c.fontname):\n",
    "        text = ''.join(c.get_text() for c in group)\n",
    "        text = ' '.join(text.split())  # Remove extra spaces\n",
    "        if is_italic:  # Italic text\n",
    "            # Get the next URL from the list if available\n",
    "            if url_index < len(url_list):\n",
    "                url = url_list[url_index]\n",
    "                url_index += 1\n",
    "                # Replace italic text with markdown link\n",
    "                new_paragraph_parts.append(f\"[{text}]({url})\")\n",
    "            else:\n",
    "                # No more URLs; keep italic text as is\n",
    "                new_paragraph_parts.append(text)\n",
    "        else:\n",
    "            new_paragraph_parts.append(text)\n",
    "\n",
    "    new_paragraph = ' '.join(new_paragraph_parts)\n",
    "    # Remove any extra spaces in the final paragraph\n",
    "    new_paragraph = ' '.join(new_paragraph.split())\n",
    "\n",
    "    return {'text': new_paragraph, 'url_index': url_index}\n",
    "\n",
    "def clean_subsections(extraction):\n",
    "    \"\"\"\n",
    "    Cleans up subsection keys and joins paragraph lists into strings.\n",
    "\n",
    "    Parameters:\n",
    "    - extraction: The nested dictionary of extracted sections and subsections.\n",
    "\n",
    "    Returns:\n",
    "    - extraction: The cleaned and updated nested dictionary.\n",
    "    \"\"\"\n",
    "    # Iterate over each section and its subsections\n",
    "    for section, subsections in extraction.items():\n",
    "        keys_to_update = {}\n",
    "        keys_to_delete = []\n",
    "        for key in subsections:\n",
    "            # Strip leading numbers and whitespace from subsection keys\n",
    "            new_key = key.lstrip(\"0123456789 \").strip()\n",
    "            if new_key:\n",
    "                keys_to_update[key] = new_key\n",
    "            else:\n",
    "                keys_to_delete.append(key)\n",
    "        # Update subsection keys outside the loop to avoid modifying the dict during iteration\n",
    "        for old_key, new_key in keys_to_update.items():\n",
    "            subsections[new_key] = subsections.pop(old_key)\n",
    "        # Delete empty keys\n",
    "        for key in keys_to_delete:\n",
    "            subsections.pop(key)\n",
    "\n",
    "    # Join paragraph lists into single strings\n",
    "    for subsections in extraction.values():\n",
    "        for sub_key in subsections:\n",
    "            if isinstance(subsections[sub_key], list):\n",
    "                subsections[sub_key] = ' '.join(subsections[sub_key])\n",
    "                # Remove extra spaces in the concatenated paragraphs\n",
    "                subsections[sub_key] = ' '.join(subsections[sub_key].split())\n",
    "\n",
    "    return extraction\n",
    "\n",
    "def extract(pdf_bytes, topic, pdf_url, save_json=False):\n",
    "    \"\"\"\n",
    "    Extracts structured text from a PDF byte stream and replaces italic text with markdown links.\n",
    "    Handles multiline sections and subsections regardless of the starting character's case.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_bytes: Bytes of the PDF file.\n",
    "    - save_json: Boolean flag to save the extraction result as a JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - extraction: A nested dictionary containing sections, subsections, and paragraphs.\n",
    "    \"\"\"\n",
    "    pdf_stream = BytesIO(pdf_bytes)\n",
    "\n",
    "    extraction = {}\n",
    "    section = \"\"\n",
    "    subsection = \"\"\n",
    "    current_section_lines = []\n",
    "    current_subsection_lines = []\n",
    "\n",
    "    # Extract URLs in order\n",
    "    url_list = extract_urls(pdf_bytes)\n",
    "    url_index = 0  # To keep track of the current URL\n",
    "\n",
    "    # Iterate over pages in the PDF\n",
    "    for page_layout in extract_pages(pdf_stream):\n",
    "        page_number = page_layout.pageid\n",
    "\n",
    "        # Skip the first page\n",
    "        if page_number == 1:\n",
    "            continue\n",
    "\n",
    "        # Iterate over elements in the page layout\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                # For each text line or character within the text container\n",
    "                for obj in element:\n",
    "                    # Handle cases where obj is a text line or character\n",
    "                    if isinstance(obj, (LTTextLine, LTTextLineHorizontal)):\n",
    "                        text_line = obj\n",
    "                    elif isinstance(obj, LTChar):\n",
    "                        # Wrap the character in a list to process it as a single-character line\n",
    "                        text_line = [obj]\n",
    "                    else:\n",
    "                        continue  # Skip other types\n",
    "\n",
    "                    # Extract characters from the text line\n",
    "                    if isinstance(text_line, list):\n",
    "                        chars = [char for char in text_line if isinstance(char, LTChar)]\n",
    "                    else:\n",
    "                        chars = [char for char in text_line if isinstance(char, LTChar)]\n",
    "                    if not chars:\n",
    "                        continue  # Skip if there are no characters\n",
    "\n",
    "                    # Get character sizes and fonts\n",
    "                    char_sizes = [char.size for char in chars]\n",
    "                    char_fonts = [\n",
    "                        char.graphicstate.ncolor\n",
    "                        for char in chars\n",
    "                        if hasattr(char.graphicstate, 'ncolor')\n",
    "                    ]\n",
    "\n",
    "                    # Define thresholds and font colors for different text types\n",
    "                    text_styles = {\n",
    "                        'section': {'size': 12.5, 'color': [1]},\n",
    "                        'subsection': {'size': 11, 'color': [0, 0, 0, 1]},\n",
    "                        'paragraph': {'size': 10, 'color': [0, 0, 0, 1]},\n",
    "                        'break': {'size': 14, 'color': [0, 0, 0, 1]}\n",
    "                    }\n",
    "\n",
    "                    # Function to check if text line matches a style\n",
    "                    def matches_style(style):\n",
    "                        return (\n",
    "                            all(abs(size - style['size']) < 0.1 for size in char_sizes) and\n",
    "                            all(font == style['color'] for font in char_fonts)\n",
    "                        )\n",
    "\n",
    "                    # Check for break condition\n",
    "                    if matches_style(text_styles['break']):\n",
    "                        # Process any pending subsection\n",
    "                        if current_subsection_lines:\n",
    "                            subsection_text = ' '.join(current_subsection_lines)\n",
    "                            if section:\n",
    "                                if subsection_text not in extraction[section]:\n",
    "                                    extraction[section][subsection_text] = []\n",
    "                                subsection = subsection_text\n",
    "                            current_subsection_lines = []\n",
    "\n",
    "                        # Process any pending section\n",
    "                        if current_section_lines:\n",
    "                            section_text = ' '.join(current_section_lines)\n",
    "                            section_text = ' '.join(section_text.split())  # Remove extra spaces\n",
    "                            extraction[section_text] = {\"main\": []}\n",
    "                            section = section_text\n",
    "                            current_section_lines = []\n",
    "\n",
    "                        # Clean up\n",
    "                        extraction = clean_subsections(extraction)\n",
    "                        if save_json:\n",
    "                            with open('./extraction.json', 'w', encoding=\"utf-8\") as fp:\n",
    "                                json.dump(extraction, fp, ensure_ascii=False)\n",
    "                        # Reset variables to start processing next section\n",
    "                        section = \"\"\n",
    "                        subsection = \"\"\n",
    "                        continue  # Continue processing the next lines\n",
    "\n",
    "                    # Check if the text line is a section header\n",
    "                    if matches_style(text_styles['section']):\n",
    "                        # Append the text line to current_section_lines\n",
    "                        text_line_text = ''.join(char.get_text() for char in chars)\n",
    "                        text_line_text = ' '.join(text_line_text.split())  # Remove extra spaces\n",
    "                        current_section_lines.append(text_line_text)\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        # If current_section_lines is not empty, process the section\n",
    "                        if current_section_lines:\n",
    "                            # Process any pending subsection\n",
    "                            if current_subsection_lines:\n",
    "                                subsection_text = ' '.join(current_subsection_lines)\n",
    "                                if section:\n",
    "                                    if subsection_text not in extraction[section]:\n",
    "                                        extraction[section][subsection_text] = []\n",
    "                                    subsection = subsection_text\n",
    "                                current_subsection_lines = []\n",
    "\n",
    "                            # Join the collected section lines to form the section title\n",
    "                            section_text = ' '.join(current_section_lines)\n",
    "                            section_text = ' '.join(section_text.split())  # Remove extra spaces\n",
    "                            extraction[section_text] = {\"main\": []}\n",
    "                            section = section_text\n",
    "                            subsection = \"\"\n",
    "                            current_section_lines = []\n",
    "\n",
    "                    # Filter out subsection numbers (e.g., \"[0, 0, 0, 0.3]\")\n",
    "                    filtered_fonts = [font for font in char_fonts if font != [0, 0, 0, 0.3]]\n",
    "\n",
    "                    # Check if the text line is a subsection header\n",
    "                    if (\n",
    "                        all(abs(size - text_styles['subsection']['size']) < 0.1 for size in char_sizes) and\n",
    "                        all(font == text_styles['subsection']['color'] for font in filtered_fonts)\n",
    "                    ):\n",
    "                        text_line_text = ''.join(char.get_text() for char in chars)\n",
    "                        text_line_text = ' '.join(text_line_text.split())  # Remove extra spaces\n",
    "                        current_subsection_lines.append(text_line_text)\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        # Process any pending subsection\n",
    "                        if current_subsection_lines:\n",
    "                            subsection_text = ' '.join(current_subsection_lines)\n",
    "                            if section:\n",
    "                                if subsection_text not in extraction[section]:\n",
    "                                    extraction[section][subsection_text] = []\n",
    "                                subsection = subsection_text\n",
    "                            current_subsection_lines = []\n",
    "\n",
    "                    # Check if the text line is a paragraph\n",
    "                    if matches_style(text_styles['paragraph']):\n",
    "                        # If section is empty, skip processing the paragraph\n",
    "                        if not section:\n",
    "                            continue\n",
    "\n",
    "                        # Process the paragraph and replace italic text with markdown links\n",
    "                        paragraph = process_paragraph(\n",
    "                            chars, url_list, url_index\n",
    "                        )\n",
    "                        url_index = paragraph['url_index']  # Update the URL index\n",
    "                        paragraph_text = paragraph['text']\n",
    "\n",
    "                        # Remove leading/trailing numbers and spaces\n",
    "                        paragraph_text = paragraph_text.strip(\"0123456789 \").strip()\n",
    "                        paragraph_text = ' '.join(paragraph_text.split())  # Remove extra spaces\n",
    "\n",
    "                        # Append paragraph to the appropriate section and subsection\n",
    "                        if subsection:\n",
    "                            extraction[section][subsection].append(paragraph_text)\n",
    "                        else:\n",
    "                            extraction[section][\"main\"].append(paragraph_text)\n",
    "\n",
    "        # After all lines are processed on the page, check for any pending subsection\n",
    "        if current_subsection_lines:\n",
    "            subsection_text = ' '.join(current_subsection_lines)\n",
    "            if section:\n",
    "                if subsection_text not in extraction[section]:\n",
    "                    extraction[section][subsection_text] = []\n",
    "                subsection = subsection_text\n",
    "            current_subsection_lines = []\n",
    "\n",
    "        # Also check for any pending section\n",
    "        if current_section_lines:\n",
    "            section_text = ' '.join(current_section_lines)\n",
    "            section_text = ' '.join(section_text.split())  # Remove extra spaces\n",
    "            extraction[section_text] = {\"main\": []}\n",
    "            section = section_text\n",
    "            current_section_lines = []\n",
    "\n",
    "    # Clean and finalize the extraction\n",
    "    extraction = clean_subsections(extraction)\n",
    "    if save_json:\n",
    "        filename = pdf_url.split(\"/\")[-1]\n",
    "        with open(f'indexing/data/ahv_parsed/{topic}/{filename}.json', 'w', encoding=\"utf-8\") as fp:\n",
    "            json.dump(extraction, fp, ensure_ascii=False)\n",
    "    return extraction\n",
    "\n",
    "async def extract_pdf_content(topic):\n",
    "    parser = AHVParser()\n",
    "\n",
    "    sitemap_url = \"https://www.ahv-iv.ch/de/Sitemap-DE\"\n",
    "\n",
    "    sitemap = await scraper.fetch(sitemap_url)\n",
    "    url_list = parser.parse_urls(sitemap)\n",
    "\n",
    "    topics = [\"Allgemeines\",\n",
    "          \"Beiträge-AHV-IV-EO-ALV\",\n",
    "          \"Leistungen-der-AHV\",\n",
    "          \"Leistungen-der-IV\",\n",
    "          \"Ergänzungsleistungen-zur-AHV-und-IV\",\n",
    "          \"Überbrückungsleistungen\",\n",
    "          \"Leistungen-der-EO-MSE-EAE-BUE-AdopE\",\n",
    "          \"Familienzulagen\",\n",
    "          \"International\",\n",
    "          \"Andere-Sozialversicherungen\",\n",
    "          #\"Jährliche-Neuerungen\"\n",
    "         ]\n",
    "\n",
    "    section_to_scrap = url_list[[i for i, url in enumerate(url_list) if topic in url][0]]\n",
    "    print(section_to_scrap)\n",
    "\n",
    "    content = scraper.scrap_urls([section_to_scrap])\n",
    "\n",
    "    soups = []\n",
    "    for page in content:\n",
    "        soups.append(BeautifulSoup(page.data, features=\"html.parser\"))\n",
    "\n",
    "    # Get PDF paths from each memento section\n",
    "    pdf_paths = []\n",
    "    for soup in soups:\n",
    "        pdf_paths.extend(parser.get_pdf_paths(soup))\n",
    "\n",
    "    # Scrap PDFs from each memento section\n",
    "    pdf_urls = [\"https://www.ahv-iv.ch\" + pdf_path for pdf_path in pdf_paths]\n",
    "\n",
    "    # Add \"it\", \"fr\" pdf paths\n",
    "    pdf_urls.extend([pdf_url.replace(\".d\", \".f\") for pdf_url in pdf_urls])\n",
    "    pdf_urls.extend([pdf_url.replace(\".d\", \".i\") for pdf_url in pdf_urls])\n",
    "\n",
    "    pdf_urls = list(set(pdf_urls))\n",
    "    print(pdf_urls)\n",
    "\n",
    "    content = scraper.scrap_urls(pdf_urls)\n",
    "\n",
    "    for c in content:\n",
    "        pdf_bytes = c.data\n",
    "        pdf_url = c.meta[\"url\"]\n",
    "        print(pdf_url)\n",
    "        print(topic)\n",
    "        extraction = extract(pdf_bytes, topic, pdf_url, save_json=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c6317-b709-4ab1-81d4-98a3d7c21bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "await extract_pdf_content(topic=\"Andere-Sozialversicherungen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8934ed-3d96-4497-abdb-1925e1d6d528",
   "metadata": {},
   "source": [
    "### Upsert to db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70578d48-93ec-4752-8bc3-8829b2d1947d",
   "metadata": {},
   "source": [
    "#### Name mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68fd945b-1933-4a10-b0f3-b3c45b0eb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "05d42120-409f-4171-bd1d-ae2b32010f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"1.01\": {\"topic\": \"Extrait du Compte Individuel (CI)\",\n",
    "          \"last_modification\": \"01.01.2024\",\n",
    "          \"etat\": \"01.01.2015\"},\n",
    " \"1.02\": {\"topic\": \"Splitting en cas de divorce\",\n",
    "          \"last_modification\": \"01.01.2024\",\n",
    "          \"etat\": \"01.01.2024\"},\n",
    " \"1.03\": {\"topic\": \"Bonifications pour tâches d’assistance\",\n",
    "          \"last_modification\": \"01.01.2024\",\n",
    "          \"etat\": \"01.01.2021\"},\n",
    " \"1.04\": {\"topic\": \"Explications concernant l’extrait du Compte Individuel (CI)\",\n",
    "          \"last_modification\": \"01.01.2024\",\n",
    "          \"etat\": \"01.01.2024\"},\n",
    " \"1.05\": {\"topic\": \"Explications concernant l’aperçu des comptes\",\n",
    "          \"last_modification\": \"31.12.2021\",\n",
    "          \"etat\": \"01.01.2015\"},\n",
    " \"1.07\": {\"topic\": \"Bonifications pour tâches éducatives\",\n",
    "          \"last_modification\": \"01.01.2024\",\n",
    "          \"etat\": \"01.01.2016\"},\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c96ee799-6298-444a-9cf6-c3c264ce8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"Allgemeines\"\n",
    "filename = \"1.07.f.json\"\n",
    "sitemap_url = \"https://www.ahv-iv.ch/de/Sitemap-DE\"\n",
    "url = f\"https://www.ahv-iv.ch/p/{filename.replace('.json', '')}\"\n",
    "with open(f\"indexing/data/ahv_parsed/{tag}/{filename}\", \"r\") as f:\n",
    "    doc = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c7c97-daac-49f0-8308-8156ba3dc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_m = re.sub(r'\\.(d|i|f)\\.json$', '', filename)\n",
    "filename_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715d9e0-5cb9-4937-8957-32c3e71e9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping[filename_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a0cc604c-e84e-46a2-9c21-32b4aa606c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv = True\n",
    "\n",
    "csv = []\n",
    "for section in doc.keys():\n",
    "    if doc[section].keys():\n",
    "        for subsection in doc[section].keys():\n",
    "            if doc[section][subsection]:\n",
    "                text = str(mapping[filename_m]) + \"\\n\" + section + \"\\n\" + subsection + \"\\n\" + doc[section][subsection]\n",
    "                #print(text)\n",
    "                #print(\"-----\")\n",
    "                if to_csv:\n",
    "                    csv.append({\n",
    "                        \"url\": url,\n",
    "                        \"text\": text,\n",
    "                        \"source\": sitemap_url,\n",
    "                        \"tag\": tag\n",
    "                    })\n",
    "\n",
    "    if to_csv:\n",
    "        pd.DataFrame(csv).to_csv(f\"indexing/data/to_upsert/{tag}/{filename.replace('.json', '')}.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7081fbf-d7d0-4834-9082-f55a64e2ab79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3912308-65b1-4b2b-97c8-9e161ba50165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d054ff1-f722-424a-bb31-e88ac9893f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = True\n",
    "to_csv = True\n",
    "upsert = False\n",
    "csv = []\n",
    "\n",
    "for i, doc in enumerate(clean_splits):\n",
    "\n",
    "    n_tokens = len(tokenizer.encode(doc))\n",
    "    if n_tokens > max_tokens:\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        text = doc\n",
    "        url = documents[\"documents\"][0].meta[\"url\"]\n",
    "        language = \"de\"\n",
    "        # CAREFUL !!!!!!\n",
    "        tag = \"Familienzulagen\"\n",
    "        if to_csv:\n",
    "            csv.append({\n",
    "                \"url\": url,\n",
    "                \"text\": text,\n",
    "                \"source\": sitemap_url,\n",
    "                \"tag\": tag\n",
    "            })\n",
    "        if upsert:\n",
    "            document_service.upsert(db, DocumentCreate(url=url, text=text, source=sitemap_url, tag=tag), embed=embed)\n",
    "\n",
    "if to_csv:\n",
    "    pd.DataFrame(csv).to_csv(\"indexing/data/parsed/FZ_noheader_1.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bb7f6-30a3-4ec7-b766-c6d4a137d85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e24b550c-64b0-494f-8eb3-99894d23bfb3",
   "metadata": {},
   "source": [
    "### 1. Fetch sections of ahv-iv.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5242d5d-9391-4dba-866e-d4363bb1369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing.scraper import scraper\n",
    "from indexing.pipelines.ahv import AHVParser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "parser = AHVParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceec413-6d37-4a36-a085-a9edafe850e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitemap_url = \"https://www.ahv-iv.ch/de/Sitemap-DE\"\n",
    "\n",
    "sitemap = await scraper.fetch(sitemap_url)\n",
    "url_list = parser.parse_urls(sitemap)\n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe77b7-be3a-4f02-b444-c7bc4094b93c",
   "metadata": {},
   "source": [
    "### 2. Select section to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ed0cf-1c27-4f1e-8be5-cb536adb87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose section to parse\n",
    "topics = [\"Allgemeines\",\n",
    "          \"Beiträge-AHV-IV-EO-ALV\",\n",
    "          \"Leistungen-der-AHV\",\n",
    "          \"Leistungen-der-IV\",\n",
    "          \"Ergänzungsleistungen-zur-AHV-und-IV\",\n",
    "          \"Überbrückungsleistungen\",\n",
    "          \"Leistungen-der-EO-MSE-EAE-BUE-AdopE\",\n",
    "          \"Familienzulagen\",\n",
    "          \"International\",\n",
    "          \"Andere-Sozialversicherungen\",\n",
    "          #\"Jährliche-Neuerungen\"\n",
    "         ]\n",
    "topic = topics[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68aa1d-ee65-4ac9-a220-3823819b3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_to_scrap = url_list[[i for i, url in enumerate(url_list) if topic in url][0]]\n",
    "section_to_scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2d274-b828-4fb0-9649-7e90e6b187e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = scraper.scrap_urls([section_to_scrap])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70542820-6169-4d6b-981b-80374eb95870",
   "metadata": {},
   "source": [
    "#### --- OPTIONAL: Auto parsing for all other sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dffafc4-e981-4f86-8ca1-731f71f2c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove FZ PDFs (manually checked OK)\n",
    "url_list.remove('https://www.ahv-iv.ch/de/Merkblätter-Formulare/Merkblätter/Familienzulagen')\n",
    "print(url_list)\n",
    "content = scraper.scrap_urls(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7799345-5eb1-407a-9f9c-4fe50585cd60",
   "metadata": {},
   "source": [
    "### 3. Get PDF URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ceab11-8dd9-4ffb-86f6-e7febb76d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = []\n",
    "for page in content:\n",
    "    soups.append(BeautifulSoup(page.data, features=\"html.parser\"))\n",
    "\n",
    "# Get PDF paths from each memento section\n",
    "pdf_paths = []\n",
    "for soup in soups:\n",
    "    pdf_paths.extend(parser.get_pdf_paths(soup))\n",
    "\n",
    "# Scrap PDFs from each memento section\n",
    "pdf_urls = [\"https://www.ahv-iv.ch\" + pdf_path for pdf_path in pdf_paths]\n",
    "\n",
    "# Add \"it\", \"fr\" pdf paths\n",
    "pdf_urls.extend([pdf_url.replace(\".d\", \".f\") for pdf_url in pdf_urls])\n",
    "pdf_urls.extend([pdf_url.replace(\".d\", \".i\") for pdf_url in pdf_urls])\n",
    "\n",
    "pdf_urls = list(set(pdf_urls))\n",
    "len(pdf_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8551ba-7272-4cd7-bd39-ee5147ed6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c858d5ea-6ab7-4b32-903b-324ab1a31680",
   "metadata": {},
   "source": [
    "#### --- OPTIONAL: Filter docs by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8a3b1-b111-4132-bce5-21440f3ee40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only german docs\n",
    "pdf_urls = [url for url in pdf_urls if url.endswith(\".d\")]\n",
    "pdf_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8550991-2100-4e28-b32e-1aebb20b1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only french docs\n",
    "pdf_urls = [url for url in pdf_urls if url.endswith(\".f\")]\n",
    "pdf_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c354499-623a-4f45-99d3-95e20d73b56b",
   "metadata": {},
   "source": [
    "### 4. Scrap PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec21e8c-f6b5-45b9-b1a9-e4a2cc799f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = scraper.scrap_urls(pdf_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16484cf5-a1de-493d-8fc3-c16b9dbdf7a8",
   "metadata": {},
   "source": [
    "### 5. Custom PDF Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895cfabe-4ed5-41ea-8966-2a43f0a09e17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfminer.high_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfminer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhigh_level\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_pages\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfminer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     LTTextContainer,\n\u001b[0;32m      5\u001b[0m     LTChar,\n\u001b[0;32m      6\u001b[0m     LTTextLine,\n\u001b[0;32m      7\u001b[0m     LTTextLineHorizontal,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfminer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdfparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PDFParser\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pdfminer.high_level'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import (\n",
    "    LTTextContainer,\n",
    "    LTChar,\n",
    "    LTTextLine,\n",
    "    LTTextLineHorizontal,\n",
    ")\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import resolve1\n",
    "import json\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb471a4-e155-42fc-9279-9a982cd8ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(pdf_bytes):\n",
    "    \"\"\"\n",
    "    Extracts URLs from a PDF byte stream in the order they appear.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "\n",
    "    parser = PDFParser(BytesIO(pdf_bytes))\n",
    "    document = PDFDocument(parser)\n",
    "\n",
    "    # Iterate through pages to extract annotations\n",
    "    for page in PDFPage.create_pages(document):\n",
    "        annotations = page.annots\n",
    "        if annotations:\n",
    "            annotations = resolve1(annotations)\n",
    "            for annotation in annotations:\n",
    "                annotation_data = resolve1(annotation)\n",
    "                # Check if the annotation is a link\n",
    "                if annotation_data.get('Subtype').name == 'Link':\n",
    "                    action = annotation_data.get('A')\n",
    "                    if action and action.resolve().get('S').name == 'URI':\n",
    "                        uri = action.resolve().get('URI')\n",
    "                        if uri:\n",
    "                            urls.append(uri.decode('utf-8'))\n",
    "    return urls\n",
    "\n",
    "def process_paragraph(chars, url_list, url_index):\n",
    "    \"\"\"\n",
    "    Processes a paragraph, replacing italic text with markdown links.\n",
    "\n",
    "    Parameters:\n",
    "    - chars: List of LTChar objects in the paragraph.\n",
    "    - url_list: List of URLs extracted from the PDF.\n",
    "    - url_index: Current index in the URL list.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with updated text and url_index.\n",
    "    \"\"\"\n",
    "    new_paragraph_parts = []\n",
    "\n",
    "    # Group chars by their style (italic or regular)\n",
    "    for is_italic, group in groupby(chars, key=lambda c: 'Italic' in c.fontname):\n",
    "        text = ''.join(c.get_text() for c in group)\n",
    "        text = ' '.join(text.split())  # Remove extra spaces\n",
    "        if is_italic:  # Italic text\n",
    "            # Get the next URL from the list if available\n",
    "            if url_index < len(url_list):\n",
    "                url = url_list[url_index]\n",
    "                url_index += 1\n",
    "                # Replace italic text with markdown link\n",
    "                new_paragraph_parts.append(f\"[{text}]({url})\")\n",
    "            else:\n",
    "                # No more URLs; keep italic text as is\n",
    "                new_paragraph_parts.append(text)\n",
    "        else:\n",
    "            new_paragraph_parts.append(text)\n",
    "\n",
    "    new_paragraph = ' '.join(new_paragraph_parts)\n",
    "    # Remove any extra spaces in the final paragraph\n",
    "    new_paragraph = ' '.join(new_paragraph.split())\n",
    "\n",
    "    return {'text': new_paragraph, 'url_index': url_index}\n",
    "\n",
    "def clean_subsections(extraction):\n",
    "    \"\"\"\n",
    "    Cleans up subsection keys and joins paragraph lists into strings.\n",
    "\n",
    "    Parameters:\n",
    "    - extraction: The nested dictionary of extracted sections and subsections.\n",
    "\n",
    "    Returns:\n",
    "    - extraction: The cleaned and updated nested dictionary.\n",
    "    \"\"\"\n",
    "    # Iterate over each section and its subsections\n",
    "    for section, subsections in extraction.items():\n",
    "        keys_to_update = {}\n",
    "        keys_to_delete = []\n",
    "        for key in subsections:\n",
    "            # Strip leading numbers and whitespace from subsection keys\n",
    "            new_key = key.lstrip(\"0123456789 \").strip()\n",
    "            if new_key:\n",
    "                keys_to_update[key] = new_key\n",
    "            else:\n",
    "                keys_to_delete.append(key)\n",
    "        # Update subsection keys outside the loop to avoid modifying the dict during iteration\n",
    "        for old_key, new_key in keys_to_update.items():\n",
    "            subsections[new_key] = subsections.pop(old_key)\n",
    "        # Delete empty keys\n",
    "        for key in keys_to_delete:\n",
    "            subsections.pop(key)\n",
    "\n",
    "    # Join paragraph lists into single strings\n",
    "    for subsections in extraction.values():\n",
    "        for sub_key in subsections:\n",
    "            if isinstance(subsections[sub_key], list):\n",
    "                subsections[sub_key] = ' '.join(subsections[sub_key])\n",
    "                # Remove extra spaces in the concatenated paragraphs\n",
    "                subsections[sub_key] = ' '.join(subsections[sub_key].split())\n",
    "\n",
    "    return extraction\n",
    "\n",
    "def extract(pdf_bytes, save_json=False):\n",
    "    \"\"\"\n",
    "    Extracts structured text from a PDF byte stream and replaces italic text with markdown links.\n",
    "    Handles multiline sections and subsections regardless of the starting character's case.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_bytes: Bytes of the PDF file.\n",
    "    - save_json: Boolean flag to save the extraction result as a JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - extraction: A nested dictionary containing sections, subsections, and paragraphs.\n",
    "    \"\"\"\n",
    "    pdf_stream = BytesIO(pdf_bytes)\n",
    "\n",
    "    extraction = {}\n",
    "    section = \"\"\n",
    "    subsection = \"\"\n",
    "    current_section_lines = []\n",
    "    current_subsection_lines = []\n",
    "\n",
    "    # Extract URLs in order\n",
    "    with open('sources/pdf_urls.json', 'r') as file:\n",
    "        pdf_urls = json.load(file)\n",
    "    pdf_urls = pdf_urls[:20] + pdf_urls[460:480] + pdf_urls[-20:]\n",
    "    url_list = [item['url'] for item in pdf_urls]\n",
    "    url_index = 0  # To keep track of the current URL\n",
    "\n",
    "    # Iterate over pages in the PDF\n",
    "    for page_layout in extract_pages(pdf_stream):\n",
    "        page_number = page_layout.pageid\n",
    "\n",
    "        # Skip the first page\n",
    "        if page_number == 1:\n",
    "            continue\n",
    "\n",
    "        # Iterate over elements in the page layout\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                # For each text line or character within the text container\n",
    "                for obj in element:\n",
    "                    # Handle cases where obj is a text line or character\n",
    "                    if isinstance(obj, (LTTextLine, LTTextLineHorizontal)):\n",
    "                        text_line = obj\n",
    "                    elif isinstance(obj, LTChar):\n",
    "                        # Wrap the character in a list to process it as a single-character line\n",
    "                        text_line = [obj]\n",
    "                    else:\n",
    "                        continue  # Skip other types\n",
    "\n",
    "                    # Extract characters from the text line\n",
    "                    if isinstance(text_line, list):\n",
    "                        chars = [char for char in text_line if isinstance(char, LTChar)]\n",
    "                    else:\n",
    "                        chars = [char for char in text_line if isinstance(char, LTChar)]\n",
    "                    if not chars:\n",
    "                        continue  # Skip if there are no characters\n",
    "\n",
    "                    # Get character sizes and fonts\n",
    "                    char_sizes = [char.size for char in chars]\n",
    "                    char_fonts = [\n",
    "                        char.graphicstate.ncolor\n",
    "                        for char in chars\n",
    "                        if hasattr(char.graphicstate, 'ncolor')\n",
    "                    ]\n",
    "\n",
    "                    # Define thresholds and font colors for different text types\n",
    "                    text_styles = {\n",
    "                        'section': {'size': 12.5, 'color': [1]},\n",
    "                        'subsection': {'size': 11, 'color': [0, 0, 0, 1]},\n",
    "                        'paragraph': {'size': 10, 'color': [0, 0, 0, 1]},\n",
    "                        'break': {'size': 14, 'color': [0, 0, 0, 1]}\n",
    "                    }\n",
    "\n",
    "                    # Function to check if text line matches a style\n",
    "                    def matches_style(style):\n",
    "                        return (\n",
    "                            all(abs(size - style['size']) < 0.1 for size in char_sizes) and\n",
    "                            all(font == style['color'] for font in char_fonts)\n",
    "                        )\n",
    "\n",
    "                    # Check for break condition\n",
    "                    if matches_style(text_styles['break']):\n",
    "                        # Process any pending subsection\n",
    "                        if current_subsection_lines:\n",
    "                            subsection_text = ' '.join(current_subsection_lines)\n",
    "                            if section:\n",
    "                                if subsection_text not in extraction[section]:\n",
    "                                    extraction[section][subsection_text] = []\n",
    "                                subsection = subsection_text\n",
    "                            current_subsection_lines = []\n",
    "\n",
    "                        # Process any pending section\n",
    "                        if current_section_lines:\n",
    "                            section_text = ' '.join(current_section_lines)\n",
    "                            section_text = ' '.join(section_text.split())  # Remove extra spaces\n",
    "                            extraction[section_text] = {\"main\": []}\n",
    "                            section = section_text\n",
    "                            current_section_lines = []\n",
    "\n",
    "                        # Clean up\n",
    "                        extraction = clean_subsections(extraction)\n",
    "                        if save_json:\n",
    "                            with open('./extraction.json', 'w', encoding=\"utf-8\") as fp:\n",
    "                                json.dump(extraction, fp, ensure_ascii=False)\n",
    "                        # Reset variables to start processing next section\n",
    "                        section = \"\"\n",
    "                        subsection = \"\"\n",
    "                        continue  # Continue processing the next lines\n",
    "\n",
    "                    # Check if the text line is a section header\n",
    "                    if matches_style(text_styles['section']):\n",
    "                        # Append the text line to current_section_lines\n",
    "                        text_line_text = ''.join(char.get_text() for char in chars)\n",
    "                        text_line_text = ' '.join(text_line_text.split())  # Remove extra spaces\n",
    "                        current_section_lines.append(text_line_text)\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        # If current_section_lines is not empty, process the section\n",
    "                        if current_section_lines:\n",
    "                            # Process any pending subsection\n",
    "                            if current_subsection_lines:\n",
    "                                subsection_text = ' '.join(current_subsection_lines)\n",
    "                                if section:\n",
    "                                    if subsection_text not in extraction[section]:\n",
    "                                        extraction[section][subsection_text] = []\n",
    "                                    subsection = subsection_text\n",
    "                                current_subsection_lines = []\n",
    "\n",
    "                            # Join the collected section lines to form the section title\n",
    "                            section_text = ' '.join(current_section_lines)\n",
    "                            section_text = ' '.join(section_text.split())  # Remove extra spaces\n",
    "                            extraction[section_text] = {\"main\": []}\n",
    "                            section = section_text\n",
    "                            subsection = \"\"\n",
    "                            current_section_lines = []\n",
    "\n",
    "                    # Filter out subsection numbers (e.g., \"[0, 0, 0, 0.3]\")\n",
    "                    filtered_fonts = [font for font in char_fonts if font != [0, 0, 0, 0.3]]\n",
    "\n",
    "                    # Check if the text line is a subsection header\n",
    "                    if (\n",
    "                        all(abs(size - text_styles['subsection']['size']) < 0.1 for size in char_sizes) and\n",
    "                        all(font == text_styles['subsection']['color'] for font in filtered_fonts)\n",
    "                    ):\n",
    "                        text_line_text = ''.join(char.get_text() for char in chars)\n",
    "                        text_line_text = ' '.join(text_line_text.split())  # Remove extra spaces\n",
    "                        current_subsection_lines.append(text_line_text)\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        # Process any pending subsection\n",
    "                        if current_subsection_lines:\n",
    "                            subsection_text = ' '.join(current_subsection_lines)\n",
    "                            if section:\n",
    "                                if subsection_text not in extraction[section]:\n",
    "                                    extraction[section][subsection_text] = []\n",
    "                                subsection = subsection_text\n",
    "                            current_subsection_lines = []\n",
    "\n",
    "                    # Check if the text line is a paragraph\n",
    "                    if matches_style(text_styles['paragraph']):\n",
    "                        # If section is empty, skip processing the paragraph\n",
    "                        if not section:\n",
    "                            continue\n",
    "\n",
    "                        # Process the paragraph and replace italic text with markdown links\n",
    "                        paragraph = process_paragraph(\n",
    "                            chars, url_list, url_index\n",
    "                        )\n",
    "                        url_index = paragraph['url_index']  # Update the URL index\n",
    "                        paragraph_text = paragraph['text']\n",
    "\n",
    "                        # Remove leading/trailing numbers and spaces\n",
    "                        paragraph_text = paragraph_text.strip(\"0123456789 \").strip()\n",
    "                        paragraph_text = ' '.join(paragraph_text.split())  # Remove extra spaces\n",
    "\n",
    "                        # Append paragraph to the appropriate section and subsection\n",
    "                        if subsection:\n",
    "                            extraction[section][subsection].append(paragraph_text)\n",
    "                        else:\n",
    "                            extraction[section][\"main\"].append(paragraph_text)\n",
    "\n",
    "        # After all lines are processed on the page, check for any pending subsection\n",
    "        if current_subsection_lines:\n",
    "            subsection_text = ' '.join(current_subsection_lines)\n",
    "            if section:\n",
    "                if subsection_text not in extraction[section]:\n",
    "                    extraction[section][subsection_text] = []\n",
    "                subsection = subsection_text\n",
    "            current_subsection_lines = []\n",
    "\n",
    "        # Also check for any pending section\n",
    "        if current_section_lines:\n",
    "            section_text = ' '.join(current_section_lines)\n",
    "            section_text = ' '.join(section_text.split())  # Remove extra spaces\n",
    "            extraction[section_text] = {\"main\": []}\n",
    "            section = section_text\n",
    "            current_section_lines = []\n",
    "\n",
    "    # Clean and finalize the extraction\n",
    "    extraction = clean_subsections(extraction)\n",
    "    if save_json:\n",
    "        with open('./extraction.json', 'w', encoding=\"utf-8\") as fp:\n",
    "            json.dump(extraction, fp, ensure_ascii=False)\n",
    "    return extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c5f36-140b-4408-9e5d-97208d854023",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "pdf_bytes = content[i].data\n",
    "content[i].meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e41d6-30d5-4a86-b90b-b5363ef9c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction = extract(pdf_bytes, save_json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701552ea-6a75-4d67-81a5-d8272fa62800",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./extraction.json', 'r', encoding=\"utf-8\") as file:\n",
    "    extraction = json.load(file)\n",
    "\n",
    "print(json.dumps(extraction,\n",
    "                 sort_keys=False,\n",
    "                 indent=4,\n",
    "                 ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6051dfb-5e64-43e1-950b-014576dd9336",
   "metadata": {},
   "source": [
    "### Extract all content by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ed1d7-bd68-447a-bc35-52c6446bfdba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e870e3-047a-4a50-8a16-5d0599ccad5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8341879-c0a3-4d2a-85c7-b264255faa4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b5444-b61c-4d20-a1aa-f0283a367d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49358dbf-fd66-4a09-82fb-1aea9526630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfcolor import PDFColorSpace, LITERAL_DEVICE_RGB, LITERAL_DEVICE_CMYK, LITERAL_DEVICE_GRAY\n",
    "from pdfminer.pdffont import PDFFontError\n",
    "\n",
    "class TextPropertyDevice(PDFDevice):\n",
    "    def __init__(self, rsrcmgr):\n",
    "        super().__init__(rsrcmgr)\n",
    "        self.characters = []\n",
    "        self.current_color_space = None\n",
    "\n",
    "    def render_string(self, textstate, seq, ncs, graphicstate):\n",
    "        font = textstate.font\n",
    "        fontsize = textstate.fontsize\n",
    "        fill_color = graphicstate.ncolor  # Non-stroking color\n",
    "        colorspace = ncs\n",
    "        color_values = self.get_color_values(fill_color, colorspace)\n",
    "        for obj in seq:\n",
    "            if isinstance(obj, (int, float)):\n",
    "                # Handle text spacing adjustments\n",
    "                continue\n",
    "            else:\n",
    "                # Decode the text\n",
    "                try:\n",
    "                    text = font.decode(obj)\n",
    "                except PDFFontError:\n",
    "                    # Handle font decoding errors\n",
    "                    continue\n",
    "                for c in text:\n",
    "                    self.characters.append({\n",
    "                        'char': c,\n",
    "                        'fontname': font.basefont,\n",
    "                        'size': fontsize,\n",
    "                        'fill_color': color_values,\n",
    "                        # Background color extraction would require additional processing\n",
    "                    })\n",
    "\n",
    "    def get_color_values(self, color, colorspace):\n",
    "        # Attempt to convert the color to RGB\n",
    "        try:\n",
    "            if isinstance(colorspace, str):\n",
    "                colorspace_name = colorspace\n",
    "            elif hasattr(colorspace, 'name'):\n",
    "                colorspace_name = colorspace.name\n",
    "            else:\n",
    "                colorspace_name = str(colorspace)\n",
    "\n",
    "            if colorspace_name == LITERAL_DEVICE_RGB:\n",
    "                r, g, b = color\n",
    "                return {'r': r, 'g': g, 'b': b}\n",
    "            elif colorspace_name == LITERAL_DEVICE_CMYK:\n",
    "                c, m, y, k = color\n",
    "                return {'c': c, 'm': m, 'y': y, 'k': k}\n",
    "            elif colorspace_name == LITERAL_DEVICE_GRAY:\n",
    "                gray = color[0]\n",
    "                return {'gray': gray}\n",
    "            else:\n",
    "                # For other color spaces, attempt to get RGB values\n",
    "                cs = PDFColorSpace(colorspace)\n",
    "                rgb = cs.get_rgb(color, None)\n",
    "                if rgb:\n",
    "                    r, g, b = rgb\n",
    "                    return {'r': r, 'g': g, 'b': b}\n",
    "                else:\n",
    "                    return {'color': color}\n",
    "        except Exception as e:\n",
    "            # Handle cases where color conversion fails\n",
    "            return {'color': color}\n",
    "\n",
    "def extract_text_properties(pdf_bytes):\n",
    "    from io import BytesIO\n",
    "    parser = PDFParser(BytesIO(pdf_bytes))\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextPropertyDevice(rsrcmgr)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "    for c in device.characters:\n",
    "        print(f\"Character: {c['char']}, Font: {c['fontname']}, Size: {c['size']}, Color: {c['fill_color']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d75c3f-d129-4e38-ba95-429fe0be37dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "extract_text_properties(pdf_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de6aff-c236-4aa5-8da9-90bd60e4a86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2ac50-02d3-412d-8b04-d3167daf0f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a9f51-2d00-4fb8-bdc8-19fb4469425c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1eb630-adff-41f0-99b8-4cf9d4c055ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTTextLine, LTTextLineHorizontal\n",
    "\n",
    "def print_char_sizes_and_colors(pdf_bytes):\n",
    "    \"\"\"\n",
    "    Iterates over all characters in the PDF and prints their size and color.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_bytes: Bytes of the PDF file.\n",
    "    \"\"\"\n",
    "    pdf_stream = BytesIO(pdf_bytes)\n",
    "\n",
    "    # Iterate over pages in the PDF\n",
    "    for page_layout in extract_pages(pdf_stream):\n",
    "        page_number = page_layout.pageid\n",
    "        print(f\"Page {page_number}\")\n",
    "\n",
    "        # Iterate over elements in the page layout\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                # For each text line or character within the text container\n",
    "                for obj in element:\n",
    "                    # Handle cases where obj is a text line or character\n",
    "                    if isinstance(obj, (LTTextLine, LTTextLineHorizontal)):\n",
    "                        text_line = obj\n",
    "                    elif isinstance(obj, LTChar):\n",
    "                        # Wrap the character in a list to process it as a single-character line\n",
    "                        text_line = [obj]\n",
    "                    else:\n",
    "                        continue  # Skip other types\n",
    "\n",
    "                    # Extract characters from the text line\n",
    "                    if isinstance(text_line, list):\n",
    "                        chars = [char for char in text_line if isinstance(char, LTChar)]\n",
    "                    else:\n",
    "                        chars = [char for char in text_line if isinstance(char, LTChar)]\n",
    "                    if not chars:\n",
    "                        continue  # Skip if there are no characters\n",
    "\n",
    "                    # For each character, print size and color\n",
    "                    for char in chars:\n",
    "                        size = char.size\n",
    "                        fontname = char.fontname\n",
    "                        ncolor = getattr(char.graphicstate, 'ncolor', None)\n",
    "                        text = char.get_text()\n",
    "                        #if ncolor == [0, 0, 0, 0] and fontname == \"LBNQWH+FrutigerLTStd-Bold\":\n",
    "                        print(f\"Character: '{text}' Size: {size} Font: {fontname} Color: {ncolor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3eedf1-36fb-432f-892d-15b75f1dab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_char_sizes_and_colors(pdf_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4304a-1ff0-4b9b-9d8c-f255636ef216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a358c2-b079-49ea-ae3a-77bb03eb2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.layout import LAParams, LTPage, LTTextBox, LTTextLine, LTChar, LTFigure, LTRect\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def extract_text_properties(pdf_bytes):\n",
    "    from io import BytesIO\n",
    "    parser = PDFParser(BytesIO(pdf_bytes))\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    laparams = LAParams()\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "        layout = device.get_result()\n",
    "        # Lists to hold characters and filled rectangles\n",
    "        chars = []\n",
    "        rects = []\n",
    "        parse_layout(layout, chars, rects)\n",
    "        for c in chars:\n",
    "            char_bbox = c.bbox\n",
    "            char_bg_color = None\n",
    "            for rect in rects:\n",
    "                rect_bbox = rect['bbox']\n",
    "                if bbox_overlap(char_bbox, rect_bbox):\n",
    "                    char_bg_color = rect['fill_color']\n",
    "                    break  # Assuming the topmost rectangle is the background\n",
    "            print(f\"Character: '{c.get_text()}', Font: {c.fontname}, Size: {c.size}, \"\n",
    "                  f\"Color: {c.non_stroking_color}, Background Color: {char_bg_color}\")\n",
    "\n",
    "def parse_layout(layout_obj, chars, rects):\n",
    "    for obj in layout_obj:\n",
    "        if isinstance(obj, LTChar):\n",
    "            chars.append(obj)\n",
    "        elif isinstance(obj, LTRect):\n",
    "            # Only consider filled rectangles\n",
    "            if obj.non_stroking_color is not None:\n",
    "                rects.append({\n",
    "                    'bbox': obj.bbox,\n",
    "                    'fill_color': obj.non_stroking_color\n",
    "                })\n",
    "        elif isinstance(obj, (LTTextBox, LTTextLine, LTFigure, LTPage)):\n",
    "            parse_layout(obj, chars, rects)\n",
    "\n",
    "def bbox_overlap(bbox1, bbox2):\n",
    "    # Determine if two bounding boxes overlap\n",
    "    x0_1, y0_1, x1_1, y1_1 = bbox1\n",
    "    x0_2, y0_2, x1_2, y1_2 = bbox2\n",
    "    return not (x1_1 <= x0_2 or x1_2 <= x0_1 or y1_1 <= y0_2 or y1_2 <= y0_1)\n",
    "\n",
    "extract_text_properties(pdf_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96019204-d8d4-402b-b934-b625e800a0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d18335-66c1-4551-a87a-2afc16949774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628f0f7-828f-46ec-9fc9-a361e2803214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def get_text_in_filled_bboxes(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from bounding boxes that have a fill color and returns the fill color in RGB.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_path: Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    - A list of dictionaries containing page number, fill color, bounding box, and extracted text.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    results = []\n",
    "\n",
    "    for page_number, page in enumerate(doc, 1):\n",
    "        # Get drawing objects on the page\n",
    "        drawables = page.get_drawings()\n",
    "\n",
    "        for d in drawables:\n",
    "            if d['fill'] is not None:\n",
    "                fill_color = d['fill']\n",
    "                # Convert fill color from floats (0-1) to RGB integers (0-255)\n",
    "                rgb_color = tuple(int(c * 255) for c in fill_color)\n",
    "\n",
    "                # Get the bounding box of the filled shape\n",
    "                rect = fitz.Rect(d['bbox'])\n",
    "                # Expand the rectangle slightly to ensure all text is captured\n",
    "                expanded_rect = rect + (-1, -1, 1, 1)\n",
    "\n",
    "                # Extract text within the bounding box\n",
    "                text = page.get_text(\"text\", clip=expanded_rect)\n",
    "\n",
    "                # Append the information to the results list\n",
    "                results.append({\n",
    "                    'page_number': page_number,\n",
    "                    'fill_color_rgb': rgb_color,\n",
    "                    'bounding_box': rect,\n",
    "                    'text': text.strip()\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = '2.03_i.pdf'\n",
    "filled_texts = get_text_in_filled_bboxes(pdf_path)\n",
    "\n",
    "# Print the extracted information\n",
    "for item in filled_texts:\n",
    "    print(f\"Page {item['page_number']}, Fill Color RGB: {item['fill_color_rgb']}\")\n",
    "    print(f\"Bounding Box: {item['bounding_box']}\")\n",
    "    print(f\"Text within the filled bounding box:\\n{item['text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301cf71-e83b-4127-8dca-d4fb08533d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87556222-ef20-4088-99b4-be5983a1998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "doc = fitz.open('2.03_i.pdf')\n",
    "\n",
    "red_rects = []\n",
    "\n",
    "for page_number, page in enumerate(doc, 1):\n",
    "\n",
    "    if page_number == 1:\n",
    "        continue\n",
    "\n",
    "    # Get drawing objects on the page\n",
    "    drawables = page.get_drawings()\n",
    "\n",
    "    for d in drawables:\n",
    "        if d['fill'] is not None:\n",
    "            fill_color = d['fill']\n",
    "            print(page_number, \"---\", fill_color)\n",
    "            # Check if the fill color is red (RGB: 1, 0, 0)\n",
    "            if fill_color == (1, 0, 0):\n",
    "                rect = fitz.Rect(d['bbox'])\n",
    "                red_rects.append((page_number, rect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28377c3e-b5fb-4c8d-a52a-fafc7da9375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a4f19-7271-45d2-8226-4b186012d982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ce9d2-8ed3-4031-884c-fe627875d2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630d671-aa3f-404d-8c44-6766b1b436ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTLine, LTFigure\n",
    "\n",
    "def extract_table_elements(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        # Create resource manager and page interpreter\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        for page in PDFPage.get_pages(file):\n",
    "            interpreter.process_page(page)\n",
    "            layout = device.get_result()\n",
    "\n",
    "            # Iterate through layout objects\n",
    "            for element in layout:\n",
    "                if isinstance(element, LTTextBox):\n",
    "                    # Process text box (e.g., get text, coordinates)\n",
    "                    print(f'Text: {element.get_text()}')\n",
    "                elif isinstance(element, LTLine):\n",
    "                    # Process line element (e.g., position, length)\n",
    "                    print(f'Line: {element.x0, element.y0, element.x1, element.y1}')\n",
    "                elif isinstance(element, LTFigure):\n",
    "                    # Process figures or complex graphics\n",
    "                    pass\n",
    "\n",
    "extract_table_elements(\"1.04_m.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ec2e4-d20b-4ccc-ad00-3bc41b00d48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fbabbe-203e-4272-8aa5-95706945a8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7f668-af25-4642-9b2c-0d2a18b5db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_tables_with_pdfplumber(pdf_bytes):\n",
    "    pdf_stream = BytesIO(pdf_bytes)\n",
    "    with pdfplumber.open(pdf_stream) as pdf:\n",
    "        for page_number, page in enumerate(pdf.pages, start=1):\n",
    "            print(f\"\\n---Page {page_number}\")\n",
    "            tables = page.extract_tables()\n",
    "            #for table_number, table in enumerate(tables, start=1):\n",
    "            #    print(f\"Table {table_number}\")\n",
    "            #    for row in table:\n",
    "            #        print('\\t'.join(str(cell) if cell is not None else '' for cell in row))\n",
    "            if page_number == 4:\n",
    "                return tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6da846-c33d-4f32-ab81-76c18d01cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = extract_tables_with_pdfplumber(pdf_bytes)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4f21e-6e08-4de0-aceb-32f02d57a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb5b1f-ca28-494d-b0cf-75b34a69c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b082a-954d-407a-8fe1-26dca619accb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170c355-5e2d-49b4-9bd8-6f34189cff9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59972e5-e692-4882-bec3-dbefba46f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import (\n",
    "    LTPage,\n",
    "    LTTextContainer,\n",
    "    LTChar,\n",
    "    LTTextLine,\n",
    "    LTTextLineHorizontal,\n",
    ")\n",
    "\n",
    "def extract_tables_from_pdf_bytes(pdf_bytes):\n",
    "    \"\"\"\n",
    "    Extracts and prints table data from a PDF byte stream using pdfminer.six.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_bytes: Bytes of the PDF file.\n",
    "    \"\"\"\n",
    "    pdf_stream = BytesIO(pdf_bytes)\n",
    "\n",
    "    # For each page in the PDF\n",
    "    for page_number, page_layout in enumerate(extract_pages(pdf_stream), start=1):\n",
    "        print(f\"\\n---Page {page_number}\")\n",
    "\n",
    "        # Collect text elements with their positions\n",
    "        text_elements = []\n",
    "\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                for text_line in element:\n",
    "                    if isinstance(text_line, (LTTextLine, LTTextLineHorizontal)):\n",
    "                        line_text = ''\n",
    "                        # Collect characters in the text line\n",
    "                        for char in text_line:\n",
    "                            if isinstance(char, LTChar):\n",
    "                                line_text += char.get_text()\n",
    "\n",
    "                        # Append text line with position info\n",
    "                        x0, y0, x1, y1 = text_line.bbox\n",
    "                        text_elements.append({\n",
    "                            'text': line_text.strip(),\n",
    "                            'x0': x0,\n",
    "                            'x1': x1,\n",
    "                            'y0': y0,\n",
    "                            'y1': y1,\n",
    "                        })\n",
    "\n",
    "        # Group text elements into rows based on y-coordinate with a tolerance\n",
    "        rows = []\n",
    "        tolerance = 5  # Adjust this value based on your PDF's characteristics\n",
    "        for element in text_elements:\n",
    "            placed = False\n",
    "            for row in rows:\n",
    "                if abs(element['y0'] - row['y']) <= tolerance:\n",
    "                    row['elements'].append(element)\n",
    "                    placed = True\n",
    "                    break\n",
    "            if not placed:\n",
    "                # Start a new row\n",
    "                rows.append({'y': element['y0'], 'elements': [element]})\n",
    "\n",
    "        # Sort rows by y-coordinate (from top to bottom)\n",
    "        rows.sort(key=lambda r: -r['y'])\n",
    "\n",
    "        # For each row, sort elements by x-coordinate (from left to right)\n",
    "        for row in rows:\n",
    "            sorted_elements = sorted(row['elements'], key=lambda e: e['x0'])\n",
    "            row_text = [elem['text'] for elem in sorted_elements if elem['text']]\n",
    "            # Print the row as tab-separated values\n",
    "            print('|' + '|'.join(row_text) + '|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1c18b-408c-41f8-9c05-3b24156b053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tables_from_pdf_bytes(pdf_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3654516-5747-45c3-8ec2-8a9ab7472783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec8ed0-14b1-4bb1-a42b-386d5333f725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729c32b-123b-4918-a954-d85ba76e55f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c8d84-5c94-4f65-974d-b81c48ea843a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9457420-ab65-41d8-9c25-360501f130d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pandas as pd\n",
    "\n",
    "def extract_tables_tabula(pdf_path):\n",
    "    # Read tables from PDF into a list of DataFrames\n",
    "    dfs = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
    "\n",
    "    print(f\"Total tables extracted: {len(dfs)}\")\n",
    "\n",
    "    # Iterate over DataFrames\n",
    "    for i, df in enumerate(dfs):\n",
    "        print(f\"\\nTable {i + 1}\")\n",
    "        print(df)\n",
    "\n",
    "        # Optionally, save the DataFrame to CSV or JSON\n",
    "        #df.to_csv(f'table_{i + 1}.csv', index=False)\n",
    "        # df.to_json(f'table_{i + 1}.json', orient='records')\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce9d0b-42ff-4e66-a74c-ba20d80abb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "pdf_path = '2.03_i.pdf'\n",
    "dfs = extract_tables_tabula(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea550bd-bea0-4a7d-bef8-29bd09419fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a67f1-b21f-47a6-bb67-386b773a909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40096248-9a16-4b6d-ab37-9b8fa83974d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs[2].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050e108-ff90-4425-8a3a-a67bab132d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[2].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea369e-e78e-4691-992e-485e6fb47b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243859b1-e947-47bc-bfdd-6e4054025aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2364c6c5-eeed-4831-8e1e-f47fe85b69d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bba7ddb2-6b89-497f-8f4f-abfc272c12b1",
   "metadata": {},
   "source": [
    "### TEST: gpt-4o OCR VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99ea39-49df-46d2-9974-b692afc0ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types.beta.threads.message_create_params import (\n",
    "    Attachment,\n",
    "    AttachmentToolFileSearch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd75ed-50b3-4827-82d2-7b6f20e4631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c1c58-8009-410b-b8cc-32573439f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"1.04_m.pdf\"\n",
    "prompt = \"\"\"You have been provided with a PDF. Extract all tables in json format as A LIST OF TUPLES [(page_n, table_json), etc.].\n",
    "CAREFULLY EXTRACT TABLE HEADERS WHICH MIGHT BE COMPLICATED.\n",
    "OUTPUT ONLY A LIST OF TUPLES\"\"\"\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "pdf_assistant = client.beta.assistants.create(\n",
    "    model=\"gpt-4o\",\n",
    "    description=\"An assistant to extract the contents of PDF files.\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    name=\"PDF assistant\",\n",
    ")\n",
    "\n",
    "# Create thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "file = client.files.create(file=open(filename, \"rb\"), purpose=\"assistants\")\n",
    "\n",
    "# Create assistant\n",
    "client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    attachments=[\n",
    "        Attachment(\n",
    "            file_id=file.id, tools=[AttachmentToolFileSearch(type=\"file_search\")]\n",
    "        )\n",
    "    ],\n",
    "    content=prompt,\n",
    ")\n",
    "\n",
    "# Run thread\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=pdf_assistant.id, timeout=1000\n",
    ")\n",
    "\n",
    "if run.status != \"completed\":\n",
    "    raise Exception(\"Run failed:\", run.status)\n",
    "\n",
    "messages_cursor = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "messages = [message for message in messages_cursor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a4d43-eb91-4852-aca1-ab47024b8c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output text\n",
    "res_txt = messages[0].content[0].text.value\n",
    "res_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22e7b3-397c-4943-aba6-f55ac8073392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec2caf-ea17-4f1c-adeb-1106792c5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    extraction = ast.literal_eval(res_txt.replace(\"```python\\n\", \"\").replace(\"\\n```\", \"\"))\n",
    "except SyntaxError as e:\n",
    "    extraction = ast.literal_eval(res_txt.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\"))\n",
    "extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1980e9d-016b-43f8-b3ca-0b0d0d9ef7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dab155-38bf-44f0-ae28-f1170ba75f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c136b-aafd-4d8b-a076-9c6996edf014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27001d86-4ec7-41ae-bb7c-963b9ced423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\n",
    "    (4, '{\"columns\":[[\"Jahr\",\"Arbeitnehmende / Nichterwerbstätige\",\"Selbständigerwerbende\"],[\"1948-1968\",\"300.00\",\"600.00\"],[\"1969-1972\",\"800.00\",\"1 540.00\"],[\"1973-1975\",\"1 000.00\",\"2 000.00\"],[\"1976-1978\",\"1 000.00\",\"1 950.00\"],[\"1979-1981\",\"2 000.00\",\"3 960.00\"],[\"1982-1985\",\"2 500.00\",\"4 940.00\"],[\"1986-1989\",\"3 000.00\",\"5 930.00\"],[\"1990-1991\",\"3 208.00\",\"6 334.00\"],[\"1992-1995\",\"3 564.00\",\"7 038.00\"],[\"1996-2002\",\"3 861.00\",\"7 623.00\"],[\"2003-2006\",\"4 208.00\",\"8 307.00\"],[\"2007-2008\",\"4 406.00\",\"8 698.00\"],[\"2009-2010\",\"4 554.00\",\"8 991.00\"],[\"2011-2012\",\"4 612.00\",\"9 094.00\"],[\"2013-2018\",\"4 667.00\",\"9 333.00\"],[\"2019\",\"4 702.00\",\"9 405.00\"],[\"2020\",\"4 701.00\",\"9 402.00\"],[\"2021-2022\",\"4 747.00\",\"9 494.00\"],[\"ab 2023\",\"4 851.00\",\"9 701.00\"]]}'),\n",
    "    (5, '{\"columns\":[[\"Verjährung\",\"\"],[\"9\",\"Können die Beiträge rückwirkend entrichtet oder\"],[\"\",\"eingefordert werden?\"],[\"Nein. Werden Beiträge nicht innert fünf Jahren nach Ablauf des Kalender-\",\"\"],[\"jahres, für das sie geschuldet sind, durch Verfügung geltend gemacht, so\",\"\"],[\"können sie nicht mehr eingefordert oder entrichtet werden. Beitragslücken\",\"\"],[\"können unter Umständen zu einer späteren Rentenkürzung führen.\",\"\"],[\"Beanstandung der Eintragung\",\"\"],[\"10\",\"Kann ich eine Berichtigung verlangen?\"],[\"Sie können innert 30 Tagen nach der Zustellung des Kontoauszugs bei der\",\"\"],[\"Ausgleichskasse, die das beanstandete Konto führt, eine Berichtigung ver-\",\"\"],[\"langen, wenn Sie die Richtigkeit der Einträge nicht anerkennen. Den Ent-\",\"\"],[\"scheid  über  das  Berichtigungsbegehren  fällt  die  Ausgleichskasse  in  Form\",\"\"],[\"einer Kassenverfügung.\",\"\"]]}'),\n",
    "    (6, '{\"columns\":[[\"Auskünfte und weitere\",\"\"],[\"Informationen\",\"\"],[\"Dieses  Merkblatt  vermittelt  nur  eine  Übersicht.  Für  die  Beurteilung\",\"\"],[\"von  Einzelfällen  sind  ausschliesslich  die  gesetzlichen Bestimmungen\",\"\"],[\"massgebend. Die Ausgleichskassen und ihre Zweigstellen geben gerne\",\"\"],[\"Auskunft.  Ein  Verzeichnis  aller  Ausgleichskassen  finden  Sie  unter\",\"\"],[\"www.ahv-iv.ch.\",\"\"],[\"Die Zivilstandsbezeichnungen haben auch die folgende Bedeutung:\",\"\"],[\"•\",\"Ehe/Heirat: eingetragene Partnerschaft\"],[\"•\",\"Scheidung: gerichtliche Auflösung der Partnerschaft\"],[\"•\",\"Verwitwung: Tod der eingetragenen Partnerin / des eingetragenen\"],[\"\",\"Partners\"],[\"Herausgegeben von der Informationsstelle AHV/IV in Zusammenarbeit\",\"\"],[\"mit dem Bundesamt für Sozialversicherungen.\",\"\"],[\"Ausgabe  November  2023.  Auch  auszugsweiser  Abdruck  ist  nur  mit\",\"\"],[\"schriftlicher Einwilligung der Informationsstelle AHV/IV erlaubt.\",\"\"],[\"Dieses  Merkblatt  kann  bei  den  Ausgleichskassen  und  deren  Zweig-\",\"\"],[\"stellen sowie den IV-Stellen bezogen werden. Bestellnummer 1.04. Es\",\"\"],[\"ist ebenfalls unter www.ahv-iv.ch verfügbar.\",\"\"]]}'),\n",
    "    (7, '{\"columns\":[[\"Explications concernant l’extrait\",\"\"],[\"du Compte Individuel (CI)\",\"\"],[\"En bref\",\"\"],[\"L’extrait de compte indique tous les revenus et bonifications pour tâches\",\"\"],[\"d’assistance communiqués aux caisses de compensation.\",\"\"],[\"Les  revenus  de  l’année  courante  ne  sont  pas  encore  inscrits  et  ceux  de\",\"\"],[\"l’année précédente peuvent également ne pas l’être si la déclaration de sa-\",\"\"],[\"laire correspondante n’a pas encore été traitée. Les inscriptions au Compte\",\"\"],[\"Individuel  des  indépendants  et  des  personnes  sans  activité  lucrative  ne\",\"\"],[\"peuvent  être  effectuées  qu’une  fois  les  cotisations  définitivement  fixées.\",\"\"],[\"Il est de ce fait possible que des inscriptions manquent bien que les cotisa-\",\"\"],[\"tions AVS/AI/APG aient été payées.\",\"\"],[\"Une vidéo explicative vous montre comment demander, de manière simple\",\"\"],[\"et rapide, un extrait de votre compte individuel: www.ahv-iv.ch/r/ci\",\"\"],[\"Code de revenu\",\"\"],[\"1\",\"Quelle est la signification du code de revenu ?\"],[\"Le code de revenu se trouve dans la colonne 2 de l’extrait de compte ; s’il\",\"\"],[\"est précédé d’un chiffre, celui-ci indique une correction.\",\"\"]]}'),\n",
    "    (8, '{\"columns\":[[\"Mois de cotisation\"],[\"2 Qu’entend-on par mois de cotisation ?\"],[\"Les mois de cotisations reflètent la durée de l’activité et sont inscrits dans\"],[\"la colonne 4 et numérotés de 1 à 12. Ils sont enregistrés pour les étrangers\"],[\"depuis 1969 et pour les Suisses depuis 1979.\"],[\"3 Qu’entend-on par inscriptions particulières ?\"],[\"InscriptionSignification\"],[\"66Début ou fin de la durée de cotisation indéterminés\"],[\"77Bénéfices en capital et indemnités pour travail con-\"],[\"sacré à la famille (rémunération des domestiques, des\"],[\"moines et des religieuses, ainsi que dons)\"],[\"99Revenu (et non durée de cotisation) modifié après\"],[\"coup\"],[\"Revenu\"],[\"4 Où les revenus sont-ils inscrits ?\"],[\"Les revenus sont inscrits dans la colonne 6. Les inscriptions correspondent\"],[\"aux revenus ou aux prestations d’assurance sur lesquels des cotisations ont\"],[\"été perçues.\"],[\"5 Quel revenu est inscrit pour les personnes sans\"],[\"activité lucrative ?\"],[\"Le revenu inscrit pour les personnes sans activité lucrative est celui qui cor-\"],[\"respond aux cotisations AVS/AI/APG versées.\"],[\"6 Où sont inscrites les bonifications pour\"],[\"tâches d’assistance ?\"],[\"Seul le droit aux bonifications pour tâches d’assistance est inscrit dans la\"],[\"colonne 3. Le montant des bonifications sera fixé au moment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb7c69-55c2-464f-ac84-b3625ed2b1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfb359-67fe-416a-b6a7-4fdf16bda8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8e98b-0d40-44d9-b85e-81266d2ef2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4cee1-0931-4f57-bb61-e9717c8b3681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33a50a2b-0f60-4288-ab0f-3d1e9a1d7f33",
   "metadata": {},
   "source": [
    "### TEST: gemini OCR VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8afa1-eecf-4e2c-a4a5-693abf3506ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c575c-d947-4122-a1d5-77e92f3585e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\", None)\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e6323-14d0-45db-80ed-cf507d2fb9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bd96e-b247-423d-89d9-3ee67e1c2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./1.01_f.pdf\"\n",
    "\n",
    "sample_file = genai.upload_file(path=file_path, display_name=\"1.01_f.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70b724-7cd2-4d64-96df-e973dd7e35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content([sample_file,\n",
    "                                   \"Extract all sections (red headers) and following text paragraph in JSON format {section: paragraph}. If a paragraph contains subsection headers (usually bold and numbered) \\\n",
    "                                   create a nested dict (eg. {section: {subsection: paragraph}}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a35f9-3890-4f1d-ab80-a54c5cc26c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55707f-eb74-40ad-8248-e6ff1bff93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2414728-f75c-4da3-b4c4-11e4f865237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf5008-3f66-488f-9a02-9cdd69c56c53",
   "metadata": {},
   "source": [
    "### TEST: PDF TO HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0d7f6-1daa-4e56-9e8d-7d4d4df056c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "\n",
    "def pdf_bytes_to_html(pdf_bytes):\n",
    "    # Extract text using pdfminer\n",
    "    text = extract_text(io.BytesIO(pdf_bytes))\n",
    "\n",
    "    # Create an HTML template\n",
    "    html_content = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; }}\n",
    "        p {{ margin: 0; padding: 5px; }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    \"\"\"\n",
    "\n",
    "    # Process text\n",
    "    for line in text.split('\\n'):\n",
    "        if line.strip():  # Skip empty lines\n",
    "            html_content += f\"<p>{line}</p>\"\n",
    "\n",
    "    html_content += \"</body></html>\"\n",
    "\n",
    "    # Handling images and layout using PyMuPDF\n",
    "    # Save the PDF to a temporary file and open it with PyMuPDF\n",
    "    temp_pdf = io.BytesIO(pdf_bytes)\n",
    "    doc = fitz.open(stream=temp_pdf, filetype=\"pdf\")\n",
    "\n",
    "    # Extract images and add them to HTML\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        images = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_name = f\"image_page{page_num + 1}_{img_index}.{image_ext}\"\n",
    "            # Save the image locally or use base64 encoding to embed it directly in HTML\n",
    "\n",
    "            # For simplicity, this example assumes you save images and link them\n",
    "            with open(image_name, \"wb\") as image_file:\n",
    "                image_file.write(image_bytes)\n",
    "\n",
    "            # Add image reference to HTML\n",
    "            html_content += f'<img src=\"{image_name}\" alt=\"Page {page_num + 1} Image {img_index}\"><br>'\n",
    "\n",
    "    html_content += \"</body></html>\"\n",
    "\n",
    "    # Close the PDF document\n",
    "    doc.close()\n",
    "\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81ed3f-d5db-44e9-af51-43534fed4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(io.BytesIO(content[0].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40d468-3f30-461a-85ba-a972f4a11893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e749d-db22-4038-aa5a-122273bd0675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d12d9-1893-43e6-bf8a-9262bf5a6b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b3b6a-69ea-4667-a7dd-0c8eb8beb3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ad7b0-affd-4361-ae9a-1f1381be5d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99600e-ae00-4ff7-8d35-c391d6252543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af160ec-a3b9-4f44-b0c3-88cf89f0e591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc80590-7b89-4208-a283-ee70cf26e9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af15cd95-1241-4a9f-8c91-66737df874bc",
   "metadata": {},
   "source": [
    "##### -------- END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6c51e-b2fa-4e3f-a3e2-bb1aebde3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = parser.convert_to_documents(content)\n",
    "\n",
    "# Remove empty documents\n",
    "documents = parser.remove_empty_documents(documents[\"documents\"])\n",
    "\n",
    "# Clean documents\n",
    "documents = parser.clean_documents(documents)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b7fbe-65af-45ce-97e5-220c7475f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3773cb7-05c9-4259-8fb1-6af8307990bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[\"documents\"][0].content)\n",
    "print(documents[\"documents\"][0].meta[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f97e6d-588d-4427-9f2a-15b3bc2ae0e0",
   "metadata": {},
   "source": [
    "### 5. Chunk documents by subtopic header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda8830-2e51-4a2b-83cc-6fd021ead078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f26f2-0e67-45ef-9ef0-fce122698ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = documents[\"documents\"][1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc17f10-308d-4498-a63d-4195d62ef6ed",
   "metadata": {},
   "source": [
    "#### TO DO: dict with {pdf_name: sections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d20be-67cc-495a-bc73-ddef231d965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = {\n",
    "    \"https://www.ahv-iv.ch/p/1.01.d\": [\"Auf einen Blick\", \"Antrag für den Kontoauszug\", \"Beanstandung der Eintragungen\"],\n",
    "    \"https://www.ahv-iv.ch/p/1.02.d\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.03.d\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.04.d\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.05.d\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.07.d\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.01.f\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.02.f\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.03.f\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.04.f\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.05.f\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.07.f\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.01.i\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.02.i\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.03.i\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.04.i\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.05.i\": [],\n",
    "    \"https://www.ahv-iv.ch/p/1.07.i\": [],\n",
    "    \"https://www.ahv-iv.ch/p/6.08.d\": [\"Auf einen Blick\", \"Anspruch\", \"Unterstellung\", \"Finanzierung\", \"Verfahren\", \"Auskünfte und weitere Informationen\"],\n",
    "    \"https://www.ahv-iv.ch/p/6.09.d\": [\"Auf einen Blick\", \"Anspruch\", \"Anspruchskonkurrenz und Differenzzahlung bei derselben Person\", \"Anspruchskonkurrenz und Differenzzahlung bei verschiedenen Personen\", \"Beispiele zur Anspruchskonkurrenz, wenn FamZG und FLG betroffen sind\", \"Finanzierung\", \"Verfahren\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb175a1-77b3-465d-b7ad-71b72a0f3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections = [\n",
    "#    \"Auf einen Blick\",\n",
    "#    \"Anspruch\",\n",
    "#    \"Unterstellung\",\n",
    "#    \"Finanzierung\",\n",
    "#    \"Verfahren\",\n",
    "#    \"Auskünfte und weitere Informationen\"\n",
    "#]\n",
    "\n",
    "sections = [\n",
    "    \"Auf einen Blick\",\n",
    "    \"Anspruch\",\n",
    "    \"Anspruchskonkurrenz und Differenzzahlung bei derselben Person\",\n",
    "    \"Anspruchskonkurrenz und Differenzzahlung bei verschiedenen Personen\",\n",
    "    \"Beispiele zur Anspruchskonkurrenz, wenn FamZG und FLG betroffen sind\",\n",
    "    \"Finanzierung\",\n",
    "    \"Verfahren\",\n",
    "]\n",
    "\n",
    "#sections = sections_608 + sections_609\n",
    "#sections = list(set(sections))\n",
    "\n",
    "# Construct regex pattern\n",
    "patterns = [rf\"[\\n\\x0c]?\\d*{re.escape(section)}\\n\" for section in sections]\n",
    "pattern = '|'.join(patterns)\n",
    "\n",
    "splits = re.split(pattern, text)\n",
    "\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721af88c-0c89-4d2e-aea0-d251bea717b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_with_section = []\n",
    "\n",
    "for split, sec in zip(splits[1:], sections):\n",
    "    split = sec + \"\\n\\n\" + split\n",
    "    splits_with_section.append(split)\n",
    "    print(split)\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed4235-4e2a-431d-bd54-0cf1070ad396",
   "metadata": {},
   "source": [
    "#### Remove footer (Weitere Informationen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f11981-01a3-4b34-9712-c08f1e3dd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "footer = [r\"\\x0c12Auskünfte und weitere Informationen\",\n",
    "             r\"Dieses Merkblatt vermittelt nur eine Übersicht.*\"]\n",
    "\n",
    "clean_splits = []\n",
    "for split in splits_with_section:\n",
    "    for pattern in footer:\n",
    "        split = re.sub(pattern, '', split, flags=re.DOTALL)\n",
    "        split = split.replace(\"12Auskünfte und weitere Informationen\", \"\")\n",
    "    clean_splits.append(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e019d-6e49-43d4-b2ce-fe0013978041",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60becfce-75df-46d2-bf04-15b75ef855cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in clean_splits:\n",
    "    print(split)\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad655138-ae06-44b1-a33e-c75759cfb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge split 0 with all splits\n",
    "#header = clean_splits[0]\n",
    "\n",
    "#final_splits = []\n",
    "#for split in clean_splits:\n",
    "#    split_with_header = header + \"\\n\\n\" + split\n",
    "#    final_splits.append(split_with_header)\n",
    "#    print(split_with_header)\n",
    "#    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ff8fb-c20e-4e4c-91ed-fb0846c31267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for split in final_splits:\n",
    "#    print(split)\n",
    "#    print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b42b4-a5de-4a95-b690-3ec48f39b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 8191\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e01b7-f5d3-4d17-bd1a-fa2c9dd30fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = True\n",
    "to_csv = True\n",
    "upsert = False\n",
    "csv = []\n",
    "\n",
    "for i, doc in enumerate(clean_splits):\n",
    "\n",
    "    n_tokens = len(tokenizer.encode(doc))\n",
    "    if n_tokens > max_tokens:\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        text = doc\n",
    "        url = documents[\"documents\"][0].meta[\"url\"]\n",
    "        language = \"de\"\n",
    "        # CAREFUL !!!!!!\n",
    "        tag = \"Familienzulagen\"\n",
    "        if to_csv:\n",
    "            csv.append({\n",
    "                \"url\": url,\n",
    "                \"text\": text,\n",
    "                \"source\": sitemap_url,\n",
    "                \"tag\": tag\n",
    "            })\n",
    "        if upsert:\n",
    "            document_service.upsert(db, DocumentCreate(url=url, text=text, source=sitemap_url, tag=tag), embed=embed)\n",
    "\n",
    "if to_csv:\n",
    "    pd.DataFrame(csv).to_csv(\"indexing/data/parsed/FZ_noheader_1.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00754fb-ea15-4b5c-9522-37cd8c3475c5",
   "metadata": {},
   "source": [
    "#### TO DO:\n",
    "1. evaluate retrieval + on this chunking/parsing\n",
    "2. evaluate retrieval on **adding short topic/subtopic summary as header** (--> see medium article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7974c-994a-4e77-b20d-656a8864fd9f",
   "metadata": {},
   "source": [
    "# Continue with non FZ sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45ea36-fd02-4618-a8ba-c86cef61458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ae544-ffaf-4fbf-910c-e19c4d06f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [url.split(\"/\")[-1] for url in url_list]\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3bb5a1-0deb-4462-9179-86b28beabc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {\n",
    "    \"Allgemeines\": [\"1.01\", \"1.02\", \"1.03\", \"1.04\", \"1.05\", \"1.07\"],\n",
    "    \"Beiträge-AHV-IV-EO-ALV\": [\"2.01\", \"2.02\", \"2.03\", \"2.04\", \"2.05\", \"2.06\", \"2.07\", \"2.08\", \"2.09\", \"2.10\", \"2.11\", \"2.12\"],\n",
    "    \"Leistungen-der-AHV\": [\"31\", \"3.01\", \"3.02\", \"3.03\", \"3.04\", \"3.05\", \"3.06\", \"3.07\", \"3.08\"],\n",
    "    \"Leistungen-der-IV\": [\"4.01\", \"4.02\", \"4.03\", \"4.04\", \"4.05\", \"4.06\", \"4.07\", \"4.08\", \"4.09\", \"4.11\", \"4.12\", \"4.13\", \"4.14\", \"4.15\", \"4.16\"],\n",
    "    \"Ergänzungsleistungen-zur-AHV-und-IV\": [\"5.01\", \"5.02\", \"51\", \"52\"],\n",
    "    \"Überbrückungsleistungen\": [\"5.03\"],\n",
    "    \"Leistungen-der-EO-MSE-EAE-BUE-AdopE\": [\"6.01\", \"6.02\", \"6.04\", \"6.10\", \"6.11\"],\n",
    "    \"International\": [\"10.01\", \"10.02\", \"10.03\", \"11.01\", \"880\", \"890\"],\n",
    "    \"Andere-Sozialversicherungen\": [\"6.05\", \"6.06\", \"6.07\"],\n",
    "    \"Jährliche-Neuerungen\": [\"1.2024\", \"1.2023\", \"1.2021\", \"1.2020\", \"1.2019\", \"1.2016\", \"1.2015\", \"1.2014\", \"1.2013\", \"1.2012\", \"1.2011\", \"1.2009\", \"1.2008\", \"1.2007\", \"1.2005\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd971b37-c2ed-45c3-80f1-6ea06679f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tag_key(tags, search_string):\n",
    "    for key, values in tags.items():\n",
    "        if search_string in values:\n",
    "            return key\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76851ef9-90b8-4931-ac9b-1e4621e41e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098ba94-994b-4ba3-b860-ec7bbb3b634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from schemas.document import DocumentCreate\n",
    "\n",
    "embed = True\n",
    "max_tokens = 8192\n",
    "long_docs = []\n",
    "\n",
    "for i, doc in enumerate(chunks[\"documents\"]):\n",
    "\n",
    "    n_tokens = len(tokenizer.encode(doc.content))\n",
    "    if n_tokens > max_tokens:\n",
    "        print(i)\n",
    "        long_docs.append(doc)\n",
    "    else:\n",
    "        text = doc.content\n",
    "        url = doc.meta[\"url\"]\n",
    "        language = \"fr\"\n",
    "        pdf_id = doc.meta[\"url\"].split(\"/\")[-1].replace(\".f\", \"\")\n",
    "        tag = find_tag_key(tags, pdf_id)\n",
    "        print(tag)\n",
    "        document_service.upsert(db, DocumentCreate(url=url, text=text, source=sitemap_url, tag=tag), embed=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82847fb9-3763-4732-8268-8caa4a1ff8d6",
   "metadata": {},
   "source": [
    "# Long docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80cd40-0040-41fb-bb6b-bfec3c180bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef5a46-1ce0-4aa3-bddd-abc0179bb6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9bd703-929d-45c0-a337-61e139830e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74002cd-02d4-4c2a-9252-65892b6efaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ad04d-5400-4842-9a75-bf17479f0b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa8461-ede6-4038-beaf-46f3ad333513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8991ee31-44d2-4221-a3bf-e5232bf6143a",
   "metadata": {},
   "source": [
    "### Evaluate RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582dbfb-0bac-4007-9541-14f996d906b6",
   "metadata": {},
   "source": [
    "# EVAL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344643e-fe8f-4d54-aef7-3c09499ca0c6",
   "metadata": {},
   "source": [
    "### Get all FZ docs (unchunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b0f8e-0490-4910-9555-cc3bba8a9bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = document_service.get_all_documents(db)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c234a-3a51-4655-83ae-6960b04a6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(doc.text, doc.url)\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3a2167-ade3-4a5a-94fe-ea8669e46ac0",
   "metadata": {},
   "source": [
    "### Evaluate retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c946ad3-6877-4e98-b00e-fef9980d795e",
   "metadata": {},
   "source": [
    "- Is correct doc retrieved for FZ questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad840a-9236-4329-a1fa-91908ee268f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FZ questions\n",
    "fz_eval = pd.read_csv(\"indexing/data/memento_eval_qa_FZ.csv\")\n",
    "fz_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958d32f-59dd-4b2c-967f-cdc44a7c80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969149c4-200d-4e45-81a7-749fee16d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = {}\n",
    "\n",
    "for question in fz_eval.question:\n",
    "    request = RAGRequest(query=question)\n",
    "    doc = processor.retrieve(db, request, language=None, tag=None, k=k)\n",
    "    recall[question] = doc\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30f85e-093c-43b5-b921-8f6ef4c5a407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea4df4-e309-40ce-82ca-c9c6987600e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_recall = {}\n",
    "for (question, doc), url in zip(recall.items(), fz_eval.url):\n",
    "    #retrieval_recall[doc[0].url] = 1 if doc[0].url == url else 0\n",
    "    retrieval_recall[question] = 1 if url.replace(\"www.\", \"\") in [d.url for d in doc] else 0\n",
    "    print(question)\n",
    "    print(\"\\n\".join([d.url for d in doc]))\n",
    "    print(\"----------------------\")\n",
    "    print(url)\n",
    "    print(\"----------------------\")\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f95c7-5a25-4024-a0c4-dd4721d85803",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(retrieval_recall.values())/len(retrieval_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99fff2a-34a4-4aa7-a754-c2aa611e6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffff2c7-38c1-4bce-b225-108e25f51b94",
   "metadata": {},
   "source": [
    "# Retrieval results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99019d8d-0884-4c25-82ed-22fb1d8db234",
   "metadata": {},
   "source": [
    "eak.admin.ch\n",
    "\n",
    "avg recall\n",
    "- TopKRetriever(k=1), text-embedding-ada-002: 0.375\n",
    "- TopKRetriever(k=10), text-embedding-ada-002: 0.905\n",
    "- **top_k_retriever(k=100), reranking(k=5), text-embedding-ada-002: 1**\n",
    "- TopKRetriever(k=1), text-embedding-3-small: 0 --> NEED TO RE-EMBED\n",
    "- TopKRetriever(k=10), text-embedding-3-small: 0.048 --> NEED TO RE-EMBED\n",
    "\n",
    "ahv-iv\n",
    "\n",
    "avg recall\n",
    "- TopKRetriever(k=1), text-embedding-ada-002: 0.069\n",
    "- TopKRetriever(k=10), text-embedding-ada-002: 0.483\n",
    "- top_k_retriever(k=100), reranking(k=5), text-embedding-ada-002: 0.79\n",
    "- - **top_k_retriever(k=100), reranking(k=10), text-embedding-ada-002: 0.897** --> need to solve large pdf chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5c31f-6309-49d4-a720-fd0f4e5ca9dd",
   "metadata": {},
   "source": [
    "### Make request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330db690-943b-4067-a9bb-46a55a3ba19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = RAGRequest(query=\"hello\")\n",
    "\n",
    "# test\n",
    "processor.retrieve(db, request, language=None, tag=None, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5682e732-3c54-4055-a33b-098de6184124",
   "metadata": {},
   "source": [
    "### Setup LLM client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ab3ff-37fa-4d8d-b2c6-690bf06e1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client.max_output_tokens = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9688fd0-36b3-4869-ae0c-6044c900d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a 10000 token poem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f93d3-22bf-4ef3-9e34-333c3eab45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": prompt},]\n",
    "\n",
    "# test\n",
    "llm_client.generate(messages).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d372c-d3a9-4e32-b354-1f0fc94c6cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c80c94-75ee-49ff-8033-ba2fb732cc58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492f4dd-324a-4792-9ce3-62a38ad4d139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa04ea-f985-425c-ad33-ca01fb1c8d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a45a649-f516-48ff-8411-c8fadde9d311",
   "metadata": {},
   "source": [
    "# LLM chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aafa44-e895-4fca-8216-5b2e2f9946f0",
   "metadata": {},
   "source": [
    "The idea is to prompt an LLM to semantically chunk documents. This approach diverges from the semantic chunking methodology where actual text embeddings are being optimized to be as similar as possible for chunks containing similar information, and dissimilar for chunks containing dissimilar information.\n",
    "\n",
    "For each document, we chunk it into paragraphs and track the following:\n",
    "- **text**: text chunk\n",
    "- **url**: source url of the document\n",
    "- **language**: language of the document\n",
    "- **tag**: document topic\n",
    "- **n_tokens**: number of tokens per chunk\n",
    "- **parent_doc**: the url of the document from which this chunk originates\n",
    "\n",
    "We compute token statistics according to the LLM model tokenizer (here `gpt-4o`, so `cl100k_base` from tiktoken) and only call the chunker LLM to semantically chunk documents over the mean token count across documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdcd099-5901-4357-aaf7-07b6cdeb102d",
   "metadata": {},
   "source": [
    "### Retrieve content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daefd12-5134-47b6-a3f1-2a6f9189607a",
   "metadata": {},
   "source": [
    "##### https://www.eak.admin.ch/eak/de/home.sitemap.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240509f-e32b-4662-8947-95b4ffa64347",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitemap_url = \"https://www.eak.admin.ch/eak/de/home.sitemap.xml\"\n",
    "embed = False\n",
    "admin_indexer.splitter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983255e-d21d-40d2-aa9e-34b76830c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index admin data\n",
    "await admin_indexer.index(sitemap_url, db, embed=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b688b-73a6-4f9b-ad01-68670dde1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all raw documents\n",
    "docs = document_service.get_all_documents(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a109c7c-57e9-4e89-a624-78f9f423aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25db6d-3fbb-46c1-9b24-bd2b008aa6f3",
   "metadata": {},
   "source": [
    "### Compute token statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47934bf-e164-4aa0-8e48-8cccdeb5c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73092e84-152f-4be4-828a-7cde3a705e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {}\n",
    "\n",
    "for doc in docs:\n",
    "    tokens[doc.url] = {\"n_tokens\": len(tokenizer.encode(doc.text)),\n",
    "                       \"text\": doc.text}\n",
    "\n",
    "tokens_df = pd.DataFrame.from_dict(tokens, orient=\"index\")\n",
    "tokens_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74fc83-fa86-48b8-9840-15e021edb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_stats = tokens_df.describe()\n",
    "token_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba241a-d311-498f-8e88-46ec306c6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "tokens_df.plot(kind=\"bar\", ax=ax)\n",
    "plt.axhline(y=token_stats.loc[\"75%\", \"n_tokens\"]+token_stats.loc[\"std\", \"n_tokens\"], color='r', linestyle='--', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94454a0-4df5-4004-815f-fd136617a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_docs = []\n",
    "\n",
    "for i, row in tokens_df.iterrows():\n",
    "    if row.n_tokens > token_stats.loc[\"75%\", \"n_tokens\"]+token_stats.loc[\"std\", \"n_tokens\"]:\n",
    "        long_docs.append((row.name, row.text))\n",
    "\n",
    "len(long_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a066b6a-a7d2-4c13-9ad3-a9d3071c4b8f",
   "metadata": {},
   "source": [
    "#### LLM chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35336c6e-57f9-47f1-9a5a-12e67b2c4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a highly advanced language model trained for the task of segmenting documents into meaningful and independent chunks\n",
    "for Retrieval-Augmented Generation (RAG) purposes. Your goal is to process a provided document and split it into distinct chunks\n",
    "that can be understood on their own. Each chunk should contain a self-contained idea or piece of information that is unrelated to\n",
    "the other chunks.\n",
    "\n",
    "Here’s how you should approach this task:\n",
    "\n",
    "1. Chunk Identification: Carefully read through the document and identify potential breakpoints where a new, independent idea or topic begins.\n",
    "\n",
    "2. Chunk Validation: Ensure that each identified chunk can be understood independently without requiring context from previous or subsequent chunks.\n",
    "\n",
    "3. Chunk Creation: If a segment of the document can be split based on the criteria above, separate it into a distinct chunk. If not, do not split the text.\n",
    "\n",
    "4. Output Format: Provide each chunk separated by \"\\n\\n\"\n",
    "\n",
    "Remember, only create a chunk if the information it contains is unrelated to the other chunks and can be understood independently and\n",
    "extract text chunks *AS IS*, without editing them.\n",
    "\n",
    "You must try to create as large chunks as possible and ALL text must be present in the chunks.\n",
    "\n",
    "DOCUMENT: {doc}\n",
    "\n",
    "CHUNKS:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a7ab9-0893-4239-89ff-8af740f5217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tqdm.tqdm(long_docs):\n",
    "\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt.format(doc=doc[1])},]\n",
    "    res = llm_client.generate(messages).choices[0].message.content\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b1ffd-6b2c-4795-ad04-736784c9ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e1ad6-5ef5-4da2-8cc8-62aafd892ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.encode(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195b6f6-49bf-4f60-a196-b0cb839d3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1d4b5-7e3b-4e84-bdf5-050b2489ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in res.split(\"\\n\\n\"):\n",
    "    print(chunk)\n",
    "    print(\"--------_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e95ab-27f2-4537-8334-252edd53be99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

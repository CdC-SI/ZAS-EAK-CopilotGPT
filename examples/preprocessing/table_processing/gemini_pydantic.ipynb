{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: camelot-py[cv] in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (1.0.0)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (8.1.7)\n",
      "Requirement already satisfied: chardet>=5.1.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (5.2.0)\n",
      "Requirement already satisfied: numpy>=1.26.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (1.26.4)\n",
      "Requirement already satisfied: openpyxl>=3.1.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (3.1.5)\n",
      "Requirement already satisfied: pdfminer-six>=20240706 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (20240706)\n",
      "Requirement already satisfied: pypdf<6.0,>=4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (5.3.0)\n",
      "Requirement already satisfied: pandas>=2.2.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (2.2.3)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (0.9.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (4.11.0.86)\n",
      "Requirement already satisfied: pypdfium2>=4 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (4.30.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.0.1->camelot-py[cv]) (0.4.6)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.2.2->camelot-py[cv]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.2.2->camelot-py[cv]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.2.2->camelot-py[cv]) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pdfminer-six>=20240706->camelot-py[cv]) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pdfminer-six>=20240706->camelot-py[cv]) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.17.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[cv]) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: camelot-py 1.0.0 does not provide the extra 'cv'\n"
     ]
    }
   ],
   "source": [
    "%pip install camelot-py[cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "C:\\ProgramData\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "print(camelot.__version__)\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未在 PDF 文件中检测到任何表格。\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "import os\n",
    "\n",
    "def extract_tables(file_path, output_path, compress=False):\n",
    "    \"\"\"\n",
    "    使用 Camelot 从 PDF 中提取表格，并导出为 CSV 文件。\n",
    "    :param file_path: 输入 PDF 文件路径\n",
    "    :param output_path: 输出 CSV 文件路径（将存储所有提取的表格）\n",
    "    :param compress: 是否对输出进行压缩（默认 False）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 提取所有页面的表格\n",
    "        tables = camelot.read_pdf(file_path, flavor='lattice', pages='all')\n",
    "        if not tables:\n",
    "            print(\"未在 PDF 文件中检测到任何表格。\")\n",
    "            return\n",
    "        \n",
    "        # 如果输出目录不存在，则创建目录\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # 导出所有提取的表格到 CSV 文件\n",
    "        tables.export(output_path, f='csv', compress=compress)\n",
    "        print(f\"表格已成功导出至 {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"在提取表格时发生错误：\", e)\n",
    "\n",
    "def main():\n",
    "    # 在主函数中直接定义文件路径\n",
    "    file_path = \"2.02_EN.pdf\"  # 输入 PDF 文件路径\n",
    "    output_path = \"output_lattice/table.csv\"   # 输出 CSV 文件路径\n",
    "    compress = False                   # 是否压缩输出文件\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"输入文件 {file_path} 不存在。\")\n",
    "        return\n",
    "\n",
    "    extract_tables(file_path, output_path, compress)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "这是文本型 PDF\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"2.02_EN.pdf\")\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() or \"\"\n",
    "if text.strip():\n",
    "    print(\"这是文本型 PDF\")\n",
    "else:\n",
    "    print(\"这可能是图像型 PDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymupdf in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (1.25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未找到表格\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "\n",
    "def extract_pdf_table_to_json(pdf_path, page_num=0, output_file=\"table.json\"):\n",
    "    \"\"\"\n",
    "    提取 PDF 指定页面的第一个表格并转换为 JSON\n",
    "    :param pdf_path: PDF 文件路径\n",
    "    :param page_num: 目标页面编号（默认第0页）\n",
    "    :param output_file: 输出 JSON 文件名\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    try:\n",
    "        page = doc[page_num]\n",
    "    except IndexError:\n",
    "        print(f\"错误：页面 {page_num} 不存在\")\n",
    "        return\n",
    "\n",
    "    # 查找页面中的表格\n",
    "    tables = page.find_tables()\n",
    "    \n",
    "    if len(tables.tables) == 0:\n",
    "        print(\"未找到表格\")\n",
    "        return\n",
    "    \n",
    "    # 提取第一个表格数据\n",
    "    table_data = tables[0].extract()\n",
    "    \n",
    "    # 转换为 JSON 格式（二维数组）\n",
    "    json_data = {\n",
    "        \"table\": table_data,\n",
    "        \"metadata\": {\n",
    "            \"page\": page_num + 1,  # 转换为人类可读页码\n",
    "            \"columns\": len(table_data[0]),\n",
    "            \"rows\": len(table_data)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 保存到文件\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"表格已保存至 {output_file}\")\n",
    "\n",
    "# 使用示例\n",
    "extract_pdf_table_to_json(\n",
    "    pdf_path=\"2.02_EN.pdf\",\n",
    "    page_num=0,\n",
    "    output_file=\"output.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "页面原始文本结构:\n",
      "{'width': 419.52801513671875, 'height': 595.2760009765625, 'blocks': [{'number': 1, 'type': 0, 'bbox': (99.21260070800781, 65.32991790771484, 219.97662353515625, 82.03192138671875), 'lines': [{'spans': [{'size': 14.0, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Bold', 'color': 13027014, 'alpha': 255, 'ascender': 0.9430000185966492, 'descender': -0.25, 'text': '2.02 Contributions', 'origin': (99.21260070800781, 78.53192138671875), 'bbox': (99.21260070800781, 65.32991790771484, 219.97662353515625, 82.03192138671875)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (99.21260070800781, 65.32991790771484, 219.97662353515625, 82.03192138671875)}]}, {'number': 2, 'type': 0, 'bbox': (81.49610137939453, 110.29820251464844, 363.80706787109375, 169.0712890625), 'lines': [{'spans': [{'size': 16.0, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Bold', 'color': 1907995, 'alpha': 255, 'ascender': 0.9430000185966492, 'descender': -0.25, 'text': 'Self-employed contributions to Old-', 'origin': (90.70860290527344, 125.38619995117188), 'bbox': (90.70860290527344, 110.29820251464844, 358.99169921875, 129.38619995117188)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (90.70860290527344, 110.29820251464844, 358.99169921875, 129.38619995117188)}, {'spans': [{'size': 16.0, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Bold', 'color': 1907995, 'alpha': 255, 'ascender': 0.9430000185966492, 'descender': -0.25, 'text': 'Age and Survivors’ Insurance (OASI), ', 'origin': (86.45670318603516, 145.22879028320312), 'bbox': (86.45670318603516, 130.1407928466797, 363.80706787109375, 149.22879028320312)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (86.45670318603516, 130.1407928466797, 363.80706787109375, 149.22879028320312)}, {'spans': [{'size': 16.0, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Bold', 'color': 1907995, 'alpha': 255, 'ascender': 0.9430000185966492, 'descender': -0.25, 'text': 'Disability Insurance (DI) and Income ', 'origin': (81.49610137939453, 165.0712890625), 'bbox': (81.49610137939453, 149.98329162597656, 353.4865417480469, 169.0712890625)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (81.49610137939453, 149.98329162597656, 353.4865417480469, 169.0712890625)}]}, {'number': 3, 'type': 0, 'bbox': (73.70079803466797, 207.07069396972656, 201.85189819335938, 218.68069458007812), 'lines': [{'spans': [{'size': 10.0, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Light', 'color': 1907995, 'alpha': 255, 'ascender': 0.9110000133514404, 'descender': -0.25, 'text': 'Position as of 1', 'origin': (73.70079803466797, 216.18069458007812), 'bbox': (73.70079803466797, 207.07069396972656, 138.58778381347656, 218.68069458007812)}, {'size': 5.829999923706055, 'flags': 5, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Light', 'color': 1907995, 'alpha': 255, 'ascender': 0.9110000133514404, 'descender': -0.25, 'text': 'st', 'origin': (137.9781951904297, 212.8507080078125), 'bbox': (137.9781951904297, 207.53958129882812, 142.34486389160156, 214.30821228027344)}, {'size': 10.0, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Light', 'color': 1907995, 'alpha': 255, 'ascender': 0.9110000133514404, 'descender': -0.25, 'text': ' January 2025', 'origin': (142.3459014892578, 216.18069458007812), 'bbox': (142.3459014892578, 207.07069396972656, 201.85189819335938, 218.68069458007812)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (73.70079803466797, 207.07069396972656, 201.85189819335938, 218.68069458007812)}]}, {'number': 4, 'type': 0, 'bbox': (76.535400390625, 169.82579040527344, 289.5770263671875, 188.91378784179688), 'lines': [{'spans': [{'size': 16.0, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Bold', 'color': 1907995, 'alpha': 255, 'ascender': 0.9430000185966492, 'descender': -0.25, 'text': 'Compensation Insurance (IC)', 'origin': (76.535400390625, 184.91378784179688), 'bbox': (76.535400390625, 169.82579040527344, 289.5770263671875, 188.91378784179688)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (76.535400390625, 169.82579040527344, 289.5770263671875, 188.91378784179688)}]}]}\n",
      "\n",
      "检测到 0 个表格候选区域\n",
      "已生成调试文件: debug_tables.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "def debug_pdf_tables(pdf_path, page_num=0):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_num]\n",
    "    \n",
    "    # 查看页面文本结构\n",
    "    print(\"页面原始文本结构:\")\n",
    "    print(page.get_text(\"dict\"))\n",
    "    \n",
    "    # 检测所有表格\n",
    "    tables = page.find_tables()\n",
    "    print(f\"\\n检测到 {len(tables.tables)} 个表格候选区域\")\n",
    "    \n",
    "    # 绘制表格识别框（可视化调试）\n",
    "    for i, table in enumerate(tables.tables):\n",
    "        print(f\"表格 {i+1} 的边界框坐标:\", table.bbox)\n",
    "        # 在PDF上绘制红色框\n",
    "        highlight = page.add_highlight_annot(table.bbox)\n",
    "        highlight.set_colors(stroke=(1, 0, 0))  # 红色边框\n",
    "        highlight.update()\n",
    "    \n",
    "    # 保存带标注的PDF\n",
    "    doc.save(\"debug_tables.pdf\")\n",
    "    print(\"已生成调试文件: debug_tables.pdf\")\n",
    "\n",
    "debug_pdf_tables(\"2.02_EN.pdf\", page_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfplumber) (10.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 22.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 21.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pdfminer.six, pdfplumber\n",
      "  Attempting uninstall: pdfminer.six\n",
      "    Found existing installation: pdfminer.six 20240706\n",
      "    Uninstalling pdfminer.six-20240706:\n",
      "      Successfully uninstalled pdfminer.six-20240706\n",
      "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pdfplumber.exe is installed in 'C:\\Users\\shaer\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "camelot-py 1.0.0 requires pdfminer-six>=20240706, but you have pdfminer-six 20231228 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Year of birth Reference age\n",
      "0    Up to 1960            64\n",
      "Empty DataFrame\n",
      "Columns: [1962, 64 plus 6 months]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [From 1964, 65]\n",
      "Index: []\n",
      "  Contribution rates    None\n",
      "0               OASI   8.1 %\n",
      "1                 DI   1.4 %\n",
      "2                 IC   0.5 %\n",
      "3              Total  10.0 %\n",
      "   Annual income in francs           None  \\\n",
      "0              of at least  but less than   \n",
      "1                   10 100         17 600   \n",
      "2                   17 600         23 000   \n",
      "3                   23 000         25 500   \n",
      "4                   25 500         28 000   \n",
      "5                   28 000         30 500   \n",
      "6                   30 500         33 000   \n",
      "7                   33 000         35 500   \n",
      "8                   35 500         38 000   \n",
      "9                   38 000         40 500   \n",
      "10                  40 500         43 000   \n",
      "11                  43 000         45 500   \n",
      "12                  45 500         48 000   \n",
      "13                  48 000         50 500   \n",
      "14                  50 500         53 000   \n",
      "15                  53 000         55 500   \n",
      "16                  55 500         58 000   \n",
      "17                  58 000         60 500   \n",
      "18                  60 500                  \n",
      "\n",
      "   OASI/DI/IC contribu-\\ntion rate as percent-\\nage of earnings  \n",
      "0                                                None            \n",
      "1                                               5.371            \n",
      "2                                               5.494            \n",
      "3                                               5.617            \n",
      "4                                               5.741            \n",
      "5                                               5.864            \n",
      "6                                               5.987            \n",
      "7                                               6.235            \n",
      "8                                               6.481            \n",
      "9                                               6.728            \n",
      "10                                              6.976            \n",
      "11                                              7.222            \n",
      "12                                              7.469            \n",
      "13                                              7.840            \n",
      "14                                              8.209            \n",
      "15                                              8.580            \n",
      "16                                              8.951            \n",
      "17                                              9.321            \n",
      "18                                             10.000            \n",
      "   Year Interest rate\n",
      "0  2016         0,0 %\n",
      "1  2017         0,5 %\n",
      "2  2018         0,5 %\n",
      "3  2019         0,0 %\n",
      "4  2020         0,0 %\n",
      "5  2021         0,0 %\n",
      "6  2022         1,5 %\n",
      "7  2023         2,0 %\n",
      "                                          Relates to  \\\n",
      "0                          Contributions on\\naccount   \n",
      "1  Difference between\\ncontributions on account\\n...   \n",
      "\n",
      "    Payment not\\nreceived within         Interest accrued\\nfrom  \n",
      "0  30 days after end\\nof quarter  1st day after end\\nof quarter  \n",
      "1     30 days from invoice\\ndate     1st day from invoice\\ndate  \n",
      "                                          Relates to  \\\n",
      "0  Contributions on account amount\\nto less than ...   \n",
      "1                   Contributions for previous years   \n",
      "\n",
      "                               Interest accrued from  \n",
      "0  1st January after the calendar year\\nfollowing...  \n",
      "1  1st January after end of relevant\\ncontributio...  \n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "with pdfplumber.open(\"2.02_EN.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        tables = page.extract_tables()\n",
    "        for table in tables:\n",
    "            # 直接转成DataFrame\n",
    "            import pandas as pd\n",
    "            df = pd.DataFrame(table[1:], columns=table[0])\n",
    "            print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "页码 1 未检测到表格\n",
      "页码 2 未检测到表格\n",
      "页码 3 未检测到表格\n",
      "已保存表格：page_4_table_1.json\n",
      "已保存表格：page_4_table_2.json\n",
      "已保存表格：page_4_table_3.json\n",
      "已保存表格：page_5_table_1.json\n",
      "已保存表格：page_5_table_2.json\n",
      "已保存表格：page_6_table_1.json\n",
      "页码 7 未检测到表格\n",
      "已保存表格：page_8_table_1.json\n",
      "已保存表格：page_8_table_2.json\n",
      "页码 9 未检测到表格\n",
      "页码 10 未检测到表格\n",
      "页码 11 未检测到表格\n",
      "页码 12 未检测到表格\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def tables_to_json(pdf_path, output_dir=\"output_tables\"):\n",
    "    \"\"\"\n",
    "    将PDF中所有表格转为统一JSON格式\n",
    "    :param pdf_path: PDF文件路径\n",
    "    :param output_dir: 输出目录\n",
    "    \"\"\"\n",
    "    # 创建输出目录\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            # 提取当前页所有表格\n",
    "            tables = page.extract_tables()\n",
    "            \n",
    "            if not tables:\n",
    "                print(f\"页码 {page_num+1} 未检测到表格\")\n",
    "                continue\n",
    "                \n",
    "            for table_num, table in enumerate(tables, 1):\n",
    "                # 清洗数据（处理None值和换行符）\n",
    "                cleaned_table = [\n",
    "                    [\n",
    "                        cell.replace(\"\\n\", \" \").strip() if cell is not None else \"\"\n",
    "                        for cell in row\n",
    "                    ]\n",
    "                    for row in table\n",
    "                ]   \n",
    "                # 构建统一JSON结构\n",
    "                json_data = {\n",
    "                    \"metadata\": {\n",
    "                        \"source\": pdf_path,\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"table_number\": table_num,\n",
    "                        \"dimensions\": {\n",
    "                            \"rows\": len(cleaned_table),\n",
    "                            \"columns\": len(cleaned_table[0]) if cleaned_table else 0\n",
    "                        }\n",
    "                    },\n",
    "                    \"data\": {\n",
    "                        \"headers\": cleaned_table[0] if cleaned_table else [],\n",
    "                        \"rows\": cleaned_table[1:] if len(cleaned_table) > 1 else []\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # 生成文件名\n",
    "                filename = f\"page_{page_num+1}_table_{table_num}.json\"\n",
    "                output_path = Path(output_dir) / filename\n",
    "                \n",
    "                # 保存JSON文件\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                print(f\"已保存表格：{filename}\")\n",
    "\n",
    "# 使用示例\n",
    "tables_to_json(\n",
    "    pdf_path=\"2.02_EN.pdf\",\n",
    "    output_dir=\"plumber_json_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存表格：page_4_table_1.json\n",
      "已保存表格：page_5_table_1.json\n",
      "已保存表格：page_5_table_2.json\n",
      "已保存表格：page_6_table_1.json\n",
      "已保存表格：page_8_table_1.json\n",
      "已保存表格：page_8_table_2.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def process_pdf_tables(input_dir=\".\", output_base=\"output_tables\"):\n",
    "    \"\"\"\n",
    "    批量处理指定目录下的所有PDF文件表格\n",
    "    :param input_dir: 输入目录路径（默认当前目录）\n",
    "    :param output_base: 输出根目录（默认output_tables）\n",
    "    \"\"\"\n",
    "    # 获取所有PDF文件（包括子目录）\n",
    "    pdf_files = glob.glob(f\"{input_dir}/**/*.pdf\", recursive=True)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"在 {input_dir} 目录中未找到PDF文件\")\n",
    "        return\n",
    "\n",
    "    for pdf_path in pdf_files:\n",
    "        try:\n",
    "            # 创建对应输出目录\n",
    "            pdf_stem = Path(pdf_path).stem  # 获取不带扩展名的文件名\n",
    "            output_dir = Path(output_base) / pdf_stem\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            print(f\"\\n正在处理文件：{Path(pdf_path).name}\")\n",
    "            \n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                process_single_pdf(doc, pdf_path, output_dir)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {pdf_path} 失败：{str(e)}\")\n",
    "\n",
    "def process_single_pdf(doc, pdf_path, output_dir):\n",
    "    \"\"\"处理单个PDF文件\"\"\"\n",
    "    for page_index in range(len(doc)):\n",
    "        page = doc[page_index]\n",
    "        tables = page.find_tables()\n",
    "        \n",
    "        if not tables:\n",
    "            print(f\"  第 {page_index + 1} 页没有检测到表格\")\n",
    "            continue\n",
    "\n",
    "        for table_num, table in enumerate(tables, start=1):\n",
    "            try:\n",
    "                # 提取表格数据\n",
    "                table_data = table.extract()\n",
    "                \n",
    "                # 构建JSON结构\n",
    "                json_data = {\n",
    "                    \"metadata\": {\n",
    "                        \"source\": str(pdf_path),\n",
    "                        \"page\": page_index + 1,\n",
    "                        \"table_number\": table_num,\n",
    "                        \"bbox\": list(table.bbox),\n",
    "                        \"dimensions\": {\n",
    "                            \"rows\": len(table_data),\n",
    "                            \"columns\": len(table_data[0]) if table_data else 0\n",
    "                        }\n",
    "                    },\n",
    "                    \"data\": table_data\n",
    "                }\n",
    "                \n",
    "                # 生成文件名\n",
    "                filename = f\"page_{page_index+1}_table_{table_num}.json\"\n",
    "                output_path = output_dir / filename\n",
    "                \n",
    "                # 保存文件\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                print(f\"  已保存表格：{filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  表格处理失败：{str(e)}\")\n",
    "\n",
    "# 使用示例（处理当前目录及其子目录下的所有PDF）\n",
    "process_pdf_tables(\n",
    "    input_dir=\".\", \n",
    "    output_base=\"output_tables\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camelot\n",
    "Stream\n",
    "Suitable for tables where whitespaces between cells simulate the table structure. It leverages PDFMiner's functionality to group characters into words and sentences, analyzing margins to infer table boundaries. Ideal for borderless tables but struggles with complex layouts.\n",
    "\n",
    "Lattice\n",
    "Designed for tables with explicit demarcation lines. Detects line segments and intersections via image processing (using OpenCV) to define precise table boundaries. Highly accurate for multi-table pages and merged cells but fails for borderless tables.\n",
    "\n",
    "Network\n",
    "Relies on text element bounding boxes to identify horizontal/vertical alignment patterns. Effective for tables without lines but with strong text alignment. Struggles with irregular or loosely structured layouts.\n",
    "\n",
    "Hybrid\n",
    "Combines Network's text alignment analysis with Lattice's line detection. Uses Lattice's precise boundaries to enhance Network results. Optimized for mixed-layout tables (partially lined + text-aligned) but computationally intensive.\n",
    "\n",
    "While Camelot's official examples include similar tables, none of the four modes worked reliably for our specific case due to irregular text alignment and partial/no borders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未检测到任何表格\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_tables_with_camelot(pdf_path, output_dir=\"camelot_output\"):\n",
    "    \"\"\"\n",
    "    使用Camelot提取表格并保存为JSON\n",
    "    :param pdf_path: PDF文件路径\n",
    "    :param output_dir: 输出目录\n",
    "    \"\"\"\n",
    "\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "  \n",
    "    try:\n",
    "        tables = camelot.read_pdf(\n",
    "            pdf_path, \n",
    "            flavor=\"network\",\n",
    "            edge_tol=50,   \n",
    "            row_tol=10      \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"文件解析失败: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    if not tables:\n",
    "        print(\"未检测到任何表格\")\n",
    "        return\n",
    "\n",
    "    for table_num, table in enumerate(tables, 1):\n",
    "        df = table.df\n",
    "        cleaned_table = [\n",
    "            [cell.strip().replace(\"\\n\", \" \") if cell else \"\" \n",
    "             for cell in row\n",
    "            ]\n",
    "            for row in df.values.tolist()\n",
    "        ]\n",
    "             \n",
    "        json_data = {\n",
    "            \"metadata\": {\n",
    "                \"source\": pdf_path,\n",
    "                \"page\": table.page + 1,  \n",
    "                \"table_number\": table_num,\n",
    "                \"accuracy\": round(table.parsing_report[\"accuracy\"], 2),\n",
    "                \"dimensions\": {\n",
    "                    \"rows\": len(cleaned_table),\n",
    "                    \"columns\": len(cleaned_table[0]) if cleaned_table else 0\n",
    "                }\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"headers\": cleaned_table[0] if cleaned_table else [],\n",
    "                \"rows\": cleaned_table[1:] if len(cleaned_table) > 1 else []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        \n",
    "        filename = f\"page_{table.page+1}_table_{table_num}.json\"\n",
    "        output_path = Path(output_dir) / filename\n",
    "        \n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"已保存表格：{filename}\")\n",
    "\n",
    "extract_tables_with_camelot(\n",
    "    pdf_path=\"2.01_EN.pdf\",\n",
    "    output_dir=\"camelot_json_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "处理第 1 个表格 (页面 2)\n",
      "解析报告: {'accuracy': 100.0, 'whitespace': 33.91, 'order': 1, 'page': 1}\n",
      "边界框: (31.56, 51.966239999999885, 738.46704, 590.64576)\n",
      "已保存表格数据：page_2_table_1.json\n",
      "可视化出错: PlotMethods.__call__() got an unexpected keyword argument 'figsize'\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "def extract_tables_with_camelot(pdf_path, output_dir=\"camelot_output\", debug=False, visualize=False):\n",
    "    \"\"\"\n",
    "    使用Camelot提取表格并保存为JSON，同时增加调试信息和可视化图像的保存\n",
    "    :param pdf_path: PDF文件路径\n",
    "    :param output_dir: 输出目录\n",
    "    :param debug: 是否打印调试信息\n",
    "    :param visualize: 是否生成并显示可视化图像\n",
    "    \"\"\"\n",
    "    \n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        tables = camelot.read_pdf(\n",
    "            pdf_path, \n",
    "            flavor=\"network\",  # 如遇到问题，可尝试改为 \"lattice\"\n",
    "            edge_tol=50,    # 提高边缘检测敏感度\n",
    "            row_tol=10      # 优化行间距识别\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"文件解析失败: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    if not tables or len(tables) == 0:\n",
    "        print(\"未检测到任何表格\")\n",
    "        return\n",
    "\n",
    "    for table_num, table in enumerate(tables, 1):\n",
    "        # 如果开启调试，打印当前表格的解析报告和其他信息\n",
    "        if debug:\n",
    "            print(f\"------------------------------\")\n",
    "            print(f\"处理第 {table_num} 个表格 (页面 {table.page + 1})\")\n",
    "            print(\"解析报告:\", table.parsing_report)\n",
    "            print(\"边界框:\", table._bbox)  # 内部边界信息\n",
    "        \n",
    "        # 获取数据（自动处理合并单元格）\n",
    "        df = table.df\n",
    "        \n",
    "        # 清洗数据（处理空值和换行符）\n",
    "        cleaned_table = [\n",
    "            [cell.strip().replace(\"\\n\", \" \") if cell else \"\" \n",
    "             for cell in row\n",
    "            ]\n",
    "            for row in df.values.tolist()\n",
    "        ]\n",
    "        \n",
    "        # 构建JSON结构\n",
    "        json_data = {\n",
    "            \"metadata\": {\n",
    "                \"source\": pdf_path,\n",
    "                \"page\": table.page + 1,  # Camelot页码从0开始\n",
    "                \"table_number\": table_num,\n",
    "                \"accuracy\": round(table.parsing_report.get(\"accuracy\", 0), 2),\n",
    "                \"dimensions\": {\n",
    "                    \"rows\": len(cleaned_table),\n",
    "                    \"columns\": len(cleaned_table[0]) if cleaned_table else 0\n",
    "                }\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"headers\": cleaned_table[0] if cleaned_table else [],\n",
    "                \"rows\": cleaned_table[1:] if len(cleaned_table) > 1 else []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 生成JSON文件名\n",
    "        json_filename = f\"page_{table.page+1}_table_{table_num}.json\"\n",
    "        json_output_path = Path(output_dir) / json_filename\n",
    "        \n",
    "        # 保存JSON文件\n",
    "        with open(json_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"已保存表格数据：{json_filename}\")\n",
    "        \n",
    "        # 如果开启可视化，生成并保存表格的图像\n",
    "        if visualize:\n",
    "            try:\n",
    "                # 使用 grid 图展示表格检测结果\n",
    "                plot = camelot.plot(table, kind='grid', figsize=(10, 10))\n",
    "                # 保存图像文件\n",
    "                img_filename = f\"debug_page_{table.page+1}_table_{table_num}.png\"\n",
    "                img_output_path = Path(output_dir) / img_filename\n",
    "                plot.savefig(img_output_path)\n",
    "                print(f\"已保存调试图像：{img_filename}\")\n",
    "                # 显示图像\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"可视化出错: {str(e)}\")\n",
    "\n",
    "# 使用示例\n",
    "extract_tables_with_camelot(\n",
    "    pdf_path=\"column_separators.pdf\",\n",
    "    output_dir=\"camelot_json_output\",\n",
    "    debug=True,      \n",
    "    visualize=True   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo LLAMA_CLOUD_API_KEY=llx-WQMylwiPgluWrsrbK8NHnNISeemscgGSjB6tiKdBJOeA4SzJ > .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: llama-index in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (0.12.18)\n",
      "Requirement already satisfied: llama-parse in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (0.6.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.4.6)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.18 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.12.18)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.6.6)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.3.20)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.4.5)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-parse) (0.6.1)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services>=0.6.1->llama-parse) (8.1.7)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.11 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services>=0.6.1->llama-parse) (0.1.12)\n",
      "Requirement already satisfied: pydantic!=2.10 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services>=0.6.1->llama-parse) (2.10.6)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.60.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.18->llama-index) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (3.10.8)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (3.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.17.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from click<9.0.0,>=8.1.7->llama-cloud-services>=0.6.1->llama-parse) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud<0.2.0,>=0.1.11->llama-cloud-services>=0.6.1->llama-parse) (2024.8.30)\n",
      "Requirement already satisfied: anyio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.18->llama-index) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.18->llama-index) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.18->llama-index) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=2.10->llama-cloud-services>=0.6.1->llama-parse) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=2.10->llama-cloud-services>=0.6.1->llama-parse) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.18->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.18->llama-index) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.18->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.18->llama-index) (3.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.18->llama-index) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (1.6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: llama-cloud-services in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (0.6.1)\n",
      "Requirement already satisfied: llama-index-core in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (0.12.18)\n",
      "Requirement already satisfied: llama-index-readers-file in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services) (8.1.7)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.11 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services) (0.1.12)\n",
      "Requirement already satisfied: pydantic!=2.10 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services) (2.10.6)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services) (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (3.10.8)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.17.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file) (4.13.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file) (5.3.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file) (0.0.26)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from click<9.0.0,>=8.1.7->llama-cloud-services) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud<0.2.0,>=0.1.11->llama-cloud-services) (2024.8.30)\n",
      "Requirement already satisfied: anyio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from nltk>3.8.1->llama-index-core) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=2.10->llama-cloud-services) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=2.10->llama-cloud-services) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.31.0->llama-index-core) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json->llama-index-core) (3.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file) (2024.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index llama-parse python-dotenv\n",
    "%pip install nest_asyncio\n",
    "%pip install llama-cloud-services llama-index-core llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: llx-WQMylwiPgluWrsrbK8NHnNISeemscgGSjB6tiKdBJOeA4SzJ\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# check if load or not\n",
    "load_dotenv()  \n",
    "api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "print(\"API Key:\", api_key)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 0cc3d78b-11ef-4b3e-94fb-03dd024201dc\n",
      "....18\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() \n",
    "\n",
    "\n",
    "parser = LlamaParse(api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"), result_type=\"markdown\")\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['2.01_EN.pdf'],\n",
    "    file_extractor=file_extractor\n",
    ").load_data() \n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: llx-WQMylwiPgluWrsrbK8NHnNISeemscgGSjB6tiKdBJOeA4SzJ\n",
      "Started parsing the file under job_id af6a0a1c-6d68-4e40-b6c4-622ca808a3e5\n",
      "Documents loaded: 18\n",
      "Processing document 1/18...\n",
      "Document 1: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 2/18...\n",
      "Document 2: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 3/18...\n",
      "Document 3: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 4/18...\n",
      "Document 4: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 5/18...\n",
      "Document 5: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 6/18...\n",
      "Document 6: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 7/18...\n",
      "Document 7: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 8/18...\n",
      "Document 8: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 9/18...\n",
      "Document 9: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 10/18...\n",
      "Document 10: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 11/18...\n",
      "Document 11: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 12/18...\n",
      "Document 12: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 13/18...\n",
      "Document 13: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 14/18...\n",
      "Document 14: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 15/18...\n",
      "Document 15: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 16/18...\n",
      "Document 16: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 17/18...\n",
      "Document 17: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 18/18...\n",
      "Document 18: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "JSON output saved at: output\\parsed_tables.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from pdf2image import convert_from_path\n",
    "from google.api_core.exceptions import InvalidArgument\n",
    "import google.generativeai as genai\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import nest_asyncio\n",
    "from pydantic import BaseModel, root_validator\n",
    "from typing import List, Optional\n",
    "\n",
    "# 应用 nest_asyncio 解决 Jupyter Notebook 环境下的异步问题\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 加载环境变量并配置 Gemini/LlamaParse API\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "print(\"API Key:\", api_key)\n",
    "\n",
    "# 定义 Pydantic 模型（结构化输出）\n",
    "class CellContent(BaseModel):\n",
    "    content: str\n",
    "\n",
    "class Table(BaseModel):\n",
    "    header_names: List[str]\n",
    "    n_cols: int\n",
    "    n_rows: int\n",
    "    row_content: List[List[CellContent]]\n",
    "\n",
    "class ParsedTable(BaseModel):\n",
    "    tables: List[Table]\n",
    "    description: str\n",
    "\n",
    "class PageResult(BaseModel):\n",
    "    page: int\n",
    "    parsed_table: Optional[ParsedTable]\n",
    "    error: Optional[str]\n",
    "    raw_output: str\n",
    "\n",
    "# 如果你需要定义外层文档结构，可以使用 RootModel（Pydantic v2 的方式），但这里直接使用 List[PageResult]即可。\n",
    "\n",
    "# 定义一个清洗函数，去除返回文本中可能的 markdown 代码块标记\n",
    "def clean_json_output(raw_text: str) -> str:\n",
    "    raw_text = raw_text.strip()\n",
    "    # 使用正则移除开头的 ```json 和结尾的 ```\n",
    "    pattern = r\"^```(?:json)?\\s*|```$\"\n",
    "    cleaned = re.sub(pattern, \"\", raw_text, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n",
    "\n",
    "# 定义用于提取表格的 prompt（要求输出纯 JSON，不含 markdown）\n",
    "prompt = \"\"\"<instructions>\n",
    "  <context>\n",
    "    You will receive an image converted from a PDF page. The image may contain table data as well as irrelevant elements such as logos, headers, and footers.\n",
    "  </context>\n",
    "  <goal>\n",
    "    Please extract only the table data from the image and ignore any non-relevant content.\n",
    "    Your tasks are:\n",
    "      1. Identify and extract all tables.\n",
    "      2. Discard any information that is not part of a table.\n",
    "  </goal>\n",
    "  <response_format>\n",
    "    Please output the parsed result in pure JSON format with the following structure:\n",
    "    {\n",
    "      \"tables\": [\n",
    "        {\n",
    "          \"header_names\": [\"Column1\", \"Column2\", \"...\"],\n",
    "          \"n_cols\": integer,\n",
    "          \"n_rows\": integer,\n",
    "          \"row_content\": [\n",
    "            [ {\"content\": \"Cell content\"}, {\"content\": \"Cell content\"}, ... ],\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"description\": \"Short description summarizing the table content.\"\n",
    "    }\n",
    "  </response_format>\n",
    "  <additional_instructions>\n",
    "    - Extract and return only table-related data, ignoring logos, headers, footers, etc.\n",
    "    - Output must be in pure JSON format without any markdown code fences or additional commentary.\n",
    "  </additional_instructions>\n",
    "</instructions>\"\"\"\n",
    "\n",
    "# 配置 LlamaParse，指定 result_type 为 \"markdown\"（如果模型不支持直接 json，则可能返回 markdown 格式，此时后处理函数会处理）\n",
    "parser = LlamaParse(api_key=api_key, result_type=\"markdown\")\n",
    "# 建立文件提取器映射：扩展名为 .pdf 由 LlamaParse 处理\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "# 使用 SimpleDirectoryReader 读取 PDF 文件（你可以在 input_files 中指定多个文件）\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['2.01_EN.pdf'],\n",
    "    file_extractor=file_extractor\n",
    ").load_data()\n",
    "print(\"Documents loaded:\", len(documents))\n",
    "\n",
    "results = []  # 用于存储每一页的解析结果\n",
    "\n",
    "# 遍历每个 Document（通常每个 Document 对应 PDF 的一页）\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Processing document {i+1}/{len(documents)}...\")\n",
    "    raw_output = doc.text  # 假设 doc.text 包含 LlamaParse 返回的内容\n",
    "    cleaned_output = clean_json_output(raw_output)\n",
    "    try:\n",
    "        # 尝试解析为 Python 对象\n",
    "        page_json = json.loads(cleaned_output)\n",
    "        # 利用 Pydantic 模型验证输出结构\n",
    "        parsed_table = ParsedTable.parse_obj(page_json)\n",
    "        results.append({\n",
    "            \"page\": i + 1,\n",
    "            \"parsed_table\": parsed_table.dict(),\n",
    "            \"raw_output\": raw_output\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Document {i+1}: Error parsing output with Pydantic: {e}\")\n",
    "        results.append({\n",
    "            \"page\": i + 1,\n",
    "            \"parsed_table\": None,\n",
    "            \"error\": str(e),\n",
    "            \"raw_output\": raw_output\n",
    "        })\n",
    "\n",
    "# 保存结果到 JSON 文件\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"parsed_tables.json\")\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "print(\"JSON output saved at:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 948f40db-e2b7-4096-9bf8-c16d5309a9d5\n",
      "Error while parsing the file '<bytes/buffer>': 'json'\n",
      "Number of documents loaded: 0\n"
     ]
    }
   ],
   "source": [
    "# 定义定制的 prompt，要求仅提取表格数据并输出为 JSON 格式\n",
    "prompt = \"\"\"<instructions>\n",
    "  <context>\n",
    "    You will receive an pdf and the pdf may contain table data as well as irrelevant elements such as logos, headers, and footers.\n",
    "  </context>\n",
    "  <goal>\n",
    "    Please extract only the table data from the pdf and ignore any non-relevant content. Your tasks are:\n",
    "    1. Identify and extract all tables.\n",
    "    2. Discard any information that is not part of a table.\n",
    "    3. Parse the table data and output the results in a JSON format.\n",
    "    4. Provide a short description of the extracted table data.\n",
    "  </goal>\n",
    "  <response_format>\n",
    "    Please output the parsed result in the following JSON structure:\n",
    "    \n",
    "    {\n",
    "      \"tables\": [\n",
    "        {\n",
    "          \"header_names\": [\"Column1\", \"Column2\", \"...\"],\n",
    "          \"n_cols\": integer,\n",
    "          \"n_rows\": integer,\n",
    "          \"row_content\": [\n",
    "            { \"content\": \"Cell content\" },\n",
    "            { \"content\": \"Cell content\" },\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"description\": \"Short description\"\n",
    "    }\n",
    "  </response_format>\n",
    "  <additional_instructions>\n",
    "    - Extract and return only table-related data, ensuring that all non-table elements (e.g., logos, headers, footers) are ignored.\n",
    "    - The output must be in pure JSON format without any additional commentary.\n",
    "    - You can use XML-like formatting in your instructions to enhance clarity.\n",
    "  </additional_instructions>\n",
    "</instructions>\"\"\"\n",
    "\n",
    "# 实例化 LlamaParse 时传入自定义 prompt，要求结果为 JSON 格式输出\n",
    "parser = LlamaParse(\n",
    "    api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
    "    result_type=\"json\",\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# 配置文件提取器：对于 .pdf 文件使用我们定制的 parser\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "# 使用 SimpleDirectoryReader 读取 PDF 文件并解析数据\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['data/2.01_EN.pdf'],\n",
    "    file_extractor=file_extractor\n",
    ").load_data()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "for doc in documents:\n",
    "    # 此处 doc.text 应该为纯 JSON 格式的字符串（如果 LLM 正常返回的话）\n",
    "    print(\"Parsed Document:\")\n",
    "    print(doc.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理第 1 页...\n",
      "处理页面 1 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 2 页...\n",
      "处理页面 2 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 3 页...\n",
      "处理页面 3 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 4 页...\n",
      "处理页面 4 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 5 页...\n",
      "处理页面 5 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 6 页...\n",
      "处理页面 6 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 7 页...\n",
      "处理页面 7 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 8 页...\n",
      "处理页面 8 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 9 页...\n",
      "处理页面 9 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 10 页...\n",
      "处理页面 10 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 11 页...\n",
      "处理页面 11 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 12 页...\n",
      "处理页面 12 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 13 页...\n",
      "处理页面 13 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 14 页...\n",
      "处理页面 14 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 15 页...\n",
      "处理页面 15 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 16 页...\n",
      "处理页面 16 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 17 页...\n",
      "处理页面 17 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "正在处理第 18 页...\n",
      "处理页面 18 时出错: 403 Permission denied: Consumer 'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/750767877696\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ\\' has been suspended.\"\n",
      "]\n",
      "表格解析完成，结果已保存到 output/parsed_tables.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from pdf2image import convert_from_path\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "# 加载环境变量并配置 Gemini API\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# 初始化 Gemini Pro Vision 模型\n",
    "model = genai.GenerativeModel('gemini-pro-vision')\n",
    "\n",
    "# 定义 Pydantic 模型\n",
    "class Metadata(BaseModel):\n",
    "    source: str\n",
    "    page: int\n",
    "    table_number: int\n",
    "    dimensions: dict\n",
    "    \n",
    "\n",
    "class TableData(BaseModel):\n",
    "    headers: List[str]\n",
    "    rows: List[List[Optional[str]]]\n",
    "\n",
    "class TableOutput(BaseModel):\n",
    "    metadata: Metadata\n",
    "    data: TableData\n",
    "\n",
    "def convert_pdf_to_images(pdf_path: str) -> List[Path]:\n",
    "    \"\"\"将PDF转换为图片\"\"\"\n",
    "    images = convert_from_path(pdf_path)\n",
    "    image_paths = []\n",
    "    \n",
    "    output_dir = Path(\"temp_images\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        image_path = output_dir / f\"page_{i+1}.png\"\n",
    "        image.save(image_path)\n",
    "        image_paths.append(image_path)\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "def get_image_data(image_path: str) -> str:\n",
    "    \"\"\"将图片转换为base64编码\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode()\n",
    "\n",
    "def extract_tables_from_image(image_path: str, page_num: int) -> dict:\n",
    "    \"\"\"使用Gemini提取图片中的表格\"\"\"\n",
    "    try:\n",
    "        image_data = get_image_data(image_path)\n",
    "        \n",
    "        # 构建结构化提示词\n",
    "        prompt = \"\"\"你是一个专业的表格识别助手。请仔细分析图片中的表格内容。\n",
    "\n",
    "任务：\n",
    "1. 识别并提取表格的所有内容\n",
    "2. 确定表格的维度（行数和列数）\n",
    "3. 分离表头和数据行\n",
    "4. 保持原始数据的完整性和准确性\n",
    "\n",
    "输出要求：\n",
    "- 使用JSON格式\n",
    "- 保持数据的层次结构\n",
    "- 确保所有单元格内容完整捕获\n",
    "- 处理空单元格时使用null\n",
    "\n",
    "示例输出格式：\n",
    "{\n",
    "  \"metadata\": {\n",
    "    \"source\": \"输入的PDF文件名\",\n",
    "    \"page\": 当前页码,\n",
    "    \"table_number\": 当前页中的表格序号,\n",
    "    \"dimensions\": {\n",
    "      \"rows\": 实际行数,\n",
    "      \"columns\": 实际列数\n",
    "    }\n",
    "  },\n",
    "  \"data\": {\n",
    "    \"headers\": [\"完整的表头1\", \"完整的表头2\", ...],\n",
    "    \"rows\": [\n",
    "      [\"第1行第1列\", \"第1行第2列\", ...],\n",
    "      [\"第2行第1列\", \"第2行第2列\", ...],\n",
    "      ...\n",
    "    ]\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "        # 配置生成参数\n",
    "        generation_config = {\n",
    "            \"temperature\": 0.1,  # 降低随机性\n",
    "            \"top_p\": 0.8,\n",
    "            \"top_k\": 40\n",
    "        }\n",
    "        \n",
    "        # 调用Gemini API\n",
    "        response = model.generate_content(\n",
    "            contents=[prompt, image_data],\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        \n",
    "        # 解析响应\n",
    "        result = json.loads(response.text)\n",
    "        \n",
    "        # 使用Pydantic验证\n",
    "        validated_output = TableOutput(**result)\n",
    "        return validated_output.dict()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理页面 {page_num} 时出错: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_pdf(pdf_path: str) -> List[dict]:\n",
    "    \"\"\"处理整个PDF文件\"\"\"\n",
    "    results = []\n",
    "    image_paths = convert_pdf_to_images(pdf_path)\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(f\"正在处理第 {i+1} 页...\")\n",
    "        result = extract_tables_from_image(str(image_path), i+1)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"2.01_EN.pdf\"\n",
    "    results = process_pdf(pdf_path)\n",
    "    \n",
    "    # 保存结果\n",
    "    output_dir = Path(\"output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(output_dir / \"parsed_tables.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"表格解析完成，结果已保存到 output/parsed_tables.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "from google.generativeai import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-genai>=1\n",
      "  Downloading google_genai-1.3.0-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from google-genai>=1) (2.38.0)\n",
      "Collecting httpx<1.0.0dev,>=0.28.1 (from google-genai>=1)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0dev,>=2.0.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from google-genai>=1) (2.10.6)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.28.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from google-genai>=1) (2.32.3)\n",
      "Collecting websockets<15.0dev,>=13.0 (from google-genai>=1)\n",
      "  Downloading websockets-14.2-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0dev,>=4.11.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from google-genai>=1) (4.12.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai>=1) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai>=1) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai>=1) (4.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1.0.0dev,>=0.28.1->google-genai>=1) (4.6.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1.0.0dev,>=0.28.1->google-genai>=1) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1.0.0dev,>=0.28.1->google-genai>=1) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1.0.0dev,>=0.28.1->google-genai>=1) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1.0.0dev,>=0.28.1->google-genai>=1) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai>=1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai>=1) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0dev,>=2.28.1->google-genai>=1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0dev,>=2.28.1->google-genai>=1) (2.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-genai>=1) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from anyio->httpx<1.0.0dev,>=0.28.1->google-genai>=1) (1.3.1)\n",
      "Downloading google_genai-1.3.0-py3-none-any.whl (137 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading websockets-14.2-cp312-cp312-win_amd64.whl (164 kB)\n",
      "Installing collected packages: websockets, httpx, google-genai\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.2\n",
      "    Uninstalling httpx-0.27.2:\n",
      "      Successfully uninstalled httpx-0.27.2\n",
      "Successfully installed google-genai-1.3.0 httpx-0.28.1 websockets-14.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\shaer\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"google-genai>=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "# Create a client\n",
    "api_key = \"AIzaSyA_ToQSQnQVA0eM2Pnjzi0Zz2c4utJk-TE\"\n",
    "client = genai.Client(api_key=api_key)\n",
    " \n",
    "# Define the model you are going to use\n",
    "model_id =  \"gemini-2.0-flash\" # or \"gemini-2.0-flash-lite-preview-02-05\"  , \"gemini-2.0-pro-exp-02-05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 下载文件函数\n",
    "def download_file(url, filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"已成功下载: {filename}\")\n",
    "    else:\n",
    "        print(f\"下载失败: {filename}\")\n",
    "\n",
    "# 下载示例PDF文件\n",
    "download_file(\n",
    "    \"https://storage.googleapis.com/generativeai-downloads/data/pdf_structured_outputs/handwriting_form.pdf\",\n",
    "    \"handwriting_form.pdf\"\n",
    ")\n",
    "download_file(\n",
    "    \"https://storage.googleapis.com/generativeai-downloads/data/pdf_structured_outputs/invoice.pdf\",\n",
    "    \"invoice.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newest version of gemini_pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q \"google-genai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) is a broad field that aims to create machines that can perform tasks that typically require human intelligence. Instead of just following pre-programmed rules, AI systems learn from data and adapt to new situations. Here's a breakdown of how it works, simplified into key concepts:\n",
      "\n",
      "**1. Core Idea: Learning from Data**\n",
      "\n",
      "*   AI's fundamental principle is to learn from data rather than being explicitly programmed for every possible scenario. The more data an AI system is exposed to, the better it becomes at identifying patterns, making predictions, and solving problems.\n",
      "\n",
      "**2. Key Components & Techniques:**\n",
      "\n",
      "*   **Machine Learning (ML):** This is the most common and widely used approach in AI. ML algorithms allow computers to learn from data without being explicitly programmed.\n",
      "    *   **Supervised Learning:** The algorithm is trained on a labeled dataset, meaning each input is paired with the correct output (e.g., images of cats labeled as \"cat\"). The algorithm learns to map inputs to outputs, allowing it to predict the output for new, unseen inputs. Examples include:\n",
      "        *   **Classification:** Categorizing data into predefined classes (e.g., spam detection, image recognition).\n",
      "        *   **Regression:** Predicting a continuous value (e.g., predicting house prices, stock market trends).\n",
      "    *   **Unsupervised Learning:** The algorithm is trained on an unlabeled dataset, where the inputs are not paired with any specific output. The algorithm learns to identify hidden patterns, structures, and relationships in the data. Examples include:\n",
      "        *   **Clustering:** Grouping similar data points together (e.g., customer segmentation, anomaly detection).\n",
      "        *   **Dimensionality Reduction:** Reducing the number of variables in a dataset while preserving important information.\n",
      "    *   **Reinforcement Learning:**  The algorithm learns through trial and error by interacting with an environment. It receives rewards or penalties for its actions and learns to maximize the reward over time (e.g., game playing, robotics).\n",
      "\n",
      "*   **Deep Learning (DL):** A subfield of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data.  DL is particularly effective for complex tasks like image recognition, natural language processing, and speech recognition.\n",
      "    *   **Neural Networks:** Inspired by the structure of the human brain, neural networks are composed of interconnected nodes (neurons) that process and transmit information. The connections between neurons have weights that are adjusted during training to improve the network's performance.\n",
      "    *   **Convolutional Neural Networks (CNNs):** Commonly used for image and video analysis, CNNs use specialized layers to extract features from images, such as edges, textures, and shapes.\n",
      "    *   **Recurrent Neural Networks (RNNs):** Designed for processing sequential data, such as text and time series. RNNs have feedback loops that allow them to remember past information, making them suitable for tasks like language modeling and machine translation.\n",
      "\n",
      "*   **Natural Language Processing (NLP):**  Focuses on enabling computers to understand, interpret, and generate human language. NLP techniques are used in chatbots, machine translation, sentiment analysis, and text summarization.\n",
      "\n",
      "*   **Computer Vision:** Allows computers to \"see\" and interpret images and videos. Computer vision techniques are used in facial recognition, object detection, image segmentation, and autonomous driving.\n",
      "\n",
      "*   **Robotics:** Combines AI with physical hardware to create robots that can perform tasks autonomously. Robots can be used in manufacturing, healthcare, exploration, and many other fields.\n",
      "\n",
      "*   **Expert Systems:** Designed to mimic the decision-making abilities of a human expert in a specific domain. Expert systems use knowledge bases and inference engines to provide advice and solutions.\n",
      "\n",
      "**3. The Training Process:**\n",
      "\n",
      "*   **Data Collection:** Gathering a large and relevant dataset is crucial for training an AI model. The quality and quantity of the data directly impact the model's performance.\n",
      "*   **Data Preprocessing:** The data is cleaned, transformed, and prepared for training. This may involve handling missing values, removing noise, and converting data into a suitable format.\n",
      "*   **Model Selection:** Choosing the appropriate AI algorithm or model architecture based on the task and the characteristics of the data.\n",
      "*   **Training:** Feeding the data to the chosen model and adjusting its parameters (e.g., weights in a neural network) to minimize errors and improve accuracy. This process is often iterative, involving multiple passes through the data.\n",
      "*   **Validation and Testing:** Evaluating the model's performance on a separate dataset that it has not seen during training. This helps to assess how well the model generalizes to new data and prevent overfitting (where the model learns the training data too well and performs poorly on unseen data).\n",
      "*   **Deployment:**  Integrating the trained model into a real-world application or system.\n",
      "*   **Monitoring and Maintenance:** Continuously monitoring the model's performance and retraining it with new data to maintain accuracy and adapt to changing conditions.\n",
      "\n",
      "**4. Different Levels of AI:**\n",
      "\n",
      "*   **Narrow or Weak AI:** Designed to perform a specific task, like playing chess or recognizing faces.  Most current AI systems fall into this category.\n",
      "*   **General or Strong AI:** Possesses human-level intelligence and can perform any intellectual task that a human being can.  This is a more theoretical concept and doesn't currently exist.\n",
      "*   **Super AI:** Surpasses human intelligence in all aspects, including creativity, problem-solving, and general wisdom. This is a highly speculative concept.\n",
      "\n",
      "**In summary:**\n",
      "\n",
      "AI works by learning from data using algorithms. These algorithms can be categorized into machine learning (supervised, unsupervised, and reinforcement learning), with deep learning as a powerful subfield. Data is collected, preprocessed, used to train a model, and then validated and tested. The trained model is then deployed and continuously monitored and maintained.  The current state of AI is primarily focused on narrow AI, excelling at specific tasks through specialized algorithms and vast amounts of data.\n",
      "\n",
      "It's a complex and rapidly evolving field, but hopefully, this explanation provides a good foundation for understanding the core principles of how AI works.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "client = genai.Client(\n",
    "    project=\"gen-lang-client-0117133250\",\n",
    "    location=\"us-central1\",\n",
    "    vertexai=True,\n",
    "    http_options=HttpOptions(api_version=\"v1\"))\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=\"How does AI work?\",\n",
    ")\n",
    "print(response.text)\n",
    "# Example response:\n",
    "# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n",
    "#\n",
    "# Here's a simplified overview:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"google-genai>=1\"\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file is a leaflet about self-employed contributions to Old-Age and Survivors' Insurance (OASI), Disability Insurance (DI) and Income Compensation Insurance (IC) in Switzerland. It covers topics such as who is considered self-employed, the obligation to pay contributions, contribution amounts, how contributions are calculated, contributions on account, final contributions, payment of contributions, default interest, credit interest, contributions by OASI pensioners, and contributions on IC compensation and on daily allowances paid by DI, UI and military insurance. The leaflet is valid as of 1st January 2025.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client(api_key='AIzaSyC4kSTD0mAJM0zUzrUWXKOwuUj2G4qGrkQ')\n",
    "\n",
    "# client = genai.Client(\n",
    "#     vertexai=True, \n",
    "#     project='your-project-id', \n",
    "#     location='us-central1',\n",
    "#     http_options=types.HttpOptions(api_version='v1')\n",
    "# )\n",
    "\n",
    "\n",
    "# 上传本地文件\n",
    "file = client.files.upload(file='2.02_EN.pdf', config={'display_name': '2.02_EN.pdf'})\n",
    "import requests\n",
    "\n",
    "# def download_file(url, filename):\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code == 200:\n",
    "#         with open(filename, 'wb') as f:\n",
    "#             f.write(response.content)\n",
    "#         print(f\"已成功下载: {filename}\")\n",
    "#     else:\n",
    "#         print(f\"下载失败: {filename}\")\n",
    "\n",
    "# # 下载文件\n",
    "# download_file(\n",
    "#     \"https://storage.cloud.google.com/table_processing/2.02_EN.pdf?authuser=1\",\n",
    "#     \"2.02_EN.pdf\"\n",
    "# )\n",
    "\n",
    "file_pdf = client.files.upload(file='2.02_EN.pdf', config={'display_name': '2.02_EN'})\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents=['Could you summarize this file?', file]\n",
    ")\n",
    "print(response.text)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 2.02_EN equals to 3097 tokens\n"
     ]
    }
   ],
   "source": [
    "file_size = client.models.count_tokens(model=model_id,contents=file_pdf)\n",
    "print(f'File: {file_pdf.display_name} equals to {file_size.total_tokens} tokens')\n",
    "# File: invoice equals to 821 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Union, Optional\n",
    "import json\n",
    "# 定义表格单元格的数据类型\n",
    "class CellValue(BaseModel):\n",
    "    value: Union[str, int, float] = Field(description=\"单元格的值，可以是字符串、整数或浮点数\")\n",
    "    data_type: str = Field(description=\"数据类型：text, number, date 等\")\n",
    "\n",
    "# 定义表格元数据\n",
    "class TableMetadata(BaseModel):\n",
    "    table_id: str = Field(description=\"表格的唯一标识符\")\n",
    "    pdf_name: str = Field(description=\"表格所在的PDF文件名\")\n",
    "    summary: Optional[str] = Field(default=None, description=\"表格概述\")\n",
    "    title: Optional[str] = Field(default=None, description=\"表格标题\")\n",
    "    language: str = Field(description=\"表格内容的语言，如：de, fr, it, en\")\n",
    "    source_url: Optional[str] = Field(default=None, description=\"表格来源的URL\")\n",
    "    page_number: Optional[int] = Field(default=None, description=\"PDF页码\")\n",
    "    total_rows: int = Field(description=\"总行数\")\n",
    "    total_columns: int = Field(description=\"总列数\")\n",
    "\n",
    "# 定义表格数据\n",
    "class TableData(BaseModel):\n",
    "    headers: List[str] = Field(description=\"表格列标题\")\n",
    "    rows: List[List[CellValue]] = Field(description=\"表格数据行\")\n",
    "\n",
    "# 定义完整表格\n",
    "class Table(BaseModel):\n",
    "    metadata: TableMetadata = Field(description=\"表格元数据\")\n",
    "    data: TableData = Field(description=\"表格内容数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始响应已保存到 raw_response.json\n",
      "解析后的表格数据已保存到 parsed_table.json\n",
      "表格HTML预览已保存到 table_preview.html\n",
      "\n",
      "解析完成！\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Union, Optional, Any\n",
    "import json\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# 定义表格单元格的数据类型 - 修改value类型为Any以容纳可能的None值\n",
    "class CellValue(BaseModel):\n",
    "    value: Optional[Any] = Field(description=\"单元格的值，可以是字符串、整数、浮点数或null\")\n",
    "    data_type: str = Field(description=\"数据类型：text, number, date 等\")\n",
    "\n",
    "# 定义表格元数据\n",
    "class TableMetadata(BaseModel):\n",
    "    table_id: str = Field(description=\"表格的唯一标识符\")\n",
    "    pdf_name: str = Field(description=\"表格所在的PDF文件名\")\n",
    "    summary: Optional[str] = Field(default=None, description=\"表格概述\")\n",
    "    title: Optional[str] = Field(default=None, description=\"表格标题\")\n",
    "    language: str = Field(description=\"表格内容的语言，如：de, fr, it, en\")\n",
    "    source_url: Optional[str] = Field(default=None, description=\"表格来源的URL\")\n",
    "    page_number: Optional[int] = Field(default=None, description=\"PDF页码\")\n",
    "    total_rows: int = Field(description=\"总行数\")\n",
    "    total_columns: int = Field(description=\"总列数\")\n",
    "\n",
    "# 定义表格数据\n",
    "class TableData(BaseModel):\n",
    "    headers: List[Optional[str]] = Field(description=\"表格列标题\")\n",
    "    rows: List[List[CellValue]] = Field(description=\"表格数据行\")\n",
    "\n",
    "# 定义完整表格\n",
    "class Table(BaseModel):\n",
    "    metadata: TableMetadata = Field(description=\"表格元数据\")\n",
    "    data: TableData = Field(description=\"表格内容数据\")\n",
    "\n",
    "def parse_pdf_to_table(pdf_bytes: bytes, pdf_name: str) -> Table:\n",
    "    # 初始化客户端\n",
    "    api_key = \"AIzaSyA_ToQSQnQVA0eM2Pnjzi0Zz2c4utJk-TE\"\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # 获取Pydantic模型的JSON schema\n",
    "    table_schema = Table.model_json_schema()\n",
    "    \n",
    "    # 构造提示词：嵌入JSON Schema\n",
    "    instruction = f\"\"\"\n",
    "    请解析下方PDF中的表格，并严格按以下JSON格式返回数据：\n",
    "    {json.dumps(table_schema, indent=2)}\n",
    "    \n",
    "    附加要求：\n",
    "    1. metadata.pdf_name 必须为 {pdf_name}\n",
    "    2. 若表格缺失或无法解析，返回错误字段 \"error\"\n",
    "    3. 请注意所有单元格值可以为null，不要留空值\n",
    "    \"\"\"\n",
    "    \n",
    "    # 创建文本部分\n",
    "    text_part = types.Part(text=instruction)\n",
    "    \n",
    "    # 创建PDF部分\n",
    "    pdf_part = types.Part(\n",
    "        inline_data=types.Blob(\n",
    "            mime_type=\"application/pdf\",\n",
    "            data=pdf_bytes\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 创建内容\n",
    "    contents = [\n",
    "        text_part,\n",
    "        pdf_part\n",
    "    ]\n",
    "    \n",
    "    # 使用正确的API调用方式\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=contents,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 解析响应\n",
    "    try:\n",
    "        # 获取响应文本\n",
    "        response_text = response.text\n",
    "        \n",
    "        # 输出原始响应到文件\n",
    "        with open(\"raw_response.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response_text)\n",
    "        print(f\"原始响应已保存到 raw_response.json\")\n",
    "        \n",
    "        # 验证返回的是有效JSON\n",
    "        json_data = json.loads(response_text)\n",
    "        \n",
    "        # 检查返回的是列表还是字典\n",
    "        if isinstance(json_data, list) and len(json_data) > 0:\n",
    "            # 如果返回的是列表，取第一个元素\n",
    "            json_data = json_data[0]\n",
    "        \n",
    "        # 清理数据，确保没有验证错误\n",
    "        if \"data\" in json_data and \"rows\" in json_data[\"data\"]:\n",
    "            for row_idx, row in enumerate(json_data[\"data\"][\"rows\"]):\n",
    "                for cell_idx, cell in enumerate(row):\n",
    "                    # 确保每个单元格都有value和data_type\n",
    "                    if \"value\" not in cell:\n",
    "                        cell[\"value\"] = None\n",
    "                    if \"data_type\" not in cell:\n",
    "                        cell[\"data_type\"] = \"text\"\n",
    "        \n",
    "        # 使用Pydantic解析\n",
    "        table = Table.model_validate(json_data)\n",
    "        \n",
    "        # 将解析后的表格保存为漂亮的JSON\n",
    "        with open(\"parsed_table.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(table.model_dump_json(indent=2, exclude_none=True))\n",
    "        print(f\"解析后的表格数据已保存到 parsed_table.json\")\n",
    "        \n",
    "        # 将表格数据导出为HTML以便查看\n",
    "        html_content = table_to_html(table)\n",
    "        with open(\"table_preview.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_content)\n",
    "        print(f\"表格HTML预览已保存到 table_preview.html\")\n",
    "        \n",
    "        return table\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"解析失败: {str(e)}\")\n",
    "\n",
    "def table_to_html(table: Table) -> str:\n",
    "    \"\"\"将表格转换为HTML格式\"\"\"\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>{table.metadata.title or 'PDF表格解析结果'}</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            h1 {{ color: #333; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}\n",
    "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "            th {{ background-color: #f2f2f2; }}\n",
    "            .metadata {{ background-color: #f9f9f9; padding: 10px; border-radius: 5px; margin-bottom: 20px; }}\n",
    "            .metadata p {{ margin: 5px 0; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>{table.metadata.title or 'PDF表格解析结果'}</h1>\n",
    "        \n",
    "        <div class=\"metadata\">\n",
    "            <h3>元数据</h3>\n",
    "            <p><strong>表格ID:</strong> {table.metadata.table_id}</p>\n",
    "            <p><strong>PDF文件:</strong> {table.metadata.pdf_name}</p>\n",
    "            <p><strong>语言:</strong> {table.metadata.language}</p>\n",
    "            <p><strong>总行数:</strong> {table.metadata.total_rows}</p>\n",
    "            <p><strong>总列数:</strong> {table.metadata.total_columns}</p>\n",
    "            {f'<p><strong>概述:</strong> {table.metadata.summary}</p>' if table.metadata.summary else ''}\n",
    "            {f'<p><strong>页码:</strong> {table.metadata.page_number}</p>' if table.metadata.page_number is not None else ''}\n",
    "        </div>\n",
    "        \n",
    "        <h3>表格数据</h3>\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # 添加表头\n",
    "    for header in table.data.headers:\n",
    "        html += f\"<th>{header or ''}</th>\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "    \"\"\"\n",
    "    \n",
    "    # 添加表格内容\n",
    "    for row in table.data.rows:\n",
    "        html += \"<tr>\"\n",
    "        for cell in row:\n",
    "            # 处理不同类型的单元格值\n",
    "            cell_value = cell.value if cell.value is not None else \"\"\n",
    "            html += f\"<td>{cell_value}</td>\"\n",
    "        html += \"</tr>\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "# -------------------------- \n",
    "# 使用示例\n",
    "# -------------------------- \n",
    "if __name__ == \"__main__\":\n",
    "    # 读取PDF文件\n",
    "    with open(\"2.02_EN.pdf\", \"rb\") as f:\n",
    "        pdf_bytes = f.read()\n",
    "    \n",
    "    # 调用函数\n",
    "    try:\n",
    "        table = parse_pdf_to_table(pdf_bytes, \"2.02_EN.pdf\")\n",
    "        print(\"\\n解析完成！\")\n",
    "    except ValueError as e:\n",
    "        print(f\"错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始响应已保存到 raw_response.json\n",
      "解析后的所有表格数据已保存到 parsed_tables.json\n",
      "表格HTML预览已保存到 table_previews/ 目录\n",
      "\n",
      "解析完成！共提取 6 个表格。\n",
      "请打开 table_previews/index.html 查看所有表格。\n"
     ]
    }
   ],
   "source": [
    "# from google import genai\n",
    "# from google.genai import types\n",
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List, Union, Optional, Any, Dict\n",
    "# import json\n",
    "# import base64\n",
    "# import os\n",
    "# import time\n",
    "\n",
    "# # 定义表格单元格的数据类型 - 修改value类型为Any以容纳可能的None值\n",
    "# class CellValue(BaseModel):\n",
    "#     value: Optional[Any] = Field(description=\"单元格的值，可以是字符串、整数、浮点数或null\")\n",
    "#     data_type: str = Field(description=\"数据类型：text, number, date 等\")\n",
    "\n",
    "# # 定义表格元数据\n",
    "# class TableMetadata(BaseModel):\n",
    "#     table_id: str = Field(description=\"表格的唯一标识符\")\n",
    "#     pdf_name: str = Field(description=\"表格所在的PDF文件名\")\n",
    "#     summary: Optional[str] = Field(default=None, description=\"表格概述\")\n",
    "#     title: Optional[str] = Field(default=None, description=\"表格标题\")\n",
    "#     language: str = Field(description=\"表格内容的语言，如：de, fr, it, en\")\n",
    "#     source_url: Optional[str] = Field(default=None, description=\"表格来源的URL\")\n",
    "#     page_number: Optional[int] = Field(default=None, description=\"PDF页码\")\n",
    "#     total_rows: int = Field(description=\"总行数\")\n",
    "#     total_columns: int = Field(description=\"总列数\")\n",
    "\n",
    "# # 定义表格数据\n",
    "# class TableData(BaseModel):\n",
    "#     headers: List[Optional[str]] = Field(description=\"表格列标题\")\n",
    "#     rows: List[List[CellValue]] = Field(description=\"表格数据行\")\n",
    "\n",
    "# # 定义完整表格\n",
    "# class Table(BaseModel):\n",
    "#     metadata: TableMetadata = Field(description=\"表格元数据\")\n",
    "#     data: TableData = Field(description=\"表格内容数据\")\n",
    "\n",
    "# # 定义多个表格的列表，用于整个PDF的处理结果\n",
    "# class PdfTables(BaseModel):\n",
    "#     tables: List[Table] = Field(description=\"PDF中所有表格的列表\")\n",
    "#     total_tables: int = Field(description=\"表格总数\")\n",
    "\n",
    "# def generate_table_title(client, table: Table) -> str:\n",
    "#     \"\"\"使用Gemini API生成表格的描述性标题\"\"\"\n",
    "#     try:\n",
    "#         # 准备表格数据的文本表示\n",
    "#         headers_text = \", \".join([h for h in table.data.headers if h])\n",
    "        \n",
    "#         # 提取前几行数据作为示例\n",
    "#         sample_rows = []\n",
    "#         for row_idx, row in enumerate(table.data.rows):\n",
    "#             if row_idx >= 3:  # 只取前3行\n",
    "#                 break\n",
    "#             row_text = \", \".join([str(cell.value) for cell in row if cell.value is not None])\n",
    "#             sample_rows.append(row_text)\n",
    "        \n",
    "#         rows_text = \"\\n\".join(sample_rows)\n",
    "        \n",
    "#         # 构造提示词\n",
    "#         prompt = f\"\"\"\n",
    "#         请为以下表格生成一个简短、准确、具体的标题，描述表格的主题内容。\n",
    "#         不要使用\"表格\"、\"数据\"等通用词作为标题的开头。\n",
    "        \n",
    "#         表格头部: {headers_text}\n",
    "#         表格数据样例:\n",
    "#         {rows_text}\n",
    "#         页码: {table.metadata.page_number or '未知'}\n",
    "        \n",
    "#         表格标题:\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # 调用API生成标题\n",
    "#         response = client.models.generate_content(\n",
    "#             model=\"gemini-2.0-flash\",\n",
    "#             contents=prompt,\n",
    "#             config=types.GenerateContentConfig(\n",
    "#                 max_output_tokens=50,  # 限制输出长度\n",
    "#                 temperature=0.2  # 降低随机性\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#         # 提取并清理标题\n",
    "#         title = response.text.strip()\n",
    "#         # 如果标题有引号，去掉引号\n",
    "#         if title.startswith('\"') and title.endswith('\"'):\n",
    "#             title = title[1:-1]\n",
    "#         if title.startswith(\"'\") and title.endswith(\"'\"):\n",
    "#             title = title[1:-1]\n",
    "            \n",
    "#         return title if title else \"未命名表格\"\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"生成表格标题时出错: {str(e)}\")\n",
    "#         return \"未命名表格\"\n",
    "\n",
    "# def parse_pdf_tables(pdf_bytes: bytes, pdf_name: str) -> PdfTables:\n",
    "#     \"\"\"解析PDF中的所有表格\"\"\"\n",
    "#     # 初始化客户端\n",
    "#     api_key = \"AIzaSyA_ToQSQnQVA0eM2Pnjzi0Zz2c4utJk-TE\"\n",
    "#     client = genai.Client(api_key=api_key)\n",
    "    \n",
    "#     # 获取Pydantic模型的JSON schema\n",
    "#     table_schema = Table.model_json_schema()\n",
    "    \n",
    "#     # 构造提示词：嵌入JSON Schema，强调提取所有表格\n",
    "#     instruction = f\"\"\"\n",
    "#     请依次解析PDF中的所有表格，按照以下JSON格式返回每个表格的数据。\n",
    "#     这个PDF包含多页和多个表格，请完整提取每一个表格，不要遗漏。\n",
    "    \n",
    "#     表格结构定义：\n",
    "#     {json.dumps(table_schema, indent=2)}\n",
    "    \n",
    "#     请返回一个JSON数组，每个元素代表一个表格，格式必须符合上述模式。\n",
    "    \n",
    "#     附加要求：\n",
    "#     1. 每个表格的metadata.pdf_name必须为 {pdf_name}\n",
    "#     2. 每个表格的metadata.table_id应该是从\"table_1\"开始递增编号\n",
    "#     3. 每个表格的metadata.page_number必须指明表格出现的PDF页码\n",
    "#     4. 请为每个表格创建一个metadata.title，描述表格的主题\n",
    "#     5. 请注意所有单元格值可以为null，不要留空值\n",
    "#     6. 请提取所有页面中的所有表格，不要遗漏任何一个\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 创建文本部分\n",
    "#     text_part = types.Part(text=instruction)\n",
    "    \n",
    "#     # 创建PDF部分\n",
    "#     pdf_part = types.Part(\n",
    "#         inline_data=types.Blob(\n",
    "#             mime_type=\"application/pdf\",\n",
    "#             data=pdf_bytes\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # 创建内容\n",
    "#     contents = [\n",
    "#         text_part,\n",
    "#         pdf_part\n",
    "#     ]\n",
    "    \n",
    "#     # 使用更长的最大输出token数，以确保能够返回所有表格\n",
    "#     response = client.models.generate_content(\n",
    "#         model=\"gemini-2.0-flash\",\n",
    "#         contents=contents,\n",
    "#         config=types.GenerateContentConfig(\n",
    "#             response_mime_type=\"application/json\",\n",
    "#             max_output_tokens=8192  # 增加输出长度限制\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # 解析响应\n",
    "#     try:\n",
    "#         # 获取响应文本\n",
    "#         response_text = response.text\n",
    "        \n",
    "#         # 输出原始响应到文件\n",
    "#         with open(\"raw_response.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(response_text)\n",
    "#         print(f\"原始响应已保存到 raw_response.json\")\n",
    "        \n",
    "#         # 验证返回的是有效JSON\n",
    "#         json_data = json.loads(response_text)\n",
    "        \n",
    "#         # 确保json_data是列表\n",
    "#         if not isinstance(json_data, list):\n",
    "#             json_data = [json_data]\n",
    "        \n",
    "#         # 初始化表格列表\n",
    "#         tables = []\n",
    "        \n",
    "#         # 处理每个表格\n",
    "#         for table_idx, table_json in enumerate(json_data):\n",
    "#             try:\n",
    "#                 # 清理数据，确保没有验证错误\n",
    "#                 if \"data\" in table_json and \"rows\" in table_json[\"data\"]:\n",
    "#                     for row_idx, row in enumerate(table_json[\"data\"][\"rows\"]):\n",
    "#                         for cell_idx, cell in enumerate(row):\n",
    "#                             # 确保每个单元格都有value和data_type\n",
    "#                             if \"value\" not in cell:\n",
    "#                                 cell[\"value\"] = None\n",
    "#                             if \"data_type\" not in cell:\n",
    "#                                 cell[\"data_type\"] = \"text\"\n",
    "                \n",
    "#                 # 使用Pydantic解析单个表格\n",
    "#                 table = Table.model_validate(table_json)\n",
    "                \n",
    "#                 # 如果表格没有标题，生成一个\n",
    "#                 if not table.metadata.title:\n",
    "#                     title = generate_table_title(client, table)\n",
    "#                     table.metadata.title = title\n",
    "#                     print(f\"为表格 {table_idx+1} 生成标题: {title}\")\n",
    "                \n",
    "#                 tables.append(table)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"警告: 表格 {table_idx+1} 解析失败: {str(e)}\")\n",
    "        \n",
    "#         # 创建完整的PdfTables对象\n",
    "#         pdf_tables = PdfTables(\n",
    "#             tables=tables,\n",
    "#             total_tables=len(tables)\n",
    "#         )\n",
    "        \n",
    "#         # 将解析后的所有表格保存为JSON\n",
    "#         with open(\"parsed_tables.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(pdf_tables.model_dump_json(indent=2, exclude_none=True))\n",
    "#         print(f\"解析后的所有表格数据已保存到 parsed_tables.json\")\n",
    "        \n",
    "#         # 将每个表格生成单独的HTML文件\n",
    "#         os.makedirs(\"table_previews\", exist_ok=True)\n",
    "#         for idx, table in enumerate(tables):\n",
    "#             html_content = table_to_html(table, idx+1)\n",
    "#             filename = f\"table_previews/table_{idx+1}.html\"\n",
    "#             with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "#                 f.write(html_content)\n",
    "        \n",
    "#         # 创建一个索引HTML文件，列出所有表格\n",
    "#         create_index_html(tables)\n",
    "#         print(f\"表格HTML预览已保存到 table_previews/ 目录\")\n",
    "        \n",
    "#         return pdf_tables\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         # 如果整体解析失败，尝试分批处理\n",
    "#         print(f\"整体解析失败，尝试按页处理: {str(e)}\")\n",
    "#         return parse_pdf_tables_by_pages(client, pdf_bytes, pdf_name)\n",
    "\n",
    "# def parse_pdf_tables_by_pages(client, pdf_bytes: bytes, pdf_name: str) -> PdfTables:\n",
    "#     \"\"\"按页解析PDF中的表格，用于处理大型PDF\"\"\"\n",
    "#     tables = []\n",
    "#     table_id_counter = 1\n",
    "    \n",
    "#     # 尝试最多处理30页\n",
    "#     for page in range(1, 31):\n",
    "#         try:\n",
    "#             # 构造提示词\n",
    "#             page_instruction = f\"\"\"\n",
    "#             请解析PDF第{page}页中的所有表格，按照JSON格式返回。\n",
    "#             如果该页没有表格，请返回空数组[]。\n",
    "            \n",
    "#             附加要求：\n",
    "#             1. 每个表格的metadata.pdf_name必须为 {pdf_name}\n",
    "#             2. 每个表格的metadata.table_id应该是\"table_{table_id_counter}\"\n",
    "#             3. 每个表格的metadata.page_number必须为 {page}\n",
    "#             4. 请为每个表格创建一个metadata.title，描述表格的主题\n",
    "#             5. 请注意所有单元格值可以为null，不要留空值\n",
    "#             \"\"\"\n",
    "            \n",
    "#             # 创建内容\n",
    "#             contents = [\n",
    "#                 types.Part(text=page_instruction),\n",
    "#                 types.Part(\n",
    "#                     inline_data=types.Blob(\n",
    "#                         mime_type=\"application/pdf\",\n",
    "#                         data=pdf_bytes\n",
    "#                     )\n",
    "#                 )\n",
    "#             ]\n",
    "            \n",
    "#             # 使用API\n",
    "#             page_response = client.models.generate_content(\n",
    "#                 model=\"gemini-2.0-flash\",\n",
    "#                 contents=contents,\n",
    "#                 config=types.GenerateContentConfig(\n",
    "#                     response_mime_type=\"application/json\"\n",
    "#                 )\n",
    "#             )\n",
    "            \n",
    "#             # 解析响应\n",
    "#             page_text = page_response.text\n",
    "#             page_data = json.loads(page_text)\n",
    "            \n",
    "#             # 确保page_data是列表\n",
    "#             if not isinstance(page_data, list):\n",
    "#                 if not page_data:  # 空响应\n",
    "#                     continue\n",
    "#                 page_data = [page_data]\n",
    "            \n",
    "#             if not page_data:  # 空列表，无表格\n",
    "#                 print(f\"第{page}页没有检测到表格\")\n",
    "#                 continue\n",
    "            \n",
    "#             # 处理页面上的每个表格\n",
    "#             for table_json in page_data:\n",
    "#                 try:\n",
    "#                     # 清理数据并修正表格ID\n",
    "#                     if \"metadata\" in table_json:\n",
    "#                         table_json[\"metadata\"][\"table_id\"] = f\"table_{table_id_counter}\"\n",
    "#                         table_json[\"metadata\"][\"page_number\"] = page\n",
    "                    \n",
    "#                     if \"data\" in table_json and \"rows\" in table_json[\"data\"]:\n",
    "#                         for row in table_json[\"data\"][\"rows\"]:\n",
    "#                             for cell in row:\n",
    "#                                 if \"value\" not in cell:\n",
    "#                                     cell[\"value\"] = None\n",
    "#                                 if \"data_type\" not in cell:\n",
    "#                                     cell[\"data_type\"] = \"text\"\n",
    "                    \n",
    "#                     # 使用Pydantic解析\n",
    "#                     table = Table.model_validate(table_json)\n",
    "                    \n",
    "#                     # 如果表格没有标题，生成一个\n",
    "#                     if not table.metadata.title:\n",
    "#                         title = generate_table_title(client, table)\n",
    "#                         table.metadata.title = title\n",
    "#                         print(f\"为表格 {table_id_counter} 生成标题: {title}\")\n",
    "                    \n",
    "#                     tables.append(table)\n",
    "#                     table_id_counter += 1\n",
    "#                 except Exception as table_err:\n",
    "#                     print(f\"警告: 第{page}页表格解析失败: {str(table_err)}\")\n",
    "            \n",
    "#             print(f\"已处理第{page}页，找到 {len(page_data)} 个表格\")\n",
    "            \n",
    "#         except Exception as page_err:\n",
    "#             print(f\"警告: 处理第{page}页时出错: {str(page_err)}\")\n",
    "#             # 如果连续3页没有表格，假设已到达PDF末尾\n",
    "#             if page > 3 and len(tables) == 0:\n",
    "#                 break\n",
    "        \n",
    "#         # 防止API限流\n",
    "#         time.sleep(1)\n",
    "    \n",
    "#     # 创建完整的PdfTables对象\n",
    "#     pdf_tables = PdfTables(\n",
    "#         tables=tables,\n",
    "#         total_tables=len(tables)\n",
    "#     )\n",
    "    \n",
    "#     # 将解析后的所有表格保存为JSON\n",
    "#     with open(\"parsed_tables.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(pdf_tables.model_dump_json(indent=2, exclude_none=True))\n",
    "#     print(f\"解析后的所有表格数据已保存到 parsed_tables.json\")\n",
    "    \n",
    "#     # 将每个表格生成单独的HTML文件\n",
    "#     os.makedirs(\"table_previews\", exist_ok=True)\n",
    "#     for idx, table in enumerate(tables):\n",
    "#         html_content = table_to_html(table, idx+1)\n",
    "#         filename = f\"table_previews/table_{idx+1}.html\"\n",
    "#         with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(html_content)\n",
    "    \n",
    "#     # 创建一个索引HTML文件，列出所有表格\n",
    "#     create_index_html(tables)\n",
    "#     print(f\"表格HTML预览已保存到 table_previews/ 目录\")\n",
    "    \n",
    "#     return pdf_tables\n",
    "\n",
    "# def table_to_html(table: Table, index: int = 0) -> str:\n",
    "#     \"\"\"将表格转换为HTML格式\"\"\"\n",
    "#     html = f\"\"\"\n",
    "#     <!DOCTYPE html>\n",
    "#     <html>\n",
    "#     <head>\n",
    "#         <title>表格 {index}: {table.metadata.title or '未命名表格'}</title>\n",
    "#         <style>\n",
    "#             body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "#             h1 {{ color: #333; }}\n",
    "#             table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}\n",
    "#             th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "#             th {{ background-color: #f2f2f2; }}\n",
    "#             .metadata {{ background-color: #f9f9f9; padding: 10px; border-radius: 5px; margin-bottom: 20px; }}\n",
    "#             .metadata p {{ margin: 5px 0; }}\n",
    "#             .navigation {{ margin-top: 20px; }}\n",
    "#             .navigation a {{ margin-right: 10px; padding: 5px 10px; background-color: #f0f0f0; text-decoration: none; color: #333; border-radius: 3px; }}\n",
    "#         </style>\n",
    "#     </head>\n",
    "#     <body>\n",
    "#         <div class=\"navigation\">\n",
    "#             <a href=\"index.html\">返回表格列表</a>\n",
    "#         </div>\n",
    "        \n",
    "#         <h1>表格 {index}: {table.metadata.title or '未命名表格'}</h1>\n",
    "        \n",
    "#         <div class=\"metadata\">\n",
    "#             <h3>元数据</h3>\n",
    "#             <p><strong>表格ID:</strong> {table.metadata.table_id}</p>\n",
    "#             <p><strong>PDF文件:</strong> {table.metadata.pdf_name}</p>\n",
    "#             <p><strong>语言:</strong> {table.metadata.language}</p>\n",
    "#             <p><strong>页码:</strong> {table.metadata.page_number or '未知'}</p>\n",
    "#             <p><strong>总行数:</strong> {table.metadata.total_rows}</p>\n",
    "#             <p><strong>总列数:</strong> {table.metadata.total_columns}</p>\n",
    "#             {f'<p><strong>概述:</strong> {table.metadata.summary}</p>' if table.metadata.summary else ''}\n",
    "#         </div>\n",
    "        \n",
    "#         <h3>表格数据</h3>\n",
    "#         <table>\n",
    "#             <thead>\n",
    "#                 <tr>\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 添加表头\n",
    "#     for header in table.data.headers:\n",
    "#         html += f\"<th>{header or ''}</th>\"\n",
    "    \n",
    "#     html += \"\"\"\n",
    "#                 </tr>\n",
    "#             </thead>\n",
    "#             <tbody>\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 添加表格内容\n",
    "#     for row in table.data.rows:\n",
    "#         html += \"<tr>\"\n",
    "#         for cell in row:\n",
    "#             # 处理不同类型的单元格值\n",
    "#             cell_value = cell.value if cell.value is not None else \"\"\n",
    "#             html += f\"<td>{cell_value}</td>\"\n",
    "#         html += \"</tr>\"\n",
    "    \n",
    "#     html += \"\"\"\n",
    "#             </tbody>\n",
    "#         </table>\n",
    "#     </body>\n",
    "#     </html>\n",
    "#     \"\"\"\n",
    "    \n",
    "#     return html\n",
    "\n",
    "# def create_index_html(tables: List[Table]) -> None:\n",
    "#     \"\"\"创建一个HTML索引页面，列出所有表格\"\"\"\n",
    "#     html = \"\"\"\n",
    "#     <!DOCTYPE html>\n",
    "#     <html>\n",
    "#     <head>\n",
    "#         <title>PDF表格提取结果</title>\n",
    "#         <style>\n",
    "#             body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "#             h1 { color: #333; }\n",
    "#             .table-list { display: flex; flex-wrap: wrap; }\n",
    "#             .table-card { \n",
    "#                 border: 1px solid #ddd;\n",
    "#                 border-radius: 5px;\n",
    "#                 padding: 15px;\n",
    "#                 margin: 10px;\n",
    "#                 width: 280px;\n",
    "#                 background-color: #f9f9f9;\n",
    "#             }\n",
    "#             .table-card h3 { margin-top: 0; }\n",
    "#             .table-card p { margin: 5px 0; }\n",
    "#             .table-card a { \n",
    "#                 display: block;\n",
    "#                 text-align: center;\n",
    "#                 margin-top: 10px;\n",
    "#                 padding: 8px;\n",
    "#                 background-color: #4CAF50;\n",
    "#                 color: white;\n",
    "#                 text-decoration: none;\n",
    "#                 border-radius: 3px;\n",
    "#             }\n",
    "#             .summary { margin-bottom: 20px; }\n",
    "#         </style>\n",
    "#     </head>\n",
    "#     <body>\n",
    "#         <h1>PDF表格提取结果</h1>\n",
    "        \n",
    "#         <div class=\"summary\">\n",
    "#             <p><strong>PDF文件:</strong> \"\"\" + (tables[0].metadata.pdf_name if tables else \"未知\") + \"\"\"</p>\n",
    "#             <p><strong>提取表格总数:</strong> \"\"\" + str(len(tables)) + \"\"\"</p>\n",
    "#         </div>\n",
    "        \n",
    "#         <h2>表格列表</h2>\n",
    "#         <div class=\"table-list\">\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 为每个表格创建一个卡片\n",
    "#     for idx, table in enumerate(tables):\n",
    "#         html += f\"\"\"\n",
    "#         <div class=\"table-card\">\n",
    "#             <h3>表格 {idx+1}: {table.metadata.title or '未命名表格'}</h3>\n",
    "#             <p><strong>表格ID:</strong> {table.metadata.table_id}</p>\n",
    "#             <p><strong>页码:</strong> {table.metadata.page_number or '未知'}</p>\n",
    "#             <p><strong>行数:</strong> {table.metadata.total_rows}</p>\n",
    "#             <p><strong>列数:</strong> {table.metadata.total_columns}</p>\n",
    "#             <a href=\"table_{idx+1}.html\">查看表格</a>\n",
    "#         </div>\n",
    "#         \"\"\"\n",
    "    \n",
    "#     html += \"\"\"\n",
    "#         </div>\n",
    "#     </body>\n",
    "#     </html>\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 保存索引HTML\n",
    "#     with open(\"table_previews/index.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(html)\n",
    "\n",
    "# # -------------------------- \n",
    "# # 使用示例\n",
    "# # -------------------------- \n",
    "# if __name__ == \"__main__\":\n",
    "#     # 读取PDF文件\n",
    "#     with open(\"2.02_EN.pdf\", \"rb\") as f:\n",
    "#         pdf_bytes = f.read()\n",
    "    \n",
    "#     # 调用函数\n",
    "#     try:\n",
    "#         pdf_tables = parse_pdf_tables(pdf_bytes, \"2.02_EN.pdf\")\n",
    "#         print(f\"\\n解析完成！共提取 {pdf_tables.total_tables} 个表格。\")\n",
    "#         print(\"请打开 table_previews/index.html 查看所有表格。\")\n",
    "#     except ValueError as e:\n",
    "#         print(f\"错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response saved to raw_response.json\n",
      "All parsed table data saved to parsed_tables.json\n",
      "Table HTML previews saved to table_previews_f/ directory\n",
      "\n",
      "Extraction completed! Total tables extracted: 7.\n",
      "Please open table_previews_f/index.html to view all tables.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Union, Optional, Any, Dict\n",
    "import json\n",
    "import base64\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define table cell data type - Modified value type to Any to accommodate possible None values\n",
    "class CellValue(BaseModel):\n",
    "    value: Optional[Any] = Field(description=\"Cell value, can be string, integer, float or null\")\n",
    "    data_type: str = Field(description=\"Data type: text, number, date, etc.\")\n",
    "\n",
    "# Define table metadata\n",
    "class TableMetadata(BaseModel):\n",
    "    table_id: str = Field(description=\"Unique identifier for the table\")\n",
    "    pdf_name: str = Field(description=\"PDF file name containing the table\")\n",
    "    summary: Optional[str] = Field(default=None, description=\"Table summary\")\n",
    "    title: Optional[str] = Field(default=None, description=\"Table title\")\n",
    "    language: str = Field(description=\"Language of table content, e.g.: de, fr, it, en\")\n",
    "    source_url: Optional[str] = Field(default=None, description=\"Source URL of the table\")\n",
    "    page_number: Optional[int] = Field(default=None, description=\"PDF page number\")\n",
    "    total_rows: int = Field(description=\"Total number of rows\")\n",
    "    total_columns: int = Field(description=\"Total number of columns\")\n",
    "\n",
    "# Define table data\n",
    "class TableData(BaseModel):\n",
    "    headers: List[Optional[str]] = Field(description=\"Table column headers\")\n",
    "    rows: List[List[CellValue]] = Field(description=\"Table data rows\")\n",
    "\n",
    "# Define complete table\n",
    "class Table(BaseModel):\n",
    "    metadata: TableMetadata = Field(description=\"Table metadata\")\n",
    "    data: TableData = Field(description=\"Table content data\")\n",
    "\n",
    "# Define list of multiple tables for the entire PDF processing result\n",
    "class PdfTables(BaseModel):\n",
    "    tables: List[Table] = Field(description=\"List of all tables in the PDF\")\n",
    "    total_tables: int = Field(description=\"Total number of tables\")\n",
    "\n",
    "def generate_table_title(client, table: Table) -> str:\n",
    "    \"\"\"Generate a descriptive title for the table using Gemini API\"\"\"\n",
    "    try:\n",
    "        # Prepare text representation of table data\n",
    "        headers_text = \", \".join([h for h in table.data.headers if h])\n",
    "        \n",
    "        # Extract first few rows of data as examples\n",
    "        sample_rows = []\n",
    "        for row_idx, row in enumerate(table.data.rows):\n",
    "            if row_idx >= 3:  # Only take first 3 rows\n",
    "                break\n",
    "            row_text = \", \".join([str(cell.value) for cell in row if cell.value is not None])\n",
    "            sample_rows.append(row_text)\n",
    "        \n",
    "        rows_text = \"\\n\".join(sample_rows)\n",
    "        \n",
    "        # Construct prompt\n",
    "        prompt = f\"\"\"\n",
    "        Please generate a short, accurate, and specific title for the following table that describes its content.\n",
    "        Do not use generic words like \"table\", \"data\", etc. at the beginning of the title.\n",
    "        \n",
    "        Table headers: {headers_text}\n",
    "        Table data samples:\n",
    "        {rows_text}\n",
    "        Page number: {table.metadata.page_number or 'Unknown'}\n",
    "        \n",
    "        Table title:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call API to generate title\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                max_output_tokens=50,  # Limit output length\n",
    "                temperature=0.2  # Reduce randomness\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract and clean title\n",
    "        title = response.text.strip()\n",
    "        # Remove quotes if present\n",
    "        if title.startswith('\"') and title.endswith('\"'):\n",
    "            title = title[1:-1]\n",
    "        if title.startswith(\"'\") and title.endswith(\"'\"):\n",
    "            title = title[1:-1]\n",
    "            \n",
    "        return title if title else \"Untitled Table\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating table title: {str(e)}\")\n",
    "        return \"Untitled Table\"\n",
    "\n",
    "def parse_pdf_tables(pdf_bytes: bytes, pdf_name: str) -> PdfTables:\n",
    "    \"\"\"Parse all tables in a PDF\"\"\"\n",
    "    # Initialize client\n",
    "    api_key = \"AIzaSyA_ToQSQnQVA0eM2Pnjzi0Zz2c4utJk-TE\"\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Get JSON schema from Pydantic model\n",
    "    table_schema = Table.model_json_schema()\n",
    "    \n",
    "    # Construct prompt: embed JSON Schema, emphasize extracting all tables\n",
    "    instruction = f\"\"\"\n",
    "    Please parse all tables in the PDF sequentially and return each table's data in the following JSON format.\n",
    "    This PDF contains multiple pages and tables, please extract every table completely without omission.\n",
    "    \n",
    "    Table structure definition:\n",
    "    {json.dumps(table_schema, indent=2)}\n",
    "    \n",
    "    Please return a JSON array, with each element representing a table, conforming to the above schema.\n",
    "    \n",
    "    Additional requirements:\n",
    "    1. Each table's metadata.pdf_name must be {pdf_name}\n",
    "    2. Each table's metadata.table_id should be incrementally numbered starting from \"table_1\"\n",
    "    3. Each table's metadata.page_number must indicate the PDF page number where the table appears\n",
    "    4. Please create a metadata.title for each table describing the table's subject\n",
    "    5. Note that all cell values can be null, do not leave empty values\n",
    "    6. Please extract all tables from all pages, do not miss any\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create text part\n",
    "    text_part = types.Part(text=instruction)\n",
    "    \n",
    "    # Create PDF part\n",
    "    pdf_part = types.Part(\n",
    "        inline_data=types.Blob(\n",
    "            mime_type=\"application/pdf\",\n",
    "            data=pdf_bytes\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create content\n",
    "    contents = [\n",
    "        text_part,\n",
    "        pdf_part\n",
    "    ]\n",
    "    \n",
    "    # Use longer maximum output token count to ensure all tables can be returned\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=contents,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            max_output_tokens=8192  # Increase output length limit\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    try:\n",
    "        # Get response text\n",
    "        response_text = response.text\n",
    "        \n",
    "        # Output raw response to file\n",
    "        with open(\"raw_response.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response_text)\n",
    "        print(f\"Raw response saved to raw_response.json\")\n",
    "        \n",
    "        # Validate that return is valid JSON\n",
    "        json_data = json.loads(response_text)\n",
    "        \n",
    "        # Ensure json_data is a list\n",
    "        if not isinstance(json_data, list):\n",
    "            json_data = [json_data]\n",
    "        \n",
    "        # Initialize table list\n",
    "        tables = []\n",
    "        \n",
    "        # Process each table\n",
    "        for table_idx, table_json in enumerate(json_data):\n",
    "            try:\n",
    "                # Clean data to ensure no validation errors\n",
    "                if \"data\" in table_json and \"rows\" in table_json[\"data\"]:\n",
    "                    for row_idx, row in enumerate(table_json[\"data\"][\"rows\"]):\n",
    "                        for cell_idx, cell in enumerate(row):\n",
    "                            # Ensure each cell has value and data_type\n",
    "                            if \"value\" not in cell:\n",
    "                                cell[\"value\"] = None\n",
    "                            if \"data_type\" not in cell:\n",
    "                                cell[\"data_type\"] = \"text\"\n",
    "                \n",
    "                # Use Pydantic to parse individual table\n",
    "                table = Table.model_validate(table_json)\n",
    "                \n",
    "                # Generate title if table doesn't have one\n",
    "                if not table.metadata.title:\n",
    "                    title = generate_table_title(client, table)\n",
    "                    table.metadata.title = title\n",
    "                    print(f\"Generated title for table {table_idx+1}: {title}\")\n",
    "                \n",
    "                tables.append(table)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Table {table_idx+1} parsing failed: {str(e)}\")\n",
    "        \n",
    "        # Create complete PdfTables object\n",
    "        pdf_tables = PdfTables(\n",
    "            tables=tables,\n",
    "            total_tables=len(tables)\n",
    "        )\n",
    "        \n",
    "        # Save all parsed tables as JSON\n",
    "        with open(\"parsed_tables.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(pdf_tables.model_dump_json(indent=2, exclude_none=True))\n",
    "        print(f\"All parsed table data saved to parsed_tables.json\")\n",
    "        \n",
    "        # Generate separate HTML file for each table\n",
    "        os.makedirs(\"table_previews_f\", exist_ok=True)\n",
    "        for idx, table in enumerate(tables):\n",
    "            html_content = table_to_html(table, idx+1)\n",
    "            filename = f\"table_previews_f/table_{idx+1}.html\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html_content)\n",
    "        \n",
    "        # Create an index HTML file listing all tables\n",
    "        create_index_html(tables)\n",
    "        print(f\"Table HTML previews saved to table_previews_f/ directory\")\n",
    "        \n",
    "        return pdf_tables\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If overall parsing fails, try batch processing\n",
    "        print(f\"Overall parsing failed, trying page-by-page processing: {str(e)}\")\n",
    "        return parse_pdf_tables_by_pages(client, pdf_bytes, pdf_name)\n",
    "\n",
    "def parse_pdf_tables_by_pages(client, pdf_bytes: bytes, pdf_name: str) -> PdfTables:\n",
    "    \"\"\"Parse tables in PDF page by page, for handling large PDFs\"\"\"\n",
    "    tables = []\n",
    "    table_id_counter = 1\n",
    "    \n",
    "    # Try processing up to 30 pages\n",
    "    for page in range(1, 31):\n",
    "        try:\n",
    "            # Construct prompt\n",
    "            page_instruction = f\"\"\"\n",
    "            Please parse all tables on page {page} of the PDF and return in JSON format.\n",
    "            If there are no tables on this page, please return an empty array [].\n",
    "            \n",
    "            Additional requirements:\n",
    "            1. Each table's metadata.pdf_name must be {pdf_name}\n",
    "            2. Each table's metadata.table_id should be \"table_{table_id_counter}\"\n",
    "            3. Each table's metadata.page_number must be {page}\n",
    "            4. Please create a metadata.title for each table describing the table's subject\n",
    "            5. Note that all cell values can be null, do not leave empty values\n",
    "            \"\"\"\n",
    "            \n",
    "            # Create content\n",
    "            contents = [\n",
    "                types.Part(text=page_instruction),\n",
    "                types.Part(\n",
    "                    inline_data=types.Blob(\n",
    "                        mime_type=\"application/pdf\",\n",
    "                        data=pdf_bytes\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Use API\n",
    "            page_response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=contents,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    response_mime_type=\"application/json\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Parse response\n",
    "            page_text = page_response.text\n",
    "            page_data = json.loads(page_text)\n",
    "            \n",
    "            # Ensure page_data is a list\n",
    "            if not isinstance(page_data, list):\n",
    "                if not page_data:  # Empty response\n",
    "                    continue\n",
    "                page_data = [page_data]\n",
    "            \n",
    "            if not page_data:  # Empty list, no tables\n",
    "                print(f\"No tables detected on page {page}\")\n",
    "                continue\n",
    "            \n",
    "            # Process each table on the page\n",
    "            for table_json in page_data:\n",
    "                try:\n",
    "                    # Clean data and fix table ID\n",
    "                    if \"metadata\" in table_json:\n",
    "                        table_json[\"metadata\"][\"table_id\"] = f\"table_{table_id_counter}\"\n",
    "                        table_json[\"metadata\"][\"page_number\"] = page\n",
    "                    \n",
    "                    if \"data\" in table_json and \"rows\" in table_json[\"data\"]:\n",
    "                        for row in table_json[\"data\"][\"rows\"]:\n",
    "                            for cell in row:\n",
    "                                if \"value\" not in cell:\n",
    "                                    cell[\"value\"] = None\n",
    "                                if \"data_type\" not in cell:\n",
    "                                    cell[\"data_type\"] = \"text\"\n",
    "                    \n",
    "                    # Use Pydantic to parse\n",
    "                    table = Table.model_validate(table_json)\n",
    "                    \n",
    "                    # Generate title if table doesn't have one\n",
    "                    if not table.metadata.title:\n",
    "                        title = generate_table_title(client, table)\n",
    "                        table.metadata.title = title\n",
    "                        print(f\"Generated title for table {table_id_counter}: {title}\")\n",
    "                    \n",
    "                    tables.append(table)\n",
    "                    table_id_counter += 1\n",
    "                except Exception as table_err:\n",
    "                    print(f\"Warning: Page {page} table parsing failed: {str(table_err)}\")\n",
    "            \n",
    "            print(f\"Processed page {page}, found {len(page_data)} tables\")\n",
    "            \n",
    "        except Exception as page_err:\n",
    "            print(f\"Warning: Error processing page {page}: {str(page_err)}\")\n",
    "            # If 3 consecutive pages have no tables, assume we've reached the end of the PDF\n",
    "            if page > 3 and len(tables) == 0:\n",
    "                break\n",
    "        \n",
    "        # Prevent API throttling\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Create complete PdfTables object\n",
    "    pdf_tables = PdfTables(\n",
    "        tables=tables,\n",
    "        total_tables=len(tables)\n",
    "    )\n",
    "    \n",
    "    # Save all parsed tables as JSON\n",
    "    with open(\"parsed_tables.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(pdf_tables.model_dump_json(indent=2, exclude_none=True))\n",
    "    print(f\"All parsed table data saved to parsed_tables.json\")\n",
    "    \n",
    "    # Generate separate HTML file for each table\n",
    "    os.makedirs(\"table_previews_f\", exist_ok=True)\n",
    "    for idx, table in enumerate(tables):\n",
    "        html_content = table_to_html(table, idx+1)\n",
    "        filename = f\"table_previews_f/table_{idx+1}.html\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_content)\n",
    "    \n",
    "    # Create an index HTML file listing all tables\n",
    "    create_index_html(tables)\n",
    "    print(f\"Table HTML previews saved to table_previews_f/ directory\")\n",
    "    \n",
    "    return pdf_tables\n",
    "\n",
    "def table_to_html(table: Table, index: int = 0) -> str:\n",
    "    \"\"\"Convert table to HTML format\"\"\"\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Table {index}: {table.metadata.title or 'Untitled Table'}</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            h1 {{ color: #333; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}\n",
    "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "            th {{ background-color: #f2f2f2; }}\n",
    "            .metadata {{ background-color: #f9f9f9; padding: 10px; border-radius: 5px; margin-bottom: 20px; }}\n",
    "            .metadata p {{ margin: 5px 0; }}\n",
    "            .navigation {{ margin-top: 20px; }}\n",
    "            .navigation a {{ margin-right: 10px; padding: 5px 10px; background-color: #f0f0f0; text-decoration: none; color: #333; border-radius: 3px; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"navigation\">\n",
    "            <a href=\"index.html\">Return to Table List</a>\n",
    "        </div>\n",
    "        \n",
    "        <h1>Table {index}: {table.metadata.title or 'Untitled Table'}</h1>\n",
    "        \n",
    "        <div class=\"metadata\">\n",
    "            <h3>Metadata</h3>\n",
    "            <p><strong>Table ID:</strong> {table.metadata.table_id}</p>\n",
    "            <p><strong>PDF File:</strong> {table.metadata.pdf_name}</p>\n",
    "            <p><strong>Language:</strong> {table.metadata.language}</p>\n",
    "            <p><strong>Page Number:</strong> {table.metadata.page_number or 'Unknown'}</p>\n",
    "            <p><strong>Total Rows:</strong> {table.metadata.total_rows}</p>\n",
    "            <p><strong>Total Columns:</strong> {table.metadata.total_columns}</p>\n",
    "            {f'<p><strong>Summary:</strong> {table.metadata.summary}</p>' if table.metadata.summary else ''}\n",
    "        </div>\n",
    "        \n",
    "        <h3>Table Data</h3>\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add headers\n",
    "    for header in table.data.headers:\n",
    "        html += f\"<th>{header or ''}</th>\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add table content\n",
    "    for row in table.data.rows:\n",
    "        html += \"<tr>\"\n",
    "        for cell in row:\n",
    "            # Handle different types of cell values\n",
    "            cell_value = cell.value if cell.value is not None else \"\"\n",
    "            html += f\"<td>{cell_value}</td>\"\n",
    "        html += \"</tr>\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "def create_index_html(tables: List[Table]) -> None:\n",
    "    \"\"\"Create an HTML index page listing all tables\"\"\"\n",
    "    html = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>PDF Table Extraction Results</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "            h1 { color: #333; }\n",
    "            .table-list { display: flex; flex-wrap: wrap; }\n",
    "            .table-card { \n",
    "                border: 1px solid #ddd;\n",
    "                border-radius: 5px;\n",
    "                padding: 15px;\n",
    "                margin: 10px;\n",
    "                width: 280px;\n",
    "                background-color: #f9f9f9;\n",
    "            }\n",
    "            .table-card h3 { margin-top: 0; }\n",
    "            .table-card p { margin: 5px 0; }\n",
    "            .table-card a { \n",
    "                display: block;\n",
    "                text-align: center;\n",
    "                margin-top: 10px;\n",
    "                padding: 8px;\n",
    "                background-color: #4CAF50;\n",
    "                color: white;\n",
    "                text-decoration: none;\n",
    "                border-radius: 3px;\n",
    "            }\n",
    "            .summary { margin-bottom: 20px; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>PDF Table Extraction Results</h1>\n",
    "        \n",
    "        <div class=\"summary\">\n",
    "            <p><strong>PDF File:</strong> \"\"\" + (tables[0].metadata.pdf_name if tables else \"Unknown\") + \"\"\"</p>\n",
    "            <p><strong>Total Tables Extracted:</strong> \"\"\" + str(len(tables)) + \"\"\"</p>\n",
    "        </div>\n",
    "        \n",
    "        <h2>Table List</h2>\n",
    "        <div class=\"table-list\">\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a card for each table\n",
    "    for idx, table in enumerate(tables):\n",
    "        html += f\"\"\"\n",
    "        <div class=\"table-card\">\n",
    "            <h3>Table {idx+1}: {table.metadata.title or 'Untitled Table'}</h3>\n",
    "            <p><strong>Table ID:</strong> {table.metadata.table_id}</p>\n",
    "            <p><strong>Page Number:</strong> {table.metadata.page_number or 'Unknown'}</p>\n",
    "            <p><strong>Rows:</strong> {table.metadata.total_rows}</p>\n",
    "            <p><strong>Columns:</strong> {table.metadata.total_columns}</p>\n",
    "            <a href=\"table_{idx+1}.html\">View Table</a>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save index HTML\n",
    "    with open(\"table_previews_f/index.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "# -------------------------- \n",
    "# Usage example\n",
    "# -------------------------- \n",
    "if __name__ == \"__main__\":\n",
    "    # Read PDF file\n",
    "    with open(\"2.02_f.pdf\", \"rb\") as f:\n",
    "        pdf_bytes = f.read()\n",
    "    \n",
    "    # Call function\n",
    "    try:\n",
    "        pdf_tables = parse_pdf_tables(pdf_bytes, \"2.02_f.pdf\")\n",
    "        print(f\"\\nExtraction completed! Total tables extracted: {pdf_tables.total_tables}.\")\n",
    "        print(\"Please open table_previews_f/index.html to view all tables.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='models/chat-bison-001' display_name='PaLM 2 Chat (Legacy)' description='A legacy text-only model optimized for chat conversations' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=4096 output_token_limit=1024 supported_actions=['generateMessage', 'countMessageTokens']\n",
      "name='models/text-bison-001' display_name='PaLM 2 (Legacy)' description='A legacy model that understands text and generates text as an output' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8196 output_token_limit=1024 supported_actions=['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "name='models/embedding-gecko-001' display_name='Embedding Gecko' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1024 output_token_limit=1 supported_actions=['embedText', 'countTextTokens']\n",
      "name='models/gemini-1.0-pro-vision-latest' display_name='Gemini 1.0 Pro Vision' description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=12288 output_token_limit=4096 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-pro-vision' display_name='Gemini 1.0 Pro Vision' description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=12288 output_token_limit=4096 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-1.5-pro-latest' display_name='Gemini 1.5 Pro Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-1.5-pro-001' display_name='Gemini 1.5 Pro 001' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
      "name='models/gemini-1.5-pro-002' display_name='Gemini 1.5 Pro 002' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
      "name='models/gemini-1.5-pro' display_name='Gemini 1.5 Pro' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-1.5-flash-latest' display_name='Gemini 1.5 Flash Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-1.5-flash-001' display_name='Gemini 1.5 Flash 001' description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
      "name='models/gemini-1.5-flash-001-tuning' display_name='Gemini 1.5 Flash 001 Tuning' description='Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=16384 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createTunedModel']\n",
      "name='models/gemini-1.5-flash' display_name='Gemini 1.5 Flash' description='Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-1.5-flash-002' display_name='Gemini 1.5 Flash 002' description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
      "name='models/gemini-1.5-flash-8b' display_name='Gemini 1.5 Flash-8B' description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
      "name='models/gemini-1.5-flash-8b-001' display_name='Gemini 1.5 Flash-8B 001' description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
      "name='models/gemini-1.5-flash-8b-latest' display_name='Gemini 1.5 Flash-8B Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
      "name='models/gemini-1.5-flash-8b-exp-0827' display_name='Gemini 1.5 Flash 8B Experimental 0827' description='Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-1.5-flash-8b-exp-0924' display_name='Gemini 1.5 Flash 8B Experimental 0924' description='Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-flash-exp' display_name='Gemini 2.0 Flash Experimental' description='Gemini 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "name='models/gemini-2.0-flash' display_name='Gemini 2.0 Flash' description='Gemini 2.0 Flash' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-flash-001' display_name='Gemini 2.0 Flash 001' description='Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-flash-lite-001' display_name='Gemini 2.0 Flash-Lite 001' description='Stable version of Gemini 2.0 Flash Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-flash-lite' display_name='Gemini 2.0 Flash-Lite' description='Gemini 2.0 Flash-Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-flash-lite-preview-02-05' display_name='Gemini 2.0 Flash-Lite Preview 02-05' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-flash-lite-preview' display_name='Gemini 2.0 Flash-Lite Preview' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-pro-exp' display_name='Gemini 2.0 Pro Experimental' description='Experimental release (February 5th, 2025) of Gemini 2.0 Pro' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2097152 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-pro-exp-02-05' display_name='Gemini 2.0 Pro Experimental 02-05' description='Experimental release (February 5th, 2025) of Gemini 2.0 Pro' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2097152 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-exp-1206' display_name='Gemini Experimental 1206' description='Experimental release (February 5th, 2025) of Gemini 2.0 Pro' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2097152 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-flash-thinking-exp-01-21' display_name='Gemini 2.0 Flash Thinking Experimental 01-21' description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking' version='2.0-exp-01-21' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-flash-thinking-exp' display_name='Gemini 2.0 Flash Thinking Experimental 01-21' description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking' version='2.0-exp-01-21' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/gemini-2.0-flash-thinking-exp-1219' display_name='Gemini 2.0 Flash Thinking Experimental' description='Gemini 2.0 Flash Thinking Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/learnlm-1.5-pro-experimental' display_name='LearnLM 1.5 Pro Experimental' description='Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32767 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
      "name='models/embedding-001' display_name='Embedding 001' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent']\n",
      "name='models/text-embedding-004' display_name='Text Embedding 004' description='Obtain a distributed representation of a text.' version='004' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent']\n",
      "name='models/aqa' display_name='Model that performs Attributed Question Answering.' description='Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=7168 output_token_limit=1024 supported_actions=['generateAnswer']\n",
      "name='models/imagen-3.0-generate-002' display_name='Imagen 3.0 002 model' description='Vertex served Imagen 3.0 002 model' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=480 output_token_limit=8192 supported_actions=['predict']\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

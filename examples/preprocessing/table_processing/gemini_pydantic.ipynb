{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: camelot-py[cv] in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (1.0.0)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (8.1.7)\n",
      "Requirement already satisfied: chardet>=5.1.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (5.2.0)\n",
      "Requirement already satisfied: numpy>=1.26.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (1.26.4)\n",
      "Requirement already satisfied: openpyxl>=3.1.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (3.1.5)\n",
      "Requirement already satisfied: pdfminer-six>=20240706 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (20240706)\n",
      "Requirement already satisfied: pypdf<6.0,>=4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (5.3.0)\n",
      "Requirement already satisfied: pandas>=2.2.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (2.2.3)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (0.9.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (4.11.0.86)\n",
      "Requirement already satisfied: pypdfium2>=4 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (4.30.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.0.1->camelot-py[cv]) (0.4.6)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.2.2->camelot-py[cv]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.2.2->camelot-py[cv]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.2.2->camelot-py[cv]) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pdfminer-six>=20240706->camelot-py[cv]) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pdfminer-six>=20240706->camelot-py[cv]) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.17.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[cv]) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: camelot-py 1.0.0 does not provide the extra 'cv'\n"
     ]
    }
   ],
   "source": [
    "%pip install camelot-py[cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "C:\\ProgramData\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "print(camelot.__version__)\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未在 PDF 文件中检测到任何表格。\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "import os\n",
    "\n",
    "def extract_tables(file_path, output_path, compress=False):\n",
    "    \"\"\"\n",
    "    使用 Camelot 从 PDF 中提取表格，并导出为 CSV 文件。\n",
    "    :param file_path: 输入 PDF 文件路径\n",
    "    :param output_path: 输出 CSV 文件路径（将存储所有提取的表格）\n",
    "    :param compress: 是否对输出进行压缩（默认 False）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 提取所有页面的表格\n",
    "        tables = camelot.read_pdf(file_path, flavor='lattice', pages='all')\n",
    "        if not tables:\n",
    "            print(\"未在 PDF 文件中检测到任何表格。\")\n",
    "            return\n",
    "        \n",
    "        # 如果输出目录不存在，则创建目录\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # 导出所有提取的表格到 CSV 文件\n",
    "        tables.export(output_path, f='csv', compress=compress)\n",
    "        print(f\"表格已成功导出至 {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"在提取表格时发生错误：\", e)\n",
    "\n",
    "def main():\n",
    "    # 在主函数中直接定义文件路径\n",
    "    file_path = \"2.02_EN.pdf\"  # 输入 PDF 文件路径\n",
    "    output_path = \"output_lattice/table.csv\"   # 输出 CSV 文件路径\n",
    "    compress = False                   # 是否压缩输出文件\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"输入文件 {file_path} 不存在。\")\n",
    "        return\n",
    "\n",
    "    extract_tables(file_path, output_path, compress)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "这是文本型 PDF\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"2.02_EN.pdf\")\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() or \"\"\n",
    "if text.strip():\n",
    "    print(\"这是文本型 PDF\")\n",
    "else:\n",
    "    print(\"这可能是图像型 PDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymupdf in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (1.25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未找到表格\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "\n",
    "def extract_pdf_table_to_json(pdf_path, page_num=0, output_file=\"table.json\"):\n",
    "    \"\"\"\n",
    "    提取 PDF 指定页面的第一个表格并转换为 JSON\n",
    "    :param pdf_path: PDF 文件路径\n",
    "    :param page_num: 目标页面编号（默认第0页）\n",
    "    :param output_file: 输出 JSON 文件名\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    try:\n",
    "        page = doc[page_num]\n",
    "    except IndexError:\n",
    "        print(f\"错误：页面 {page_num} 不存在\")\n",
    "        return\n",
    "\n",
    "    # 查找页面中的表格\n",
    "    tables = page.find_tables()\n",
    "    \n",
    "    if len(tables.tables) == 0:\n",
    "        print(\"未找到表格\")\n",
    "        return\n",
    "    \n",
    "    # 提取第一个表格数据\n",
    "    table_data = tables[0].extract()\n",
    "    \n",
    "    # 转换为 JSON 格式（二维数组）\n",
    "    json_data = {\n",
    "        \"table\": table_data,\n",
    "        \"metadata\": {\n",
    "            \"page\": page_num + 1,  # 转换为人类可读页码\n",
    "            \"columns\": len(table_data[0]),\n",
    "            \"rows\": len(table_data)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 保存到文件\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"表格已保存至 {output_file}\")\n",
    "\n",
    "# 使用示例\n",
    "extract_pdf_table_to_json(\n",
    "    pdf_path=\"2.02_EN.pdf\",\n",
    "    page_num=0,\n",
    "    output_file=\"output.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "页面原始文本结构:\n",
      "{'width': 419.52801513671875, 'height': 595.2760009765625, 'blocks': [{'number': 1, 'type': 0, 'bbox': (99.21260070800781, 65.32991790771484, 219.97662353515625, 82.03192138671875), 'lines': [{'spans': [{'size': 14.0, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Bold', 'color': 13027014, 'alpha': 255, 'ascender': 0.9430000185966492, 'descender': -0.25, 'text': '2.02 Contributions', 'origin': (99.21260070800781, 78.53192138671875), 'bbox': (99.21260070800781, 65.32991790771484, 219.97662353515625, 82.03192138671875)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (99.21260070800781, 65.32991790771484, 219.97662353515625, 82.03192138671875)}]}, {'number': 2, 'type': 0, 'bbox': (81.49610137939453, 110.29820251464844, 363.80706787109375, 169.0712890625), 'lines': [{'spans': [{'size': 16.0, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Bold', 'color': 1907995, 'alpha': 255, 'ascender': 0.9430000185966492, 'descender': -0.25, 'text': 'Self-employed contributions to Old-', 'origin': (90.70860290527344, 125.38619995117188), 'bbox': (90.70860290527344, 110.29820251464844, 358.99169921875, 129.38619995117188)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (90.70860290527344, 110.29820251464844, 358.99169921875, 129.38619995117188)}, {'spans': [{'size': 16.0, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Bold', 'color': 1907995, 'alpha': 255, 'ascender': 0.9430000185966492, 'descender': -0.25, 'text': 'Age and Survivors’ Insurance (OASI), ', 'origin': (86.45670318603516, 145.22879028320312), 'bbox': (86.45670318603516, 130.1407928466797, 363.80706787109375, 149.22879028320312)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (86.45670318603516, 130.1407928466797, 363.80706787109375, 149.22879028320312)}, {'spans': [{'size': 16.0, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Bold', 'color': 1907995, 'alpha': 255, 'ascender': 0.9430000185966492, 'descender': -0.25, 'text': 'Disability Insurance (DI) and Income ', 'origin': (81.49610137939453, 165.0712890625), 'bbox': (81.49610137939453, 149.98329162597656, 353.4865417480469, 169.0712890625)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (81.49610137939453, 149.98329162597656, 353.4865417480469, 169.0712890625)}]}, {'number': 3, 'type': 0, 'bbox': (73.70079803466797, 207.07069396972656, 201.85189819335938, 218.68069458007812), 'lines': [{'spans': [{'size': 10.0, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Light', 'color': 1907995, 'alpha': 255, 'ascender': 0.9110000133514404, 'descender': -0.25, 'text': 'Position as of 1', 'origin': (73.70079803466797, 216.18069458007812), 'bbox': (73.70079803466797, 207.07069396972656, 138.58778381347656, 218.68069458007812)}, {'size': 5.829999923706055, 'flags': 5, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Light', 'color': 1907995, 'alpha': 255, 'ascender': 0.9110000133514404, 'descender': -0.25, 'text': 'st', 'origin': (137.9781951904297, 212.8507080078125), 'bbox': (137.9781951904297, 207.53958129882812, 142.34486389160156, 214.30821228027344)}, {'size': 10.0, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Light', 'color': 1907995, 'alpha': 255, 'ascender': 0.9110000133514404, 'descender': -0.25, 'text': ' January 2025', 'origin': (142.3459014892578, 216.18069458007812), 'bbox': (142.3459014892578, 207.07069396972656, 201.85189819335938, 218.68069458007812)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (73.70079803466797, 207.07069396972656, 201.85189819335938, 218.68069458007812)}]}, {'number': 4, 'type': 0, 'bbox': (76.535400390625, 169.82579040527344, 289.5770263671875, 188.91378784179688), 'lines': [{'spans': [{'size': 16.0, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'FrutigerLTStd-Bold', 'color': 1907995, 'alpha': 255, 'ascender': 0.9430000185966492, 'descender': -0.25, 'text': 'Compensation Insurance (IC)', 'origin': (76.535400390625, 184.91378784179688), 'bbox': (76.535400390625, 169.82579040527344, 289.5770263671875, 188.91378784179688)}], 'wmode': 0, 'dir': (1.0, 0.0), 'bbox': (76.535400390625, 169.82579040527344, 289.5770263671875, 188.91378784179688)}]}]}\n",
      "\n",
      "检测到 0 个表格候选区域\n",
      "已生成调试文件: debug_tables.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "def debug_pdf_tables(pdf_path, page_num=0):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_num]\n",
    "    \n",
    "    # 查看页面文本结构\n",
    "    print(\"页面原始文本结构:\")\n",
    "    print(page.get_text(\"dict\"))\n",
    "    \n",
    "    # 检测所有表格\n",
    "    tables = page.find_tables()\n",
    "    print(f\"\\n检测到 {len(tables.tables)} 个表格候选区域\")\n",
    "    \n",
    "    # 绘制表格识别框（可视化调试）\n",
    "    for i, table in enumerate(tables.tables):\n",
    "        print(f\"表格 {i+1} 的边界框坐标:\", table.bbox)\n",
    "        # 在PDF上绘制红色框\n",
    "        highlight = page.add_highlight_annot(table.bbox)\n",
    "        highlight.set_colors(stroke=(1, 0, 0))  # 红色边框\n",
    "        highlight.update()\n",
    "    \n",
    "    # 保存带标注的PDF\n",
    "    doc.save(\"debug_tables.pdf\")\n",
    "    print(\"已生成调试文件: debug_tables.pdf\")\n",
    "\n",
    "debug_pdf_tables(\"2.02_EN.pdf\", page_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfplumber) (10.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 22.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 21.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pdfminer.six, pdfplumber\n",
      "  Attempting uninstall: pdfminer.six\n",
      "    Found existing installation: pdfminer.six 20240706\n",
      "    Uninstalling pdfminer.six-20240706:\n",
      "      Successfully uninstalled pdfminer.six-20240706\n",
      "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pdfplumber.exe is installed in 'C:\\Users\\shaer\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "camelot-py 1.0.0 requires pdfminer-six>=20240706, but you have pdfminer-six 20231228 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Year of birth Reference age\n",
      "0    Up to 1960            64\n",
      "Empty DataFrame\n",
      "Columns: [1962, 64 plus 6 months]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [From 1964, 65]\n",
      "Index: []\n",
      "  Contribution rates    None\n",
      "0               OASI   8.1 %\n",
      "1                 DI   1.4 %\n",
      "2                 IC   0.5 %\n",
      "3              Total  10.0 %\n",
      "   Annual income in francs           None  \\\n",
      "0              of at least  but less than   \n",
      "1                   10 100         17 600   \n",
      "2                   17 600         23 000   \n",
      "3                   23 000         25 500   \n",
      "4                   25 500         28 000   \n",
      "5                   28 000         30 500   \n",
      "6                   30 500         33 000   \n",
      "7                   33 000         35 500   \n",
      "8                   35 500         38 000   \n",
      "9                   38 000         40 500   \n",
      "10                  40 500         43 000   \n",
      "11                  43 000         45 500   \n",
      "12                  45 500         48 000   \n",
      "13                  48 000         50 500   \n",
      "14                  50 500         53 000   \n",
      "15                  53 000         55 500   \n",
      "16                  55 500         58 000   \n",
      "17                  58 000         60 500   \n",
      "18                  60 500                  \n",
      "\n",
      "   OASI/DI/IC contribu-\\ntion rate as percent-\\nage of earnings  \n",
      "0                                                None            \n",
      "1                                               5.371            \n",
      "2                                               5.494            \n",
      "3                                               5.617            \n",
      "4                                               5.741            \n",
      "5                                               5.864            \n",
      "6                                               5.987            \n",
      "7                                               6.235            \n",
      "8                                               6.481            \n",
      "9                                               6.728            \n",
      "10                                              6.976            \n",
      "11                                              7.222            \n",
      "12                                              7.469            \n",
      "13                                              7.840            \n",
      "14                                              8.209            \n",
      "15                                              8.580            \n",
      "16                                              8.951            \n",
      "17                                              9.321            \n",
      "18                                             10.000            \n",
      "   Year Interest rate\n",
      "0  2016         0,0 %\n",
      "1  2017         0,5 %\n",
      "2  2018         0,5 %\n",
      "3  2019         0,0 %\n",
      "4  2020         0,0 %\n",
      "5  2021         0,0 %\n",
      "6  2022         1,5 %\n",
      "7  2023         2,0 %\n",
      "                                          Relates to  \\\n",
      "0                          Contributions on\\naccount   \n",
      "1  Difference between\\ncontributions on account\\n...   \n",
      "\n",
      "    Payment not\\nreceived within         Interest accrued\\nfrom  \n",
      "0  30 days after end\\nof quarter  1st day after end\\nof quarter  \n",
      "1     30 days from invoice\\ndate     1st day from invoice\\ndate  \n",
      "                                          Relates to  \\\n",
      "0  Contributions on account amount\\nto less than ...   \n",
      "1                   Contributions for previous years   \n",
      "\n",
      "                               Interest accrued from  \n",
      "0  1st January after the calendar year\\nfollowing...  \n",
      "1  1st January after end of relevant\\ncontributio...  \n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "with pdfplumber.open(\"2.02_EN.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        tables = page.extract_tables()\n",
    "        for table in tables:\n",
    "            # 直接转成DataFrame\n",
    "            import pandas as pd\n",
    "            df = pd.DataFrame(table[1:], columns=table[0])\n",
    "            print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "页码 1 未检测到表格\n",
      "页码 2 未检测到表格\n",
      "页码 3 未检测到表格\n",
      "已保存表格：page_4_table_1.json\n",
      "已保存表格：page_4_table_2.json\n",
      "已保存表格：page_4_table_3.json\n",
      "已保存表格：page_5_table_1.json\n",
      "已保存表格：page_5_table_2.json\n",
      "已保存表格：page_6_table_1.json\n",
      "页码 7 未检测到表格\n",
      "已保存表格：page_8_table_1.json\n",
      "已保存表格：page_8_table_2.json\n",
      "页码 9 未检测到表格\n",
      "页码 10 未检测到表格\n",
      "页码 11 未检测到表格\n",
      "页码 12 未检测到表格\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def tables_to_json(pdf_path, output_dir=\"output_tables\"):\n",
    "    \"\"\"\n",
    "    将PDF中所有表格转为统一JSON格式\n",
    "    :param pdf_path: PDF文件路径\n",
    "    :param output_dir: 输出目录\n",
    "    \"\"\"\n",
    "    # 创建输出目录\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            # 提取当前页所有表格\n",
    "            tables = page.extract_tables()\n",
    "            \n",
    "            if not tables:\n",
    "                print(f\"页码 {page_num+1} 未检测到表格\")\n",
    "                continue\n",
    "                \n",
    "            for table_num, table in enumerate(tables, 1):\n",
    "                # 清洗数据（处理None值和换行符）\n",
    "                cleaned_table = [\n",
    "                    [\n",
    "                        cell.replace(\"\\n\", \" \").strip() if cell is not None else \"\"\n",
    "                        for cell in row\n",
    "                    ]\n",
    "                    for row in table\n",
    "                ]   \n",
    "                # 构建统一JSON结构\n",
    "                json_data = {\n",
    "                    \"metadata\": {\n",
    "                        \"source\": pdf_path,\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"table_number\": table_num,\n",
    "                        \"dimensions\": {\n",
    "                            \"rows\": len(cleaned_table),\n",
    "                            \"columns\": len(cleaned_table[0]) if cleaned_table else 0\n",
    "                        }\n",
    "                    },\n",
    "                    \"data\": {\n",
    "                        \"headers\": cleaned_table[0] if cleaned_table else [],\n",
    "                        \"rows\": cleaned_table[1:] if len(cleaned_table) > 1 else []\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # 生成文件名\n",
    "                filename = f\"page_{page_num+1}_table_{table_num}.json\"\n",
    "                output_path = Path(output_dir) / filename\n",
    "                \n",
    "                # 保存JSON文件\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                print(f\"已保存表格：{filename}\")\n",
    "\n",
    "# 使用示例\n",
    "tables_to_json(\n",
    "    pdf_path=\"2.02_EN.pdf\",\n",
    "    output_dir=\"plumber_json_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存表格：page_4_table_1.json\n",
      "已保存表格：page_5_table_1.json\n",
      "已保存表格：page_5_table_2.json\n",
      "已保存表格：page_6_table_1.json\n",
      "已保存表格：page_8_table_1.json\n",
      "已保存表格：page_8_table_2.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def process_pdf_tables(input_dir=\".\", output_base=\"output_tables\"):\n",
    "    \"\"\"\n",
    "    批量处理指定目录下的所有PDF文件表格\n",
    "    :param input_dir: 输入目录路径（默认当前目录）\n",
    "    :param output_base: 输出根目录（默认output_tables）\n",
    "    \"\"\"\n",
    "    # 获取所有PDF文件（包括子目录）\n",
    "    pdf_files = glob.glob(f\"{input_dir}/**/*.pdf\", recursive=True)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"在 {input_dir} 目录中未找到PDF文件\")\n",
    "        return\n",
    "\n",
    "    for pdf_path in pdf_files:\n",
    "        try:\n",
    "            # 创建对应输出目录\n",
    "            pdf_stem = Path(pdf_path).stem  # 获取不带扩展名的文件名\n",
    "            output_dir = Path(output_base) / pdf_stem\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            print(f\"\\n正在处理文件：{Path(pdf_path).name}\")\n",
    "            \n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                process_single_pdf(doc, pdf_path, output_dir)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {pdf_path} 失败：{str(e)}\")\n",
    "\n",
    "def process_single_pdf(doc, pdf_path, output_dir):\n",
    "    \"\"\"处理单个PDF文件\"\"\"\n",
    "    for page_index in range(len(doc)):\n",
    "        page = doc[page_index]\n",
    "        tables = page.find_tables()\n",
    "        \n",
    "        if not tables:\n",
    "            print(f\"  第 {page_index + 1} 页没有检测到表格\")\n",
    "            continue\n",
    "\n",
    "        for table_num, table in enumerate(tables, start=1):\n",
    "            try:\n",
    "                # 提取表格数据\n",
    "                table_data = table.extract()\n",
    "                \n",
    "                # 构建JSON结构\n",
    "                json_data = {\n",
    "                    \"metadata\": {\n",
    "                        \"source\": str(pdf_path),\n",
    "                        \"page\": page_index + 1,\n",
    "                        \"table_number\": table_num,\n",
    "                        \"bbox\": list(table.bbox),\n",
    "                        \"dimensions\": {\n",
    "                            \"rows\": len(table_data),\n",
    "                            \"columns\": len(table_data[0]) if table_data else 0\n",
    "                        }\n",
    "                    },\n",
    "                    \"data\": table_data\n",
    "                }\n",
    "                \n",
    "                # 生成文件名\n",
    "                filename = f\"page_{page_index+1}_table_{table_num}.json\"\n",
    "                output_path = output_dir / filename\n",
    "                \n",
    "                # 保存文件\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                print(f\"  已保存表格：{filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  表格处理失败：{str(e)}\")\n",
    "\n",
    "# 使用示例（处理当前目录及其子目录下的所有PDF）\n",
    "process_pdf_tables(\n",
    "    input_dir=\".\", \n",
    "    output_base=\"output_tables\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camelot\n",
    "Stream\n",
    "Suitable for tables where whitespaces between cells simulate the table structure. It leverages PDFMiner's functionality to group characters into words and sentences, analyzing margins to infer table boundaries. Ideal for borderless tables but struggles with complex layouts.\n",
    "\n",
    "Lattice\n",
    "Designed for tables with explicit demarcation lines. Detects line segments and intersections via image processing (using OpenCV) to define precise table boundaries. Highly accurate for multi-table pages and merged cells but fails for borderless tables.\n",
    "\n",
    "Network\n",
    "Relies on text element bounding boxes to identify horizontal/vertical alignment patterns. Effective for tables without lines but with strong text alignment. Struggles with irregular or loosely structured layouts.\n",
    "\n",
    "Hybrid\n",
    "Combines Network's text alignment analysis with Lattice's line detection. Uses Lattice's precise boundaries to enhance Network results. Optimized for mixed-layout tables (partially lined + text-aligned) but computationally intensive.\n",
    "\n",
    "While Camelot's official examples include similar tables, none of the four modes worked reliably for our specific case due to irregular text alignment and partial/no borders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未检测到任何表格\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_tables_with_camelot(pdf_path, output_dir=\"camelot_output\"):\n",
    "    \"\"\"\n",
    "    使用Camelot提取表格并保存为JSON\n",
    "    :param pdf_path: PDF文件路径\n",
    "    :param output_dir: 输出目录\n",
    "    \"\"\"\n",
    "\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "  \n",
    "    try:\n",
    "        tables = camelot.read_pdf(\n",
    "            pdf_path, \n",
    "            flavor=\"network\",\n",
    "            edge_tol=50,   \n",
    "            row_tol=10      \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"文件解析失败: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    if not tables:\n",
    "        print(\"未检测到任何表格\")\n",
    "        return\n",
    "\n",
    "    for table_num, table in enumerate(tables, 1):\n",
    "        df = table.df\n",
    "        cleaned_table = [\n",
    "            [cell.strip().replace(\"\\n\", \" \") if cell else \"\" \n",
    "             for cell in row\n",
    "            ]\n",
    "            for row in df.values.tolist()\n",
    "        ]\n",
    "             \n",
    "        json_data = {\n",
    "            \"metadata\": {\n",
    "                \"source\": pdf_path,\n",
    "                \"page\": table.page + 1,  \n",
    "                \"table_number\": table_num,\n",
    "                \"accuracy\": round(table.parsing_report[\"accuracy\"], 2),\n",
    "                \"dimensions\": {\n",
    "                    \"rows\": len(cleaned_table),\n",
    "                    \"columns\": len(cleaned_table[0]) if cleaned_table else 0\n",
    "                }\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"headers\": cleaned_table[0] if cleaned_table else [],\n",
    "                \"rows\": cleaned_table[1:] if len(cleaned_table) > 1 else []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        \n",
    "        filename = f\"page_{table.page+1}_table_{table_num}.json\"\n",
    "        output_path = Path(output_dir) / filename\n",
    "        \n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"已保存表格：{filename}\")\n",
    "\n",
    "extract_tables_with_camelot(\n",
    "    pdf_path=\"2.01_EN.pdf\",\n",
    "    output_dir=\"camelot_json_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "处理第 1 个表格 (页面 2)\n",
      "解析报告: {'accuracy': 100.0, 'whitespace': 33.91, 'order': 1, 'page': 1}\n",
      "边界框: (31.56, 51.966239999999885, 738.46704, 590.64576)\n",
      "已保存表格数据：page_2_table_1.json\n",
      "可视化出错: PlotMethods.__call__() got an unexpected keyword argument 'figsize'\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "def extract_tables_with_camelot(pdf_path, output_dir=\"camelot_output\", debug=False, visualize=False):\n",
    "    \"\"\"\n",
    "    使用Camelot提取表格并保存为JSON，同时增加调试信息和可视化图像的保存\n",
    "    :param pdf_path: PDF文件路径\n",
    "    :param output_dir: 输出目录\n",
    "    :param debug: 是否打印调试信息\n",
    "    :param visualize: 是否生成并显示可视化图像\n",
    "    \"\"\"\n",
    "    \n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        tables = camelot.read_pdf(\n",
    "            pdf_path, \n",
    "            flavor=\"network\",  # 如遇到问题，可尝试改为 \"lattice\"\n",
    "            edge_tol=50,    # 提高边缘检测敏感度\n",
    "            row_tol=10      # 优化行间距识别\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"文件解析失败: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    if not tables or len(tables) == 0:\n",
    "        print(\"未检测到任何表格\")\n",
    "        return\n",
    "\n",
    "    for table_num, table in enumerate(tables, 1):\n",
    "        # 如果开启调试，打印当前表格的解析报告和其他信息\n",
    "        if debug:\n",
    "            print(f\"------------------------------\")\n",
    "            print(f\"处理第 {table_num} 个表格 (页面 {table.page + 1})\")\n",
    "            print(\"解析报告:\", table.parsing_report)\n",
    "            print(\"边界框:\", table._bbox)  # 内部边界信息\n",
    "        \n",
    "        # 获取数据（自动处理合并单元格）\n",
    "        df = table.df\n",
    "        \n",
    "        # 清洗数据（处理空值和换行符）\n",
    "        cleaned_table = [\n",
    "            [cell.strip().replace(\"\\n\", \" \") if cell else \"\" \n",
    "             for cell in row\n",
    "            ]\n",
    "            for row in df.values.tolist()\n",
    "        ]\n",
    "        \n",
    "        # 构建JSON结构\n",
    "        json_data = {\n",
    "            \"metadata\": {\n",
    "                \"source\": pdf_path,\n",
    "                \"page\": table.page + 1,  # Camelot页码从0开始\n",
    "                \"table_number\": table_num,\n",
    "                \"accuracy\": round(table.parsing_report.get(\"accuracy\", 0), 2),\n",
    "                \"dimensions\": {\n",
    "                    \"rows\": len(cleaned_table),\n",
    "                    \"columns\": len(cleaned_table[0]) if cleaned_table else 0\n",
    "                }\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"headers\": cleaned_table[0] if cleaned_table else [],\n",
    "                \"rows\": cleaned_table[1:] if len(cleaned_table) > 1 else []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 生成JSON文件名\n",
    "        json_filename = f\"page_{table.page+1}_table_{table_num}.json\"\n",
    "        json_output_path = Path(output_dir) / json_filename\n",
    "        \n",
    "        # 保存JSON文件\n",
    "        with open(json_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"已保存表格数据：{json_filename}\")\n",
    "        \n",
    "        # 如果开启可视化，生成并保存表格的图像\n",
    "        if visualize:\n",
    "            try:\n",
    "                # 使用 grid 图展示表格检测结果\n",
    "                plot = camelot.plot(table, kind='grid', figsize=(10, 10))\n",
    "                # 保存图像文件\n",
    "                img_filename = f\"debug_page_{table.page+1}_table_{table_num}.png\"\n",
    "                img_output_path = Path(output_dir) / img_filename\n",
    "                plot.savefig(img_output_path)\n",
    "                print(f\"已保存调试图像：{img_filename}\")\n",
    "                # 显示图像\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"可视化出错: {str(e)}\")\n",
    "\n",
    "# 使用示例\n",
    "extract_tables_with_camelot(\n",
    "    pdf_path=\"column_separators.pdf\",\n",
    "    output_dir=\"camelot_json_output\",\n",
    "    debug=True,      \n",
    "    visualize=True   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo LLAMA_CLOUD_API_KEY=llx-WQMylwiPgluWrsrbK8NHnNISeemscgGSjB6tiKdBJOeA4SzJ > .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: llama-index in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (0.12.18)\n",
      "Requirement already satisfied: llama-parse in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (0.6.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.4.6)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.18 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.12.18)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.6.6)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.3.20)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.4.5)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-parse) (0.6.1)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services>=0.6.1->llama-parse) (8.1.7)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.11 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services>=0.6.1->llama-parse) (0.1.12)\n",
      "Requirement already satisfied: pydantic!=2.10 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services>=0.6.1->llama-parse) (2.10.6)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.60.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.18->llama-index) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (3.10.8)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (3.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.13.0,>=0.12.18->llama-index) (1.17.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from click<9.0.0,>=8.1.7->llama-cloud-services>=0.6.1->llama-parse) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud<0.2.0,>=0.1.11->llama-cloud-services>=0.6.1->llama-parse) (2024.8.30)\n",
      "Requirement already satisfied: anyio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.18->llama-index) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.18->llama-index) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.18->llama-index) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=2.10->llama-cloud-services>=0.6.1->llama-parse) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=2.10->llama-cloud-services>=0.6.1->llama-parse) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.18->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.18->llama-index) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.18->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.18->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.18->llama-index) (3.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.18->llama-index) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (1.6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: llama-cloud-services in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (0.6.1)\n",
      "Requirement already satisfied: llama-index-core in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (0.12.18)\n",
      "Requirement already satisfied: llama-index-readers-file in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services) (8.1.7)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.11 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services) (0.1.12)\n",
      "Requirement already satisfied: pydantic!=2.10 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services) (2.10.6)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud-services) (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (3.10.8)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core) (1.17.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file) (4.13.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file) (5.3.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-readers-file) (0.0.26)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from click<9.0.0,>=8.1.7->llama-cloud-services) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from llama-cloud<0.2.0,>=0.1.11->llama-cloud-services) (2024.8.30)\n",
      "Requirement already satisfied: anyio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpx->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from nltk>3.8.1->llama-index-core) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=2.10->llama-cloud-services) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=2.10->llama-cloud-services) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.31.0->llama-index-core) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json->llama-index-core) (3.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-readers-file) (2024.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaer\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index llama-parse python-dotenv\n",
    "%pip install nest_asyncio\n",
    "%pip install llama-cloud-services llama-index-core llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: llx-WQMylwiPgluWrsrbK8NHnNISeemscgGSjB6tiKdBJOeA4SzJ\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# check if load or not\n",
    "load_dotenv()  \n",
    "api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "print(\"API Key:\", api_key)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 0cc3d78b-11ef-4b3e-94fb-03dd024201dc\n",
      "....18\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() \n",
    "\n",
    "\n",
    "parser = LlamaParse(api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"), result_type=\"markdown\")\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['2.01_EN.pdf'],\n",
    "    file_extractor=file_extractor\n",
    ").load_data() \n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: llx-WQMylwiPgluWrsrbK8NHnNISeemscgGSjB6tiKdBJOeA4SzJ\n",
      "Started parsing the file under job_id af6a0a1c-6d68-4e40-b6c4-622ca808a3e5\n",
      "Documents loaded: 18\n",
      "Processing document 1/18...\n",
      "Document 1: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 2/18...\n",
      "Document 2: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 3/18...\n",
      "Document 3: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 4/18...\n",
      "Document 4: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 5/18...\n",
      "Document 5: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 6/18...\n",
      "Document 6: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 7/18...\n",
      "Document 7: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 8/18...\n",
      "Document 8: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 9/18...\n",
      "Document 9: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 10/18...\n",
      "Document 10: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 11/18...\n",
      "Document 11: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 12/18...\n",
      "Document 12: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 13/18...\n",
      "Document 13: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 14/18...\n",
      "Document 14: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 15/18...\n",
      "Document 15: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 16/18...\n",
      "Document 16: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 17/18...\n",
      "Document 17: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "Processing document 18/18...\n",
      "Document 18: Error parsing output with Pydantic: Expecting value: line 1 column 1 (char 0)\n",
      "JSON output saved at: output\\parsed_tables.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from pdf2image import convert_from_path\n",
    "from google.api_core.exceptions import InvalidArgument\n",
    "import google.generativeai as genai\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import nest_asyncio\n",
    "from pydantic import BaseModel, root_validator\n",
    "from typing import List, Optional\n",
    "\n",
    "# 应用 nest_asyncio 解决 Jupyter Notebook 环境下的异步问题\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 加载环境变量并配置 Gemini/LlamaParse API\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "print(\"API Key:\", api_key)\n",
    "\n",
    "# 定义 Pydantic 模型（结构化输出）\n",
    "class CellContent(BaseModel):\n",
    "    content: str\n",
    "\n",
    "class Table(BaseModel):\n",
    "    header_names: List[str]\n",
    "    n_cols: int\n",
    "    n_rows: int\n",
    "    row_content: List[List[CellContent]]\n",
    "\n",
    "class ParsedTable(BaseModel):\n",
    "    tables: List[Table]\n",
    "    description: str\n",
    "\n",
    "class PageResult(BaseModel):\n",
    "    page: int\n",
    "    parsed_table: Optional[ParsedTable]\n",
    "    error: Optional[str]\n",
    "    raw_output: str\n",
    "\n",
    "# 如果你需要定义外层文档结构，可以使用 RootModel（Pydantic v2 的方式），但这里直接使用 List[PageResult]即可。\n",
    "\n",
    "# 定义一个清洗函数，去除返回文本中可能的 markdown 代码块标记\n",
    "def clean_json_output(raw_text: str) -> str:\n",
    "    raw_text = raw_text.strip()\n",
    "    # 使用正则移除开头的 ```json 和结尾的 ```\n",
    "    pattern = r\"^```(?:json)?\\s*|```$\"\n",
    "    cleaned = re.sub(pattern, \"\", raw_text, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n",
    "\n",
    "# 定义用于提取表格的 prompt（要求输出纯 JSON，不含 markdown）\n",
    "prompt = \"\"\"<instructions>\n",
    "  <context>\n",
    "    You will receive an image converted from a PDF page. The image may contain table data as well as irrelevant elements such as logos, headers, and footers.\n",
    "  </context>\n",
    "  <goal>\n",
    "    Please extract only the table data from the image and ignore any non-relevant content.\n",
    "    Your tasks are:\n",
    "      1. Identify and extract all tables.\n",
    "      2. Discard any information that is not part of a table.\n",
    "  </goal>\n",
    "  <response_format>\n",
    "    Please output the parsed result in pure JSON format with the following structure:\n",
    "    {\n",
    "      \"tables\": [\n",
    "        {\n",
    "          \"header_names\": [\"Column1\", \"Column2\", \"...\"],\n",
    "          \"n_cols\": integer,\n",
    "          \"n_rows\": integer,\n",
    "          \"row_content\": [\n",
    "            [ {\"content\": \"Cell content\"}, {\"content\": \"Cell content\"}, ... ],\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"description\": \"Short description summarizing the table content.\"\n",
    "    }\n",
    "  </response_format>\n",
    "  <additional_instructions>\n",
    "    - Extract and return only table-related data, ignoring logos, headers, footers, etc.\n",
    "    - Output must be in pure JSON format without any markdown code fences or additional commentary.\n",
    "  </additional_instructions>\n",
    "</instructions>\"\"\"\n",
    "\n",
    "# 配置 LlamaParse，指定 result_type 为 \"markdown\"（如果模型不支持直接 json，则可能返回 markdown 格式，此时后处理函数会处理）\n",
    "parser = LlamaParse(api_key=api_key, result_type=\"markdown\")\n",
    "# 建立文件提取器映射：扩展名为 .pdf 由 LlamaParse 处理\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "# 使用 SimpleDirectoryReader 读取 PDF 文件（你可以在 input_files 中指定多个文件）\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['2.01_EN.pdf'],\n",
    "    file_extractor=file_extractor\n",
    ").load_data()\n",
    "print(\"Documents loaded:\", len(documents))\n",
    "\n",
    "results = []  # 用于存储每一页的解析结果\n",
    "\n",
    "# 遍历每个 Document（通常每个 Document 对应 PDF 的一页）\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Processing document {i+1}/{len(documents)}...\")\n",
    "    raw_output = doc.text  # 假设 doc.text 包含 LlamaParse 返回的内容\n",
    "    cleaned_output = clean_json_output(raw_output)\n",
    "    try:\n",
    "        # 尝试解析为 Python 对象\n",
    "        page_json = json.loads(cleaned_output)\n",
    "        # 利用 Pydantic 模型验证输出结构\n",
    "        parsed_table = ParsedTable.parse_obj(page_json)\n",
    "        results.append({\n",
    "            \"page\": i + 1,\n",
    "            \"parsed_table\": parsed_table.dict(),\n",
    "            \"raw_output\": raw_output\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Document {i+1}: Error parsing output with Pydantic: {e}\")\n",
    "        results.append({\n",
    "            \"page\": i + 1,\n",
    "            \"parsed_table\": None,\n",
    "            \"error\": str(e),\n",
    "            \"raw_output\": raw_output\n",
    "        })\n",
    "\n",
    "# 保存结果到 JSON 文件\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"parsed_tables.json\")\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "print(\"JSON output saved at:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 948f40db-e2b7-4096-9bf8-c16d5309a9d5\n",
      "Error while parsing the file '<bytes/buffer>': 'json'\n",
      "Number of documents loaded: 0\n"
     ]
    }
   ],
   "source": [
    "# 定义定制的 prompt，要求仅提取表格数据并输出为 JSON 格式\n",
    "prompt = \"\"\"<instructions>\n",
    "  <context>\n",
    "    You will receive an pdf and the pdf may contain table data as well as irrelevant elements such as logos, headers, and footers.\n",
    "  </context>\n",
    "  <goal>\n",
    "    Please extract only the table data from the pdf and ignore any non-relevant content. Your tasks are:\n",
    "    1. Identify and extract all tables.\n",
    "    2. Discard any information that is not part of a table.\n",
    "    3. Parse the table data and output the results in a JSON format.\n",
    "    4. Provide a short description of the extracted table data.\n",
    "  </goal>\n",
    "  <response_format>\n",
    "    Please output the parsed result in the following JSON structure:\n",
    "    \n",
    "    {\n",
    "      \"tables\": [\n",
    "        {\n",
    "          \"header_names\": [\"Column1\", \"Column2\", \"...\"],\n",
    "          \"n_cols\": integer,\n",
    "          \"n_rows\": integer,\n",
    "          \"row_content\": [\n",
    "            { \"content\": \"Cell content\" },\n",
    "            { \"content\": \"Cell content\" },\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"description\": \"Short description\"\n",
    "    }\n",
    "  </response_format>\n",
    "  <additional_instructions>\n",
    "    - Extract and return only table-related data, ensuring that all non-table elements (e.g., logos, headers, footers) are ignored.\n",
    "    - The output must be in pure JSON format without any additional commentary.\n",
    "    - You can use XML-like formatting in your instructions to enhance clarity.\n",
    "  </additional_instructions>\n",
    "</instructions>\"\"\"\n",
    "\n",
    "# 实例化 LlamaParse 时传入自定义 prompt，要求结果为 JSON 格式输出\n",
    "parser = LlamaParse(\n",
    "    api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
    "    result_type=\"json\",\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# 配置文件提取器：对于 .pdf 文件使用我们定制的 parser\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "# 使用 SimpleDirectoryReader 读取 PDF 文件并解析数据\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['data/2.01_EN.pdf'],\n",
    "    file_extractor=file_extractor\n",
    ").load_data()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "for doc in documents:\n",
    "    # 此处 doc.text 应该为纯 JSON 格式的字符串（如果 LLM 正常返回的话）\n",
    "    print(\"Parsed Document:\")\n",
    "    print(doc.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

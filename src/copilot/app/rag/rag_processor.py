from rag.prompts import OPENAI_RAG_SYSTEM_PROMPT_DE, QUERY_REWRITING_PROMPT

from typing import List, Dict, Any

from rag.models import RAGRequest, EmbeddingRequest
from rag.factory import RetrieverFactory
from rag.llm.factory import LLMFactory
from rag.llm.base import BaseLLM

from sqlalchemy.orm import Session
from utils.embedding import get_embedding

from config.base_config import rag_config

# Setup logging
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class RAGProcessor:
    """
    Class implementing the RAG process

    Parameters
    ----------
    model : str
        LLM Model used for generating chatbot answers
    max_token : int
    stream : bool
    temperature : float
    top_p : float
    retriever :
    top_k : int
    """
    def __init__(self, llm_model: str, stream: bool, max_token: int, temperature: float,
                 top_p: float, retrieval_method: str, top_k: int):

        self.llm_model = llm_model
        self.stream = stream
        self.max_tokens = max_token
        self.temperature = temperature
        self.top_p = top_p
        self.llm_client = self.init_llm_client(llm_model, stream)
        self.retriever_client = self.init_retriever_client(retrieval_method=retrieval_method)
        self.k_retrieve = top_k

    def init_llm_client(self, llm_model: str = "gpt-4o-mini", stream: bool = True) -> BaseLLM:
        """
        Initialize and return an LLM client based on `llm_client`.

        Returns
        -------
        object
            An instance of the appropriate LLM client based on `llm_client`.
        """
        return LLMFactory.get_llm_client(llm_model=llm_model, stream=stream)

    def init_retriever_client(self, retrieval_method: str = "top_k"):
        """
        Initialize and return a retriever client based on `retrieval_method`.

        Returns
        -------
        object
            An instance of the appropriate retriever client based on `retrieval_method`.
        """
        return RetrieverFactory.get_retriever_client(retrieval_method=retrieval_method, processor=self)

    def create_rag_message(self, context_docs: List[Any], query: str) -> List[Dict]:
        """
        Format the RAG message to send to the OpenAI API.

        Parameters
        ----------
        context_docs : str
            Context matching the query according to the retrieval process
        query : str
            User input question

        Returns
        -------
        list of dict
            Contains the message in the correct format to send to the OpenAI API

        """
        openai_rag_system_prompt = OPENAI_RAG_SYSTEM_PROMPT_DE.format(context_docs=context_docs, query=query)
        return [{"role": "system", "content": openai_rag_system_prompt},]

    def retrieve(self, db: Session, request: RAGRequest, language: str = None, k: int = 0):
        """
        Retrieve context documents related to the user input question.

        Parameters
        ----------
        db : Session
            Database session
        request : RAGRequest
            User input question
        language : str
            Question and context documents language
        k : int, default 0
            Number of context documents to return
        """
        rows = self.retriever_client.get_documents(db, request.query, language=language, k=k)

        return rows if len(rows) > 0 else [{"text": "", "url": ""}]

    def process(self, db: Session, request: RAGRequest, language: str = None):
        """
        Process a RAGRequest to retrieve relevant documents and generate a response.

        This method retrieves relevant documents from the database, constructs a context from the documents, and then uses an LLM client to generate a response based on the request query and the context.

        Parameters
        ----------
        db : Session
            The database session to use for retrieving documents.
        request : RAGRequest
            The request to process.
        language : str, optional
            The language of the documents to retrieve. If not specified, documents in all languages are considered.

        Returns
        -------
        str
            The response generated by the LLM client.
        """
        documents = self.retrieve(db, request, language=language, k=self.k_retrieve)
        context_docs = "\n\n".join([doc.text for doc in documents]) #Â TO UPDATE
        source_url = documents[0].url # TO UPDATE

        messages = self.create_rag_message(context_docs, request.query)

        stream = self.llm_client.call(messages)

        return self.generate_stream(stream, source_url)

    async def embed(self, text_input: EmbeddingRequest):
        """
        Get the embedding of an embedding request.

        Parameters
        ----------
        text_input : EmbeddingRequest

        Returns
        -------
        dict
            The requested text embedding
        """
        embedding = get_embedding(text_input.text)
        return {"data": embedding}

    def generate_stream(self, stream, source_url):
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                yield chunk.choices[0].delta.content.encode("utf-8")
            else:
                # Send a special token indicating the end of the response
                yield f"\n\n<a href='{source_url}' target='_blank' class='source-link'>{source_url}</a>".encode("utf-8")
                return


processor = RAGProcessor(llm_model=rag_config["llm"]["model"],
                         stream=rag_config["llm"]["stream"],
                         max_token=rag_config["llm"]["max_output_tokens"],
                         temperature=rag_config["llm"]["temperature"],
                         top_p=rag_config["llm"]["top_p"],
                         retrieval_method=rag_config["retrieval"]["retrieval_method"],
                         top_k=rag_config["retrieval"]["top_k"],)
